{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "18872d5e-2ee7-4f1c-8374-c436aa9ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "from Functions_Classes import *\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(0)\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.activations import linear, relu, sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "403b851f-60a3-4d92-838e-9d6f00c5cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/training_data.xls\")\n",
    "X_test_compete = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/test_data_no_target.xls\")\n",
    "\n",
    "df = df.loc[:, df.columns != 'Perform']\n",
    "#df = df.loc[:, df.columns != 'Group']\n",
    "\n",
    "\n",
    "df_x = df.loc[:, df.columns != 'Class']\n",
    "df_y = df['Class']\n",
    "\n",
    "# X_train = df_x.copy()\n",
    "# y_train =  df_y.copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2, shuffle=True, stratify=df_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "006456f7-eecd-4486-abb2-db0ff3ef51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.loc[:, ~X_train.columns.isin(['Group'])].columns.to_list()\n",
    "X_train[numeric_columns] = X_train.loc[:, numeric_columns].replace(\n",
    "    {\n",
    "        'NA': np.nan, \n",
    "        '': np.nan,\n",
    "        ' ': np.nan\n",
    "    }\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "88066b03-5993-437f-9af7-d0f10d3bf8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>I14</th>\n",
       "      <th>I15</th>\n",
       "      <th>I16</th>\n",
       "      <th>I17</th>\n",
       "      <th>I18</th>\n",
       "      <th>I19</th>\n",
       "      <th>I20</th>\n",
       "      <th>I21</th>\n",
       "      <th>I22</th>\n",
       "      <th>I23</th>\n",
       "      <th>I24</th>\n",
       "      <th>I25</th>\n",
       "      <th>I26</th>\n",
       "      <th>I27</th>\n",
       "      <th>I28</th>\n",
       "      <th>I29</th>\n",
       "      <th>I30</th>\n",
       "      <th>I31</th>\n",
       "      <th>I32</th>\n",
       "      <th>I33</th>\n",
       "      <th>I34</th>\n",
       "      <th>I35</th>\n",
       "      <th>I36</th>\n",
       "      <th>I37</th>\n",
       "      <th>I38</th>\n",
       "      <th>I39</th>\n",
       "      <th>I40</th>\n",
       "      <th>I41</th>\n",
       "      <th>I42</th>\n",
       "      <th>I43</th>\n",
       "      <th>I44</th>\n",
       "      <th>I45</th>\n",
       "      <th>I46</th>\n",
       "      <th>I47</th>\n",
       "      <th>I48</th>\n",
       "      <th>I49</th>\n",
       "      <th>I50</th>\n",
       "      <th>I51</th>\n",
       "      <th>I52</th>\n",
       "      <th>I53</th>\n",
       "      <th>I54</th>\n",
       "      <th>I55</th>\n",
       "      <th>I56</th>\n",
       "      <th>I57</th>\n",
       "      <th>I58</th>\n",
       "      <th>dI1</th>\n",
       "      <th>dI2</th>\n",
       "      <th>dI3</th>\n",
       "      <th>dI4</th>\n",
       "      <th>dI5</th>\n",
       "      <th>dI6</th>\n",
       "      <th>dI7</th>\n",
       "      <th>dI8</th>\n",
       "      <th>dI9</th>\n",
       "      <th>dI10</th>\n",
       "      <th>dI11</th>\n",
       "      <th>dI12</th>\n",
       "      <th>dI13</th>\n",
       "      <th>dI14</th>\n",
       "      <th>dI15</th>\n",
       "      <th>dI16</th>\n",
       "      <th>dI17</th>\n",
       "      <th>dI18</th>\n",
       "      <th>dI19</th>\n",
       "      <th>dI20</th>\n",
       "      <th>dI21</th>\n",
       "      <th>dI22</th>\n",
       "      <th>dI23</th>\n",
       "      <th>dI24</th>\n",
       "      <th>dI25</th>\n",
       "      <th>dI26</th>\n",
       "      <th>dI27</th>\n",
       "      <th>dI28</th>\n",
       "      <th>dI29</th>\n",
       "      <th>dI30</th>\n",
       "      <th>dI31</th>\n",
       "      <th>dI32</th>\n",
       "      <th>dI33</th>\n",
       "      <th>dI34</th>\n",
       "      <th>dI35</th>\n",
       "      <th>dI36</th>\n",
       "      <th>dI37</th>\n",
       "      <th>dI38</th>\n",
       "      <th>dI39</th>\n",
       "      <th>dI40</th>\n",
       "      <th>dI41</th>\n",
       "      <th>dI42</th>\n",
       "      <th>dI43</th>\n",
       "      <th>dI44</th>\n",
       "      <th>dI45</th>\n",
       "      <th>dI46</th>\n",
       "      <th>dI47</th>\n",
       "      <th>dI48</th>\n",
       "      <th>dI49</th>\n",
       "      <th>dI50</th>\n",
       "      <th>dI51</th>\n",
       "      <th>dI52</th>\n",
       "      <th>dI53</th>\n",
       "      <th>dI54</th>\n",
       "      <th>dI55</th>\n",
       "      <th>dI56</th>\n",
       "      <th>dI57</th>\n",
       "      <th>dI58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426</th>\n",
       "      <td>G2</td>\n",
       "      <td>0.474336</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>-0.606795</td>\n",
       "      <td>-0.132097</td>\n",
       "      <td>0.227654</td>\n",
       "      <td>0.407601</td>\n",
       "      <td>-0.002148</td>\n",
       "      <td>0.255976</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>-0.042908</td>\n",
       "      <td>-0.031332</td>\n",
       "      <td>-0.039776</td>\n",
       "      <td>1.244465</td>\n",
       "      <td>-0.281740</td>\n",
       "      <td>-0.034615</td>\n",
       "      <td>-0.046939</td>\n",
       "      <td>0.046508</td>\n",
       "      <td>-0.189993</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>-0.201074</td>\n",
       "      <td>-0.061445</td>\n",
       "      <td>-0.044316</td>\n",
       "      <td>-0.571520</td>\n",
       "      <td>-0.145953</td>\n",
       "      <td>2.143646</td>\n",
       "      <td>-0.514716</td>\n",
       "      <td>-0.097087</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>0.599826</td>\n",
       "      <td>1.021227</td>\n",
       "      <td>1.567838</td>\n",
       "      <td>1.551942</td>\n",
       "      <td>1.468628</td>\n",
       "      <td>1.229721</td>\n",
       "      <td>1.321067</td>\n",
       "      <td>-0.084339</td>\n",
       "      <td>-0.042190</td>\n",
       "      <td>0.476034</td>\n",
       "      <td>0.192107</td>\n",
       "      <td>0.178743</td>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>-1.429454</td>\n",
       "      <td>-0.915034</td>\n",
       "      <td>-0.818222</td>\n",
       "      <td>-0.859571</td>\n",
       "      <td>-0.831964</td>\n",
       "      <td>-0.737321</td>\n",
       "      <td>-0.953812</td>\n",
       "      <td>-0.423662</td>\n",
       "      <td>0.167951</td>\n",
       "      <td>0.982461</td>\n",
       "      <td>1.507053</td>\n",
       "      <td>-0.872424</td>\n",
       "      <td>-0.117816</td>\n",
       "      <td>-0.031497</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>-0.008488</td>\n",
       "      <td>-0.122003</td>\n",
       "      <td>-0.150909</td>\n",
       "      <td>-0.168123</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>-0.143467</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>-0.017017</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.026585</td>\n",
       "      <td>-0.414900</td>\n",
       "      <td>-0.012459</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>-0.037350</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.036921</td>\n",
       "      <td>-0.025854</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>-0.046517</td>\n",
       "      <td>-0.028944</td>\n",
       "      <td>-0.756167</td>\n",
       "      <td>-0.166527</td>\n",
       "      <td>-0.281711</td>\n",
       "      <td>-0.278719</td>\n",
       "      <td>-0.292282</td>\n",
       "      <td>-0.167453</td>\n",
       "      <td>-0.215679</td>\n",
       "      <td>-0.088958</td>\n",
       "      <td>-0.036857</td>\n",
       "      <td>-0.895317</td>\n",
       "      <td>0.229733</td>\n",
       "      <td>0.222789</td>\n",
       "      <td>0.529174</td>\n",
       "      <td>0.039070</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.065233</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>-0.037055</td>\n",
       "      <td>-0.049063</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>-0.044080</td>\n",
       "      <td>0.118029</td>\n",
       "      <td>-0.006498</td>\n",
       "      <td>-0.015567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>G2</td>\n",
       "      <td>0.735381</td>\n",
       "      <td>-0.019519</td>\n",
       "      <td>-0.029400</td>\n",
       "      <td>1.373814</td>\n",
       "      <td>-0.242825</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>-0.218154</td>\n",
       "      <td>-0.029641</td>\n",
       "      <td>1.226153</td>\n",
       "      <td>-0.014224</td>\n",
       "      <td>0.696016</td>\n",
       "      <td>0.219364</td>\n",
       "      <td>-0.037933</td>\n",
       "      <td>0.711385</td>\n",
       "      <td>0.669523</td>\n",
       "      <td>-0.165935</td>\n",
       "      <td>-0.110332</td>\n",
       "      <td>-0.044059</td>\n",
       "      <td>-0.110881</td>\n",
       "      <td>0.283741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>-0.100089</td>\n",
       "      <td>-0.088502</td>\n",
       "      <td>0.737501</td>\n",
       "      <td>-0.100008</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>1.584636</td>\n",
       "      <td>-0.123937</td>\n",
       "      <td>-0.048261</td>\n",
       "      <td>-0.549628</td>\n",
       "      <td>-0.592205</td>\n",
       "      <td>-0.239093</td>\n",
       "      <td>-0.235795</td>\n",
       "      <td>-0.505718</td>\n",
       "      <td>-0.186124</td>\n",
       "      <td>-0.501709</td>\n",
       "      <td>-0.046078</td>\n",
       "      <td>-0.036884</td>\n",
       "      <td>-0.457416</td>\n",
       "      <td>0.115378</td>\n",
       "      <td>0.104334</td>\n",
       "      <td>0.187516</td>\n",
       "      <td>0.077230</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>-0.317715</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.230770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.487625</td>\n",
       "      <td>0.532675</td>\n",
       "      <td>-0.113977</td>\n",
       "      <td>-0.502087</td>\n",
       "      <td>-0.072377</td>\n",
       "      <td>-0.530395</td>\n",
       "      <td>-0.119678</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.020779</td>\n",
       "      <td>-0.001515</td>\n",
       "      <td>-0.001271</td>\n",
       "      <td>0.566549</td>\n",
       "      <td>0.112948</td>\n",
       "      <td>0.148753</td>\n",
       "      <td>0.146571</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.791676</td>\n",
       "      <td>-0.004064</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.351006</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.178190</td>\n",
       "      <td>0.477856</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>-0.003576</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>0.026280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>-0.055436</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>-0.177226</td>\n",
       "      <td>-0.033158</td>\n",
       "      <td>0.017723</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>0.098352</td>\n",
       "      <td>0.119397</td>\n",
       "      <td>0.039128</td>\n",
       "      <td>0.038712</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.045437</td>\n",
       "      <td>0.016490</td>\n",
       "      <td>0.133769</td>\n",
       "      <td>-0.149494</td>\n",
       "      <td>-0.144975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077788</td>\n",
       "      <td>0.149001</td>\n",
       "      <td>-0.015912</td>\n",
       "      <td>0.704333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.193875</td>\n",
       "      <td>0.478701</td>\n",
       "      <td>-0.024174</td>\n",
       "      <td>-0.148020</td>\n",
       "      <td>-0.161484</td>\n",
       "      <td>-0.100311</td>\n",
       "      <td>0.069675</td>\n",
       "      <td>-0.004375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>G8</td>\n",
       "      <td>0.150030</td>\n",
       "      <td>-0.030559</td>\n",
       "      <td>-0.041371</td>\n",
       "      <td>-0.180490</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>-0.143670</td>\n",
       "      <td>-0.301218</td>\n",
       "      <td>-0.039838</td>\n",
       "      <td>0.782315</td>\n",
       "      <td>-0.021184</td>\n",
       "      <td>-0.104858</td>\n",
       "      <td>0.603132</td>\n",
       "      <td>-0.063976</td>\n",
       "      <td>0.786947</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>-0.155514</td>\n",
       "      <td>-0.070871</td>\n",
       "      <td>-0.045427</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.062908</td>\n",
       "      <td>-0.163518</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.072948</td>\n",
       "      <td>-0.057468</td>\n",
       "      <td>0.767850</td>\n",
       "      <td>0.173958</td>\n",
       "      <td>-0.128118</td>\n",
       "      <td>-0.310119</td>\n",
       "      <td>-0.164138</td>\n",
       "      <td>-0.079906</td>\n",
       "      <td>-0.883528</td>\n",
       "      <td>-1.344381</td>\n",
       "      <td>-0.716429</td>\n",
       "      <td>-0.708061</td>\n",
       "      <td>-0.897027</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.724724</td>\n",
       "      <td>-0.170219</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.867710</td>\n",
       "      <td>0.626772</td>\n",
       "      <td>0.600270</td>\n",
       "      <td>0.559442</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>0.906108</td>\n",
       "      <td>-0.712064</td>\n",
       "      <td>0.854778</td>\n",
       "      <td>1.128683</td>\n",
       "      <td>1.737216</td>\n",
       "      <td>1.107266</td>\n",
       "      <td>1.701688</td>\n",
       "      <td>1.132818</td>\n",
       "      <td>-0.104753</td>\n",
       "      <td>-0.537718</td>\n",
       "      <td>0.410211</td>\n",
       "      <td>0.797962</td>\n",
       "      <td>-0.112652</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>-1.246969</td>\n",
       "      <td>-0.017840</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>-0.627947</td>\n",
       "      <td>0.082116</td>\n",
       "      <td>-0.027240</td>\n",
       "      <td>0.043286</td>\n",
       "      <td>-0.003879</td>\n",
       "      <td>-0.281142</td>\n",
       "      <td>-0.029354</td>\n",
       "      <td>-1.055325</td>\n",
       "      <td>0.657791</td>\n",
       "      <td>0.038651</td>\n",
       "      <td>-0.156749</td>\n",
       "      <td>-0.189418</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>0.036775</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>-0.690801</td>\n",
       "      <td>-0.011164</td>\n",
       "      <td>-0.048206</td>\n",
       "      <td>-0.008495</td>\n",
       "      <td>-0.009713</td>\n",
       "      <td>-0.455277</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>-0.364856</td>\n",
       "      <td>-0.055686</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.016951</td>\n",
       "      <td>0.031098</td>\n",
       "      <td>-0.228416</td>\n",
       "      <td>-0.394980</td>\n",
       "      <td>-0.390785</td>\n",
       "      <td>-0.315390</td>\n",
       "      <td>-0.323573</td>\n",
       "      <td>-0.280804</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>-1.159936</td>\n",
       "      <td>-1.124875</td>\n",
       "      <td>0.182791</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.049387</td>\n",
       "      <td>-0.078541</td>\n",
       "      <td>0.660222</td>\n",
       "      <td>-0.383559</td>\n",
       "      <td>0.192662</td>\n",
       "      <td>0.380037</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.726753</td>\n",
       "      <td>-0.008247</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>-0.054802</td>\n",
       "      <td>-0.133887</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>G11</td>\n",
       "      <td>-0.392277</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>-0.017427</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.194470</td>\n",
       "      <td>0.427458</td>\n",
       "      <td>0.290762</td>\n",
       "      <td>-0.039829</td>\n",
       "      <td>-0.240505</td>\n",
       "      <td>-0.022131</td>\n",
       "      <td>-0.481441</td>\n",
       "      <td>1.195059</td>\n",
       "      <td>-0.133470</td>\n",
       "      <td>0.360923</td>\n",
       "      <td>0.753503</td>\n",
       "      <td>-0.114618</td>\n",
       "      <td>-0.059685</td>\n",
       "      <td>-0.046232</td>\n",
       "      <td>-0.002248</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.221062</td>\n",
       "      <td>-0.119099</td>\n",
       "      <td>-0.110237</td>\n",
       "      <td>-0.664177</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>-0.336943</td>\n",
       "      <td>0.021138</td>\n",
       "      <td>-0.160624</td>\n",
       "      <td>-0.099285</td>\n",
       "      <td>-0.440708</td>\n",
       "      <td>0.383819</td>\n",
       "      <td>0.641210</td>\n",
       "      <td>0.635158</td>\n",
       "      <td>2.653875</td>\n",
       "      <td>0.184460</td>\n",
       "      <td>2.560986</td>\n",
       "      <td>-0.241563</td>\n",
       "      <td>-0.109047</td>\n",
       "      <td>-0.653331</td>\n",
       "      <td>0.699936</td>\n",
       "      <td>0.671222</td>\n",
       "      <td>-1.033898</td>\n",
       "      <td>-0.016909</td>\n",
       "      <td>-0.901262</td>\n",
       "      <td>-1.001319</td>\n",
       "      <td>-0.371000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.241250</td>\n",
       "      <td>0.242688</td>\n",
       "      <td>-0.002609</td>\n",
       "      <td>0.139621</td>\n",
       "      <td>1.329443</td>\n",
       "      <td>2.329796</td>\n",
       "      <td>0.123187</td>\n",
       "      <td>-0.060640</td>\n",
       "      <td>-0.201366</td>\n",
       "      <td>-0.004628</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>-0.039354</td>\n",
       "      <td>-0.068543</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0.130321</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.007429</td>\n",
       "      <td>-0.154127</td>\n",
       "      <td>4.104377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.260225</td>\n",
       "      <td>0.465077</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.026615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.022250</td>\n",
       "      <td>-0.034998</td>\n",
       "      <td>-0.308995</td>\n",
       "      <td>-0.103373</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>-0.091684</td>\n",
       "      <td>-0.075261</td>\n",
       "      <td>-0.258189</td>\n",
       "      <td>-0.255447</td>\n",
       "      <td>-0.126716</td>\n",
       "      <td>-0.276211</td>\n",
       "      <td>-0.155800</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>-0.101877</td>\n",
       "      <td>-0.032694</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012951</td>\n",
       "      <td>0.167433</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>0.353778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.336252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227687</td>\n",
       "      <td>0.519870</td>\n",
       "      <td>-0.019833</td>\n",
       "      <td>-0.040847</td>\n",
       "      <td>-0.052477</td>\n",
       "      <td>1.004543</td>\n",
       "      <td>-0.061889</td>\n",
       "      <td>-0.005486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>G7</td>\n",
       "      <td>-0.826989</td>\n",
       "      <td>-0.041629</td>\n",
       "      <td>-0.046757</td>\n",
       "      <td>-0.905674</td>\n",
       "      <td>0.165526</td>\n",
       "      <td>-0.942051</td>\n",
       "      <td>0.391674</td>\n",
       "      <td>-0.050908</td>\n",
       "      <td>-0.869245</td>\n",
       "      <td>-0.040587</td>\n",
       "      <td>-0.942128</td>\n",
       "      <td>-1.920122</td>\n",
       "      <td>0.109571</td>\n",
       "      <td>-0.295938</td>\n",
       "      <td>-0.482493</td>\n",
       "      <td>-0.302145</td>\n",
       "      <td>0.150220</td>\n",
       "      <td>-0.048025</td>\n",
       "      <td>0.346086</td>\n",
       "      <td>-0.710210</td>\n",
       "      <td>0.268111</td>\n",
       "      <td>-0.243370</td>\n",
       "      <td>-0.101431</td>\n",
       "      <td>-0.101193</td>\n",
       "      <td>-0.887292</td>\n",
       "      <td>-0.111807</td>\n",
       "      <td>0.080874</td>\n",
       "      <td>-0.527018</td>\n",
       "      <td>0.058842</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>0.591906</td>\n",
       "      <td>0.089524</td>\n",
       "      <td>0.089332</td>\n",
       "      <td>0.645773</td>\n",
       "      <td>-0.197195</td>\n",
       "      <td>0.443340</td>\n",
       "      <td>-0.100691</td>\n",
       "      <td>-0.055519</td>\n",
       "      <td>-0.113597</td>\n",
       "      <td>-0.179029</td>\n",
       "      <td>-0.461493</td>\n",
       "      <td>0.674174</td>\n",
       "      <td>-0.022750</td>\n",
       "      <td>-1.457341</td>\n",
       "      <td>-0.498530</td>\n",
       "      <td>-0.956111</td>\n",
       "      <td>-0.924697</td>\n",
       "      <td>-1.034101</td>\n",
       "      <td>-0.916229</td>\n",
       "      <td>-1.340563</td>\n",
       "      <td>-0.615273</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.375269</td>\n",
       "      <td>0.591471</td>\n",
       "      <td>0.527293</td>\n",
       "      <td>-0.159744</td>\n",
       "      <td>-0.047051</td>\n",
       "      <td>-0.277401</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-0.025115</td>\n",
       "      <td>0.030998</td>\n",
       "      <td>0.254083</td>\n",
       "      <td>0.314438</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>-0.007482</td>\n",
       "      <td>-0.009609</td>\n",
       "      <td>-0.051676</td>\n",
       "      <td>-0.920798</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>-0.170878</td>\n",
       "      <td>-0.094073</td>\n",
       "      <td>-0.004514</td>\n",
       "      <td>0.070196</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>-0.197957</td>\n",
       "      <td>-0.014884</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>-0.098531</td>\n",
       "      <td>0.060141</td>\n",
       "      <td>-0.458352</td>\n",
       "      <td>-0.005823</td>\n",
       "      <td>0.060331</td>\n",
       "      <td>0.022833</td>\n",
       "      <td>0.580822</td>\n",
       "      <td>0.380013</td>\n",
       "      <td>0.235347</td>\n",
       "      <td>0.232847</td>\n",
       "      <td>0.416106</td>\n",
       "      <td>-0.428854</td>\n",
       "      <td>-0.192951</td>\n",
       "      <td>0.042064</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>-0.323833</td>\n",
       "      <td>-0.594364</td>\n",
       "      <td>0.129474</td>\n",
       "      <td>-0.047782</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>0.222922</td>\n",
       "      <td>-0.022889</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>-0.097475</td>\n",
       "      <td>-0.119853</td>\n",
       "      <td>-0.220500</td>\n",
       "      <td>-0.030727</td>\n",
       "      <td>0.039075</td>\n",
       "      <td>0.105650</td>\n",
       "      <td>0.332904</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.004688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Group        I1        I2        I3        I4        I5        I6  \\\n",
       "7426    G2  0.474336  0.010288 -0.000710 -0.606795 -0.132097  0.227654   \n",
       "4264    G2  0.735381 -0.019519 -0.029400  1.373814 -0.242825  0.006498   \n",
       "5670    G8  0.150030 -0.030559 -0.041371 -0.180490  0.023586 -0.143670   \n",
       "2110   G11 -0.392277 -0.036396 -0.017427  0.000440  0.194470  0.427458   \n",
       "2116    G7 -0.826989 -0.041629 -0.046757 -0.905674  0.165526 -0.942051   \n",
       "\n",
       "            I7        I8        I9       I10       I11       I12       I13  \\\n",
       "7426  0.407601 -0.002148  0.255976  0.010343  0.069800 -0.042908 -0.031332   \n",
       "4264 -0.218154 -0.029641  1.226153 -0.014224  0.696016  0.219364 -0.037933   \n",
       "5670 -0.301218 -0.039838  0.782315 -0.021184 -0.104858  0.603132 -0.063976   \n",
       "2110  0.290762 -0.039829 -0.240505 -0.022131 -0.481441  1.195059 -0.133470   \n",
       "2116  0.391674 -0.050908 -0.869245 -0.040587 -0.942128 -1.920122  0.109571   \n",
       "\n",
       "           I14       I15       I16       I17       I18       I19       I20  \\\n",
       "7426 -0.039776  1.244465 -0.281740 -0.034615 -0.046939  0.046508 -0.189993   \n",
       "4264  0.711385  0.669523 -0.165935 -0.110332 -0.044059 -0.110881  0.283741   \n",
       "5670  0.786947  0.997630 -0.155514 -0.070871 -0.045427 -0.083950  0.062908   \n",
       "2110  0.360923  0.753503 -0.114618 -0.059685 -0.046232 -0.002248  0.065763   \n",
       "2116 -0.295938 -0.482493 -0.302145  0.150220 -0.048025  0.346086 -0.710210   \n",
       "\n",
       "           I21       I22       I23       I24       I25       I26       I27  \\\n",
       "7426 -0.042645 -0.201074 -0.061445 -0.044316 -0.571520 -0.145953  2.143646   \n",
       "4264       NaN  0.006747 -0.100089 -0.088502  0.737501 -0.100008  0.507143   \n",
       "5670 -0.163518 -0.015327 -0.072948 -0.057468  0.767850  0.173958 -0.128118   \n",
       "2110       NaN -0.221062 -0.119099 -0.110237 -0.664177  0.052724 -0.336943   \n",
       "2116  0.268111 -0.243370 -0.101431 -0.101193 -0.887292 -0.111807  0.080874   \n",
       "\n",
       "           I28       I29       I30       I31       I32       I33       I34  \\\n",
       "7426 -0.514716 -0.097087 -0.035054  0.599826  1.021227  1.567838  1.551942   \n",
       "4264  1.584636 -0.123937 -0.048261 -0.549628 -0.592205 -0.239093 -0.235795   \n",
       "5670 -0.310119 -0.164138 -0.079906 -0.883528 -1.344381 -0.716429 -0.708061   \n",
       "2110  0.021138 -0.160624 -0.099285 -0.440708  0.383819  0.641210  0.635158   \n",
       "2116 -0.527018  0.058842  0.002634  0.692140  0.591906  0.089524  0.089332   \n",
       "\n",
       "           I35       I36       I37       I38       I39       I40       I41  \\\n",
       "7426  1.468628  1.229721  1.321067 -0.084339 -0.042190  0.476034  0.192107   \n",
       "4264 -0.505718 -0.186124 -0.501709 -0.046078 -0.036884 -0.457416  0.115378   \n",
       "5670 -0.897027 -0.443576 -0.724724 -0.170219 -0.079356 -0.867710  0.626772   \n",
       "2110  2.653875  0.184460  2.560986 -0.241563 -0.109047 -0.653331  0.699936   \n",
       "2116  0.645773 -0.197195  0.443340 -0.100691 -0.055519 -0.113597 -0.179029   \n",
       "\n",
       "           I42       I43       I44       I45       I46       I47       I48  \\\n",
       "7426  0.178743  0.425832  0.051295 -1.429454 -0.915034 -0.818222 -0.859571   \n",
       "4264  0.104334  0.187516  0.077230  0.179328 -0.317715  0.771000       NaN   \n",
       "5670  0.600270  0.559442 -0.010800  0.906108 -0.712064  0.854778  1.128683   \n",
       "2110  0.671222 -1.033898 -0.016909 -0.901262 -1.001319 -0.371000       NaN   \n",
       "2116 -0.461493  0.674174 -0.022750 -1.457341 -0.498530 -0.956111 -0.924697   \n",
       "\n",
       "           I49       I50       I51       I52       I53       I54       I55  \\\n",
       "7426 -0.831964 -0.737321 -0.953812 -0.423662  0.167951  0.982461  1.507053   \n",
       "4264  0.230770       NaN  0.487625  0.532675 -0.113977 -0.502087 -0.072377   \n",
       "5670  1.737216  1.107266  1.701688  1.132818 -0.104753 -0.537718  0.410211   \n",
       "2110  0.127813       NaN -0.241250  0.242688 -0.002609  0.139621  1.329443   \n",
       "2116 -1.034101 -0.916229 -1.340563 -0.615273  0.028934  0.375269  0.591471   \n",
       "\n",
       "           I56       I57       I58       dI1       dI2       dI3       dI4  \\\n",
       "7426 -0.872424 -0.117816 -0.031497 -0.008751  0.009108  0.008876 -0.008488   \n",
       "4264 -0.530395 -0.119678 -0.035370 -0.020779 -0.001515 -0.001271  0.566549   \n",
       "5670  0.797962 -0.112652 -0.057491 -1.246969 -0.017840 -0.014250 -0.627947   \n",
       "2110  2.329796  0.123187 -0.060640 -0.201366 -0.004628 -0.008292 -0.039354   \n",
       "2116  0.527293 -0.159744 -0.047051 -0.277401 -0.005865 -0.000638 -0.025115   \n",
       "\n",
       "           dI5       dI6       dI7       dI8       dI9      dI10      dI11  \\\n",
       "7426 -0.122003 -0.150909 -0.168123  0.007286 -0.143467  0.005262 -0.017017   \n",
       "4264  0.112948  0.148753  0.146571  0.010503  0.791676 -0.004064  0.011731   \n",
       "5670  0.082116 -0.027240  0.043286 -0.003879 -0.281142 -0.029354 -1.055325   \n",
       "2110 -0.068543  0.074813  0.130321 -0.000238  0.128159 -0.007429 -0.154127   \n",
       "2116  0.030998  0.254083  0.314438  0.002076 -0.007482 -0.009609 -0.051676   \n",
       "\n",
       "          dI12      dI13      dI14      dI15      dI16      dI17      dI18  \\\n",
       "7426 -0.045596  0.007570  0.026585 -0.414900 -0.012459  0.012176 -0.000270   \n",
       "4264  0.351006  0.012634 -0.178190  0.477856  0.039291 -0.003576  0.001576   \n",
       "5670  0.657791  0.038651 -0.156749 -0.189418  0.051770  0.036775 -0.000911   \n",
       "2110  4.104377  0.000000 -0.260225  0.465077  0.126900 -0.000810  0.000136   \n",
       "2116 -0.920798  0.125177 -0.170878 -0.094073 -0.004514  0.070196  0.000679   \n",
       "\n",
       "          dI19      dI20      dI21      dI22      dI23      dI24      dI25  \\\n",
       "7426  0.020325 -0.037350  0.034140  0.004292  0.000737  0.000842  0.036921   \n",
       "4264 -0.013169  0.026280       NaN  0.002315 -0.000972 -0.001111 -0.055436   \n",
       "5670  0.020150 -0.690801 -0.011164 -0.048206 -0.008495 -0.009713 -0.455277   \n",
       "2110 -0.009085 -0.026615       NaN -0.000076 -0.000358  0.000831 -0.022250   \n",
       "2116  0.115591 -0.187369 -0.197957 -0.014884 -0.013678 -0.015323 -0.098531   \n",
       "\n",
       "          dI26      dI27      dI28      dI29      dI30      dI31      dI32  \\\n",
       "7426 -0.025854  0.210788  0.002333 -0.046517 -0.028944 -0.756167 -0.166527   \n",
       "4264 -0.005346 -0.177226 -0.033158  0.017723  0.011541  0.098352  0.119397   \n",
       "5670  0.046544 -0.364856 -0.055686  0.034768  0.016951  0.031098 -0.228416   \n",
       "2110 -0.034998 -0.308995 -0.103373 -0.001026 -0.002704 -0.091684 -0.075261   \n",
       "2116  0.060141 -0.458352 -0.005823  0.060331  0.022833  0.580822  0.380013   \n",
       "\n",
       "          dI33      dI34      dI35      dI36      dI37      dI38      dI39  \\\n",
       "7426 -0.281711 -0.278719 -0.292282 -0.167453 -0.215679 -0.088958 -0.036857   \n",
       "4264  0.039128  0.038712  0.037939  0.013403  0.017530  0.045437  0.016490   \n",
       "5670 -0.394980 -0.390785 -0.315390 -0.323573 -0.280804  0.043982  0.018647   \n",
       "2110 -0.258189 -0.255447 -0.126716 -0.276211 -0.155800 -0.006981 -0.003381   \n",
       "2116  0.235347  0.232847  0.416106 -0.428854 -0.192951  0.042064 -0.001595   \n",
       "\n",
       "          dI40      dI41      dI42      dI43      dI44      dI45      dI46  \\\n",
       "7426 -0.895317  0.229733  0.222789  0.529174  0.039070  0.005165  0.065233   \n",
       "4264  0.133769 -0.149494 -0.144975       NaN  0.077788  0.149001 -0.015912   \n",
       "5670  0.038100 -1.159936 -1.124875  0.182791 -0.007366 -0.049387 -0.078541   \n",
       "2110 -0.101877 -0.032694 -0.031706  0.000000 -0.012951  0.167433 -0.029549   \n",
       "2116  0.154025 -0.323833 -0.594364  0.129474 -0.047782 -0.026050  0.222922   \n",
       "\n",
       "          dI47      dI48      dI49      dI50      dI51      dI52      dI53  \\\n",
       "7426  0.024111 -0.003079 -0.040331 -0.037055 -0.049063  0.009558 -0.000408   \n",
       "4264  0.704333       NaN  0.135784       NaN  0.193875  0.478701 -0.024174   \n",
       "5670  0.660222 -0.383559  0.192662  0.380037  0.006750  0.726753 -0.008247   \n",
       "2110  0.353778       NaN  0.336252       NaN  0.227687  0.519870 -0.019833   \n",
       "2116 -0.022889  0.003734 -0.097475 -0.119853 -0.220500 -0.030727  0.039075   \n",
       "\n",
       "          dI54      dI55      dI56      dI57      dI58  \n",
       "7426 -0.016245 -0.044080  0.118029 -0.006498 -0.015567  \n",
       "4264 -0.148020 -0.161484 -0.100311  0.069675 -0.004375  \n",
       "5670  0.087142  0.101860 -0.054802 -0.133887  0.001193  \n",
       "2110 -0.040847 -0.052477  1.004543 -0.061889 -0.005486  \n",
       "2116  0.105650  0.332904  0.011538  0.000010 -0.004688  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0384-cbe0-44f2-8213-eb9593d584ad",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "aecc89f2-c2ee-4ea5-8684-821d96bb9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 117)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852d5c1-cd68-4fb6-a691-4436538012a3",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "346abda1-a979-431a-b6a2-0219de1a8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPClassifier(\n",
    "    hidden_layer_sizes=(256, 128, 64),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=100000, \n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "voting_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    #('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    #('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork', nn) #hidden_layer_sizes=(20,20) for 2 hidden layers with 20 neurons each\n",
    "]\n",
    "\n",
    "vote_model = VotingClassifier(\n",
    "    estimators=voting_estimators, \n",
    "    voting='hard'\n",
    ")\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    #('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork',nn)\n",
    "]\n",
    "meta_stack_classifier = LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=stacking_estimators, \n",
    "    final_estimator=meta_stack_classifier, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    #('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    #('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME'))\n",
    "    #('Voting', vote_model),\n",
    "    #('Stacking', stacking_model),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000)) # 2 hidden layers with 20 neurons each\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b2aa3-2279-4c45-be46-3fa5b9668c35",
   "metadata": {},
   "source": [
    "# Create Pipeline with different combination of preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665f57b-10f3-482f-b28b-1b4fc3a5b25e",
   "metadata": {},
   "source": [
    "## Combination 7\n",
    "#### knn impute, robust scaler, lof, smote, rfecv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8d91f42e-aedb-4430-b6dc-fedea5e7ff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xtrain:  (6400, 127)\n",
      "Shape of xtrain:  (4266, 127)\n"
     ]
    }
   ],
   "source": [
    "objs = dict()\n",
    "X_trainP, ohe_encoder = apply_one_hot_encoding(\n",
    "    X_train, \n",
    "    'Group'\n",
    ")\n",
    "objs['ohe'] = ohe_encoder\n",
    "####################################### Imputing Missing Values\n",
    "X_trainP, imp = handle_missing_vals_simple(\n",
    "    X_trainP\n",
    ")\n",
    "\n",
    "objs['miss'] = imp\n",
    "print('Shape of xtrain: ', X_trainP.shape)\n",
    "\n",
    "####################################### Robust\n",
    "std_scale_cols = (\n",
    "    X_trainP\n",
    "    .loc[:, ~X_trainP.columns.str.contains('Group')]\n",
    "    .columns\n",
    ")\n",
    "\n",
    "X_trainP, std_scaler = apply_robust_scaler(\n",
    "    X_trainP, \n",
    "    std_scale_cols\n",
    ")\n",
    "\n",
    "objs['robust_scaler'] = std_scaler\n",
    "####################################### LOF\n",
    "X_trainP_df = pd.concat(\n",
    "    [\n",
    "        X_trainP.reset_index(drop=True), \n",
    "        pd.Series(y_train, name='Class').reset_index(drop=True)\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_trainP_df = detect_outliers_with_lof(\n",
    "    data=X_trainP_df\n",
    ")[0]\n",
    "\n",
    "X_trainP = (\n",
    "    X_trainP_df\n",
    "    .loc[:, X_trainP_df.columns != 'Class']\n",
    ")\n",
    "\n",
    "y_trainP = X_trainP_df['Class']\n",
    "print('Shape of xtrain: ', X_trainP.shape)\n",
    "\n",
    "####################################### sampling\n",
    "X_trainP, y_trainP = apply_random_undersampling(X_trainP, y_trainP)\n",
    "#apply_random_undersampling\n",
    "#apply_random_oversampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d88e15d-861c-40e9-a5be-0c93825ff42b",
   "metadata": {},
   "source": [
    "# Prepare the test set for real predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "618cec44-ca6f-45f0-b9c7-a2bbcc86567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_compete_group = X_test_compete.loc[:, 'Group'].copy().reset_index(drop=True)\n",
    "X_test_compete = X_test_compete.drop('Group', axis=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bf2774a-3483-4a80-a706-ed4483152e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_competeP = pd.DataFrame(objs['miss'].transform(X_test_compete),columns=X_test_compete.columns)\n",
    "X_test_competeP = pd.DataFrame(objs['robust_scaler'].transform(X_test_competeP), columns=X_test_competeP.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "46b98bdf-df95-44c7-ac4f-362d92fe4ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_competeP = pd.concat([X_test_competeP, X_test_compete_group], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af005424-0f82-4853-b4ef-9ca569714772",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_competeP_ohe_cols = pd.DataFrame(\n",
    "        objs['ohe'].transform(X_test_competeP[['Group']]).toarray(),\n",
    "        columns=objs['ohe'].get_feature_names_out(['Group'])\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "X_test_competeP = X_test_competeP.drop('Group', axis=1).reset_index(drop=True)\n",
    "X_test_competeP = pd.concat([X_test_competeP, X_test_competeP_ohe_cols], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c0b26-1eac-4038-9c24-27ec9b953fe5",
   "metadata": {},
   "source": [
    "## Prepare the Validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3c5cc509-80a5-4fe3-b658-8dfbd7c012c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_Group = X_test.loc[:, 'Group'].copy().reset_index(drop=True)\n",
    "X_test = X_test.drop('Group', axis=1).reset_index(drop=True)\n",
    "\n",
    "X_test = pd.DataFrame(objs['miss'].transform(X_test),columns=X_test.columns)\n",
    "X_test = pd.DataFrame(objs['robust_scaler'].transform(X_test), columns=X_test.columns)\n",
    "\n",
    "X_test = pd.concat([X_test, X_test_Group], axis=1)\n",
    "\n",
    "X_test_ohe_cols = pd.DataFrame(\n",
    "        objs['ohe'].transform(X_test[['Group']]).toarray(),\n",
    "        columns=objs['ohe'].get_feature_names_out(['Group'])\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "X_test = X_test.drop('Group', axis=1).reset_index(drop=True)\n",
    "X_test = pd.concat([X_test, X_test_ohe_cols], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c60c5-eec4-42f5-8f3c-7e7f16f25262",
   "metadata": {},
   "source": [
    "## TensorFlow NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fbbb9069-bc4a-4cf6-9240-8409aa4d29e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1234)\n",
    "model = Sequential(\n",
    "    [               \n",
    "        tf.keras.Input(shape=(127,)),    \n",
    "        Dense(25, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.6),name = \"L1\"), \n",
    "        Dense(15, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.6),name = \"L2\"),\n",
    "        Dense(3, activation='linear', name = \"L3\"),\n",
    "    ], name = \"my_model\" \n",
    ")\n",
    "# model = Sequential(\n",
    "#     [\n",
    "#         Dense(120, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L1\"), \n",
    "#         Dense(40, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(0.1), name=\"L2\"),  \n",
    "#         Dense(3, activation = 'linear', name=\"L3\")  \n",
    "#     ], name=\"ComplexRegularized\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b67f1b7-fce9-4c96-8890-b747ec7e823f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [layer1, layer2, layer3] = model.layers\n",
    "\n",
    "# W1,b1 = layer1.get_weights()\n",
    "# W2,b2 = layer2.get_weights()\n",
    "# W3,b3 = layer3.get_weights()\n",
    "# print(f\"W1 shape = {W1.shape}, b1 shape = {b1.shape}\")\n",
    "# print(f\"W2 shape = {W2.shape}, b2 shape = {b2.shape}\")\n",
    "# print(f\"W3 shape = {W3.shape}, b3 shape = {b3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e8ad05cf-be35-417f-87e7-81f354969827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 33.4259 \n",
      "Epoch 2/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 16.7244\n",
      "Epoch 3/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 9.1502 \n",
      "Epoch 4/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 5.7773\n",
      "Epoch 5/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4.1801\n",
      "Epoch 6/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 3.2410\n",
      "Epoch 7/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.8178\n",
      "Epoch 8/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.4390\n",
      "Epoch 9/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.2403\n",
      "Epoch 10/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 2.0742\n",
      "Epoch 11/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.9720\n",
      "Epoch 12/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8734\n",
      "Epoch 13/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.8126\n",
      "Epoch 14/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.7226\n",
      "Epoch 15/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6659\n",
      "Epoch 16/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.6222\n",
      "Epoch 17/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.5941\n",
      "Epoch 18/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.5445\n",
      "Epoch 19/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.5347\n",
      "Epoch 20/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4725\n",
      "Epoch 21/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4591\n",
      "Epoch 22/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.4179\n",
      "Epoch 23/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3996\n",
      "Epoch 24/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3684\n",
      "Epoch 25/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3499\n",
      "Epoch 26/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3289\n",
      "Epoch 27/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.3121\n",
      "Epoch 28/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2955\n",
      "Epoch 29/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2808\n",
      "Epoch 30/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2669\n",
      "Epoch 31/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2545\n",
      "Epoch 32/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2427\n",
      "Epoch 33/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2316\n",
      "Epoch 34/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2214\n",
      "Epoch 35/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2121\n",
      "Epoch 36/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.2030\n",
      "Epoch 37/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1946\n",
      "Epoch 38/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1881\n",
      "Epoch 39/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1806\n",
      "Epoch 40/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1738\n",
      "Epoch 41/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1677\n",
      "Epoch 42/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1620\n",
      "Epoch 43/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1569\n",
      "Epoch 44/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1520\n",
      "Epoch 45/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1476\n",
      "Epoch 46/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1435\n",
      "Epoch 47/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1397\n",
      "Epoch 48/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1363\n",
      "Epoch 49/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1330\n",
      "Epoch 50/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1301\n",
      "Epoch 51/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1274\n",
      "Epoch 52/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1249\n",
      "Epoch 53/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1226\n",
      "Epoch 54/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1205\n",
      "Epoch 55/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1186\n",
      "Epoch 56/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1169\n",
      "Epoch 57/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1152\n",
      "Epoch 58/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1138\n",
      "Epoch 59/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1124\n",
      "Epoch 60/60\n",
      "\u001b[1m57/57\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.1112\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "y_train_mapped = y_trainP + 1\n",
    "history = model.fit(\n",
    "    X_trainP,y_train_mapped,\n",
    "    epochs=60\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588462ff-22d2-49e5-9fb3-8410e6742fd4",
   "metadata": {},
   "source": [
    "#### Validation set predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8e71eaa3-b8b7-4074-803a-a94069c92056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m50/50\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 980us/step\n"
     ]
    }
   ],
   "source": [
    "prediction_test = model.predict(X_test)\n",
    "prediction_test = tf.nn.softmax(prediction_test)\n",
    "yhat_validate = np.argmax(prediction_test, axis=1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ffc2534c-eab9-439d-96f3-e823307f2863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4dfacd3b-e66b-4d4e-a25d-452885ed7dd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1,  0,  1, ...,  0, -1,  1], dtype=int64)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7f9c3a4e-9bd4-4ca9-8b37-3cd0f2e53956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.86125"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_error_cost_score(np.array(y_test), yhat_validate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d65b452-240b-4c98-a022-d4e7807ad910",
   "metadata": {},
   "source": [
    "# Real Test Set Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "84762fb3-3717-4bd4-8d38-61fadb104f91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m63/63\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step  \n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X_test_competeP)\n",
    "prediction_p = tf.nn.softmax(prediction)\n",
    "yhat = np.argmax(prediction_p, axis=1)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "18bf0a25-2999-409c-b26f-c200e2d4ddca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  0, -1, ...,  1,  1,  1], dtype=int64)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c715c32-ea00-4c52-a4bf-5a932db2021a",
   "metadata": {},
   "source": [
    "## Predictions Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb0cd234-6ddb-43d1-bb41-b880804dbdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"comb10_4.txt\"\n",
    "pd.DataFrame(yhat).to_csv(file_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d17e5d2-8abe-4108-b23b-344b208c067e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
