{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18872d5e-2ee7-4f1c-8374-c436aa9ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import RFECV, SelectKBest, SequentialFeatureSelector, f_classif, mutual_info_classif\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from Functions_Classes import *\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403b851f-60a3-4d92-838e-9d6f00c5cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/training_data.xls\")\n",
    "X_test_compete = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/test_data_no_target.xls\")\n",
    "\n",
    "df = df.loc[:, df.columns != 'Perform']\n",
    "df = df.loc[:, df.columns != 'Group']\n",
    "\n",
    "\n",
    "df_x = df.loc[:, df.columns != 'Class']\n",
    "df_y = df['Class']\n",
    "\n",
    "X_train = df_x.copy()\n",
    "y_train =  df_y.copy()\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2,shuffle=True, stratify=df_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006456f7-eecd-4486-abb2-db0ff3ef51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.loc[:, ~X_train.columns.isin(['Group'])].columns.to_list()\n",
    "X_train[numeric_columns] = X_train[numeric_columns].replace(\n",
    "    {\n",
    "        'NA': np.nan, \n",
    "        '': np.nan,\n",
    "        ' ': np.nan\n",
    "    }\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88066b03-5993-437f-9af7-d0f10d3bf8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>I14</th>\n",
       "      <th>I15</th>\n",
       "      <th>I16</th>\n",
       "      <th>I17</th>\n",
       "      <th>I18</th>\n",
       "      <th>I19</th>\n",
       "      <th>I20</th>\n",
       "      <th>I21</th>\n",
       "      <th>I22</th>\n",
       "      <th>I23</th>\n",
       "      <th>I24</th>\n",
       "      <th>I25</th>\n",
       "      <th>I26</th>\n",
       "      <th>I27</th>\n",
       "      <th>I28</th>\n",
       "      <th>I29</th>\n",
       "      <th>I30</th>\n",
       "      <th>I31</th>\n",
       "      <th>I32</th>\n",
       "      <th>I33</th>\n",
       "      <th>I34</th>\n",
       "      <th>I35</th>\n",
       "      <th>I36</th>\n",
       "      <th>I37</th>\n",
       "      <th>I38</th>\n",
       "      <th>I39</th>\n",
       "      <th>I40</th>\n",
       "      <th>I41</th>\n",
       "      <th>I42</th>\n",
       "      <th>I43</th>\n",
       "      <th>I44</th>\n",
       "      <th>I45</th>\n",
       "      <th>I46</th>\n",
       "      <th>I47</th>\n",
       "      <th>I48</th>\n",
       "      <th>I49</th>\n",
       "      <th>I50</th>\n",
       "      <th>I51</th>\n",
       "      <th>I52</th>\n",
       "      <th>I53</th>\n",
       "      <th>I54</th>\n",
       "      <th>I55</th>\n",
       "      <th>I56</th>\n",
       "      <th>I57</th>\n",
       "      <th>I58</th>\n",
       "      <th>dI1</th>\n",
       "      <th>dI2</th>\n",
       "      <th>dI3</th>\n",
       "      <th>dI4</th>\n",
       "      <th>dI5</th>\n",
       "      <th>dI6</th>\n",
       "      <th>dI7</th>\n",
       "      <th>dI8</th>\n",
       "      <th>dI9</th>\n",
       "      <th>dI10</th>\n",
       "      <th>dI11</th>\n",
       "      <th>dI12</th>\n",
       "      <th>dI13</th>\n",
       "      <th>dI14</th>\n",
       "      <th>dI15</th>\n",
       "      <th>dI16</th>\n",
       "      <th>dI17</th>\n",
       "      <th>dI18</th>\n",
       "      <th>dI19</th>\n",
       "      <th>dI20</th>\n",
       "      <th>dI21</th>\n",
       "      <th>dI22</th>\n",
       "      <th>dI23</th>\n",
       "      <th>dI24</th>\n",
       "      <th>dI25</th>\n",
       "      <th>dI26</th>\n",
       "      <th>dI27</th>\n",
       "      <th>dI28</th>\n",
       "      <th>dI29</th>\n",
       "      <th>dI30</th>\n",
       "      <th>dI31</th>\n",
       "      <th>dI32</th>\n",
       "      <th>dI33</th>\n",
       "      <th>dI34</th>\n",
       "      <th>dI35</th>\n",
       "      <th>dI36</th>\n",
       "      <th>dI37</th>\n",
       "      <th>dI38</th>\n",
       "      <th>dI39</th>\n",
       "      <th>dI40</th>\n",
       "      <th>dI41</th>\n",
       "      <th>dI42</th>\n",
       "      <th>dI43</th>\n",
       "      <th>dI44</th>\n",
       "      <th>dI45</th>\n",
       "      <th>dI46</th>\n",
       "      <th>dI47</th>\n",
       "      <th>dI48</th>\n",
       "      <th>dI49</th>\n",
       "      <th>dI50</th>\n",
       "      <th>dI51</th>\n",
       "      <th>dI52</th>\n",
       "      <th>dI53</th>\n",
       "      <th>dI54</th>\n",
       "      <th>dI55</th>\n",
       "      <th>dI56</th>\n",
       "      <th>dI57</th>\n",
       "      <th>dI58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.136495</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.037772</td>\n",
       "      <td>-0.232459</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>-0.187506</td>\n",
       "      <td>-0.322545</td>\n",
       "      <td>-0.043743</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.033105</td>\n",
       "      <td>0.303035</td>\n",
       "      <td>-0.093811</td>\n",
       "      <td>-0.598917</td>\n",
       "      <td>-0.271292</td>\n",
       "      <td>-0.256749</td>\n",
       "      <td>-0.100146</td>\n",
       "      <td>-0.045525</td>\n",
       "      <td>-0.078422</td>\n",
       "      <td>-0.060129</td>\n",
       "      <td>-0.069528</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.114432</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.342845</td>\n",
       "      <td>-0.159417</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.163287</td>\n",
       "      <td>-0.080599</td>\n",
       "      <td>-0.828880</td>\n",
       "      <td>-1.064215</td>\n",
       "      <td>-0.547067</td>\n",
       "      <td>-0.540497</td>\n",
       "      <td>-0.676045</td>\n",
       "      <td>-0.305007</td>\n",
       "      <td>-0.507724</td>\n",
       "      <td>-0.191437</td>\n",
       "      <td>-0.087362</td>\n",
       "      <td>-0.856151</td>\n",
       "      <td>0.802525</td>\n",
       "      <td>0.733080</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.533290</td>\n",
       "      <td>0.195197</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>-0.228889</td>\n",
       "      <td>-0.150821</td>\n",
       "      <td>-0.104986</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.188312</td>\n",
       "      <td>-0.250701</td>\n",
       "      <td>-0.101190</td>\n",
       "      <td>-0.357521</td>\n",
       "      <td>-0.527956</td>\n",
       "      <td>0.611385</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>-0.055733</td>\n",
       "      <td>-0.065709</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>-0.079805</td>\n",
       "      <td>0.178280</td>\n",
       "      <td>0.078155</td>\n",
       "      <td>0.072802</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.211770</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>-0.188447</td>\n",
       "      <td>0.117769</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>-0.024223</td>\n",
       "      <td>0.103204</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.004447</td>\n",
       "      <td>0.148967</td>\n",
       "      <td>-0.018521</td>\n",
       "      <td>-0.014110</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>-0.002369</td>\n",
       "      <td>-0.120036</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>-0.215571</td>\n",
       "      <td>-0.021999</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.012120</td>\n",
       "      <td>-0.040172</td>\n",
       "      <td>-0.060103</td>\n",
       "      <td>-0.059464</td>\n",
       "      <td>-0.044899</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>-0.003106</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>-0.002339</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.411684</td>\n",
       "      <td>0.073090</td>\n",
       "      <td>0.526222</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>-0.020476</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.077110</td>\n",
       "      <td>0.102563</td>\n",
       "      <td>0.188481</td>\n",
       "      <td>-0.016027</td>\n",
       "      <td>-0.135451</td>\n",
       "      <td>-0.189667</td>\n",
       "      <td>0.250967</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>-0.004265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.714522</td>\n",
       "      <td>-0.042137</td>\n",
       "      <td>-0.052968</td>\n",
       "      <td>-0.796862</td>\n",
       "      <td>-0.018394</td>\n",
       "      <td>0.070102</td>\n",
       "      <td>-0.076321</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>-1.045521</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.792515</td>\n",
       "      <td>-1.082483</td>\n",
       "      <td>0.025798</td>\n",
       "      <td>-0.833652</td>\n",
       "      <td>-0.625088</td>\n",
       "      <td>-0.333608</td>\n",
       "      <td>0.072579</td>\n",
       "      <td>-0.046963</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>-0.605902</td>\n",
       "      <td>-0.131099</td>\n",
       "      <td>-0.235929</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>-0.063247</td>\n",
       "      <td>-0.798768</td>\n",
       "      <td>-0.899983</td>\n",
       "      <td>1.388771</td>\n",
       "      <td>-0.248677</td>\n",
       "      <td>-0.058083</td>\n",
       "      <td>-0.014470</td>\n",
       "      <td>0.092095</td>\n",
       "      <td>0.561368</td>\n",
       "      <td>0.224819</td>\n",
       "      <td>0.223190</td>\n",
       "      <td>0.098852</td>\n",
       "      <td>-0.128227</td>\n",
       "      <td>-0.215876</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>-0.035260</td>\n",
       "      <td>-0.123911</td>\n",
       "      <td>-0.089751</td>\n",
       "      <td>-0.094963</td>\n",
       "      <td>0.362818</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>-1.506356</td>\n",
       "      <td>-0.573679</td>\n",
       "      <td>-0.955222</td>\n",
       "      <td>-0.818880</td>\n",
       "      <td>-1.063295</td>\n",
       "      <td>-1.022679</td>\n",
       "      <td>-1.336188</td>\n",
       "      <td>-0.612039</td>\n",
       "      <td>-0.061357</td>\n",
       "      <td>-0.482805</td>\n",
       "      <td>-0.017077</td>\n",
       "      <td>1.192135</td>\n",
       "      <td>-0.114981</td>\n",
       "      <td>-0.028074</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.002288</td>\n",
       "      <td>-0.045597</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>-0.081924</td>\n",
       "      <td>-0.033862</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.261836</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.045046</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>-0.199892</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>-0.024160</td>\n",
       "      <td>-0.037420</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>-0.106893</td>\n",
       "      <td>-0.394834</td>\n",
       "      <td>-0.132496</td>\n",
       "      <td>-0.027354</td>\n",
       "      <td>-0.129804</td>\n",
       "      <td>-0.066157</td>\n",
       "      <td>-0.494334</td>\n",
       "      <td>0.123781</td>\n",
       "      <td>0.284328</td>\n",
       "      <td>0.281308</td>\n",
       "      <td>0.212767</td>\n",
       "      <td>0.192042</td>\n",
       "      <td>0.146926</td>\n",
       "      <td>-0.118826</td>\n",
       "      <td>-0.039203</td>\n",
       "      <td>-0.256107</td>\n",
       "      <td>0.176622</td>\n",
       "      <td>0.168840</td>\n",
       "      <td>0.487752</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.039633</td>\n",
       "      <td>0.025667</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>-0.016375</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>-0.018790</td>\n",
       "      <td>-0.098543</td>\n",
       "      <td>0.317744</td>\n",
       "      <td>-0.180502</td>\n",
       "      <td>-0.009215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.104791</td>\n",
       "      <td>-0.038188</td>\n",
       "      <td>-0.053191</td>\n",
       "      <td>0.620233</td>\n",
       "      <td>0.148587</td>\n",
       "      <td>0.489875</td>\n",
       "      <td>0.319274</td>\n",
       "      <td>-0.060246</td>\n",
       "      <td>0.053174</td>\n",
       "      <td>-0.025008</td>\n",
       "      <td>-0.456840</td>\n",
       "      <td>1.284450</td>\n",
       "      <td>-0.133470</td>\n",
       "      <td>3.207672</td>\n",
       "      <td>2.373230</td>\n",
       "      <td>1.304427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.361293</td>\n",
       "      <td>2.995661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.188988</td>\n",
       "      <td>-0.044158</td>\n",
       "      <td>-0.024550</td>\n",
       "      <td>-0.586562</td>\n",
       "      <td>-0.176292</td>\n",
       "      <td>-1.013037</td>\n",
       "      <td>0.066912</td>\n",
       "      <td>0.219649</td>\n",
       "      <td>0.154490</td>\n",
       "      <td>2.370951</td>\n",
       "      <td>1.384675</td>\n",
       "      <td>0.489152</td>\n",
       "      <td>0.484715</td>\n",
       "      <td>0.367301</td>\n",
       "      <td>0.749572</td>\n",
       "      <td>0.669410</td>\n",
       "      <td>0.423228</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>3.227283</td>\n",
       "      <td>-0.329997</td>\n",
       "      <td>-0.327579</td>\n",
       "      <td>-1.033898</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.211889</td>\n",
       "      <td>-1.197156</td>\n",
       "      <td>2.860444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.584223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.272375</td>\n",
       "      <td>7.427558</td>\n",
       "      <td>-0.182816</td>\n",
       "      <td>-2.713205</td>\n",
       "      <td>-1.877595</td>\n",
       "      <td>-0.568691</td>\n",
       "      <td>0.224945</td>\n",
       "      <td>0.052749</td>\n",
       "      <td>0.377640</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.226060</td>\n",
       "      <td>0.207653</td>\n",
       "      <td>0.270327</td>\n",
       "      <td>0.283061</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.454366</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.188623</td>\n",
       "      <td>-0.265918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.063796</td>\n",
       "      <td>1.076458</td>\n",
       "      <td>0.240011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028327</td>\n",
       "      <td>1.764826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>-0.012626</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>0.056514</td>\n",
       "      <td>-0.100007</td>\n",
       "      <td>-0.216081</td>\n",
       "      <td>-0.127274</td>\n",
       "      <td>-0.056206</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>-0.011770</td>\n",
       "      <td>0.493157</td>\n",
       "      <td>0.487919</td>\n",
       "      <td>0.438576</td>\n",
       "      <td>0.574623</td>\n",
       "      <td>0.564379</td>\n",
       "      <td>-0.165933</td>\n",
       "      <td>-0.051256</td>\n",
       "      <td>0.410379</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.047592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020586</td>\n",
       "      <td>0.237539</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272937</td>\n",
       "      <td>0.774169</td>\n",
       "      <td>-0.007144</td>\n",
       "      <td>0.123954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.110103</td>\n",
       "      <td>0.186669</td>\n",
       "      <td>-0.030720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.532847</td>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.023377</td>\n",
       "      <td>1.306702</td>\n",
       "      <td>-0.068909</td>\n",
       "      <td>0.048024</td>\n",
       "      <td>-0.119481</td>\n",
       "      <td>-0.021057</td>\n",
       "      <td>-1.012916</td>\n",
       "      <td>-0.011783</td>\n",
       "      <td>1.206727</td>\n",
       "      <td>0.311773</td>\n",
       "      <td>-0.005928</td>\n",
       "      <td>3.869459</td>\n",
       "      <td>-1.064793</td>\n",
       "      <td>0.107702</td>\n",
       "      <td>-0.126984</td>\n",
       "      <td>-0.044360</td>\n",
       "      <td>-0.181023</td>\n",
       "      <td>-0.691971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.195138</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>-0.093976</td>\n",
       "      <td>-0.757725</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>-1.471299</td>\n",
       "      <td>0.643575</td>\n",
       "      <td>-0.067005</td>\n",
       "      <td>-0.006874</td>\n",
       "      <td>-0.087499</td>\n",
       "      <td>0.110638</td>\n",
       "      <td>0.046880</td>\n",
       "      <td>0.047141</td>\n",
       "      <td>-0.274713</td>\n",
       "      <td>0.169046</td>\n",
       "      <td>-0.179742</td>\n",
       "      <td>0.047391</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.105158</td>\n",
       "      <td>-0.045135</td>\n",
       "      <td>-0.051329</td>\n",
       "      <td>0.202098</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>2.904519</td>\n",
       "      <td>4.514844</td>\n",
       "      <td>-0.241111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.521576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.308812</td>\n",
       "      <td>-0.542532</td>\n",
       "      <td>-0.165028</td>\n",
       "      <td>1.490354</td>\n",
       "      <td>-1.550745</td>\n",
       "      <td>-0.918676</td>\n",
       "      <td>0.013484</td>\n",
       "      <td>-0.013198</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.194792</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>0.107880</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.136566</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>-0.286395</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>0.347297</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.068701</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>-0.003034</td>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.056340</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.063094</td>\n",
       "      <td>0.062424</td>\n",
       "      <td>0.057012</td>\n",
       "      <td>0.118399</td>\n",
       "      <td>0.116161</td>\n",
       "      <td>-0.017039</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.054025</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>-0.073937</td>\n",
       "      <td>0.764136</td>\n",
       "      <td>-0.076195</td>\n",
       "      <td>-0.114682</td>\n",
       "      <td>0.119667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.018494</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.029214</td>\n",
       "      <td>0.045747</td>\n",
       "      <td>-0.076884</td>\n",
       "      <td>-0.037859</td>\n",
       "      <td>-0.012046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.200815</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>-0.036754</td>\n",
       "      <td>-0.886675</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>-1.148744</td>\n",
       "      <td>0.152517</td>\n",
       "      <td>-0.043580</td>\n",
       "      <td>-0.935537</td>\n",
       "      <td>-0.023262</td>\n",
       "      <td>-0.908986</td>\n",
       "      <td>-0.525121</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>-0.347325</td>\n",
       "      <td>0.296360</td>\n",
       "      <td>-0.242201</td>\n",
       "      <td>0.120049</td>\n",
       "      <td>-0.048293</td>\n",
       "      <td>0.290658</td>\n",
       "      <td>-0.345816</td>\n",
       "      <td>0.249586</td>\n",
       "      <td>-0.241812</td>\n",
       "      <td>-0.082055</td>\n",
       "      <td>-0.077706</td>\n",
       "      <td>-0.845163</td>\n",
       "      <td>-0.257777</td>\n",
       "      <td>0.919065</td>\n",
       "      <td>-0.522102</td>\n",
       "      <td>0.146076</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>1.281726</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.135331</td>\n",
       "      <td>0.134652</td>\n",
       "      <td>0.654099</td>\n",
       "      <td>1.437536</td>\n",
       "      <td>1.995784</td>\n",
       "      <td>-0.145004</td>\n",
       "      <td>-0.029483</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.308723</td>\n",
       "      <td>0.293393</td>\n",
       "      <td>-0.527888</td>\n",
       "      <td>-0.003680</td>\n",
       "      <td>-1.553644</td>\n",
       "      <td>-1.233945</td>\n",
       "      <td>-0.947111</td>\n",
       "      <td>-0.926073</td>\n",
       "      <td>-0.772468</td>\n",
       "      <td>-0.636440</td>\n",
       "      <td>-0.833875</td>\n",
       "      <td>-0.527935</td>\n",
       "      <td>-0.014170</td>\n",
       "      <td>-0.142943</td>\n",
       "      <td>1.070523</td>\n",
       "      <td>-0.284682</td>\n",
       "      <td>-0.155110</td>\n",
       "      <td>-0.026941</td>\n",
       "      <td>0.480767</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>-0.003234</td>\n",
       "      <td>-0.041412</td>\n",
       "      <td>0.112513</td>\n",
       "      <td>-0.157224</td>\n",
       "      <td>-0.146180</td>\n",
       "      <td>-0.014677</td>\n",
       "      <td>-0.451950</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>-0.114443</td>\n",
       "      <td>-0.307095</td>\n",
       "      <td>-0.346711</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>-0.508920</td>\n",
       "      <td>-0.096666</td>\n",
       "      <td>0.044162</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.085082</td>\n",
       "      <td>0.254664</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.015390</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>-0.012542</td>\n",
       "      <td>-0.101059</td>\n",
       "      <td>0.091145</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>0.112377</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>0.731570</td>\n",
       "      <td>0.050165</td>\n",
       "      <td>0.038419</td>\n",
       "      <td>0.038011</td>\n",
       "      <td>0.265998</td>\n",
       "      <td>1.614120</td>\n",
       "      <td>1.806955</td>\n",
       "      <td>-0.122743</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>0.126103</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.618027</td>\n",
       "      <td>-1.599633</td>\n",
       "      <td>0.032793</td>\n",
       "      <td>-0.126733</td>\n",
       "      <td>-0.163593</td>\n",
       "      <td>-0.225889</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>-0.080892</td>\n",
       "      <td>-0.095963</td>\n",
       "      <td>-0.014812</td>\n",
       "      <td>-0.324584</td>\n",
       "      <td>-0.019002</td>\n",
       "      <td>-0.379323</td>\n",
       "      <td>-0.046024</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.011008</td>\n",
       "      <td>0.010496</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0  0.136495 -0.028429 -0.037772 -0.232459 -0.016222 -0.187506 -0.322545   \n",
       "1 -0.714522 -0.042137 -0.052968 -0.796862 -0.018394  0.070102 -0.076321   \n",
       "2  0.104791 -0.038188 -0.053191  0.620233  0.148587  0.489875  0.319274   \n",
       "3 -0.532847 -0.006582 -0.023377  1.306702 -0.068909  0.048024 -0.119481   \n",
       "4 -0.200815 -0.016334 -0.036754 -0.886675  0.484495 -1.148744  0.152517   \n",
       "\n",
       "         I8        I9       I10       I11       I12       I13       I14  \\\n",
       "0 -0.043743  0.125389 -0.014757 -0.033105  0.303035 -0.093811 -0.598917   \n",
       "1 -0.063864 -1.045521 -0.037353 -0.792515 -1.082483  0.025798 -0.833652   \n",
       "2 -0.060246  0.053174 -0.025008 -0.456840  1.284450 -0.133470  3.207672   \n",
       "3 -0.021057 -1.012916 -0.011783  1.206727  0.311773 -0.005928  3.869459   \n",
       "4 -0.043580 -0.935537 -0.023262 -0.908986 -0.525121  0.015492 -0.347325   \n",
       "\n",
       "        I15       I16       I17       I18       I19       I20       I21  \\\n",
       "0 -0.271292 -0.256749 -0.100146 -0.045525 -0.078422 -0.060129 -0.069528   \n",
       "1 -0.625088 -0.333608  0.072579 -0.046963  0.223022 -0.605902 -0.131099   \n",
       "2  2.373230  1.304427       NaN       NaN -0.361293  2.995661       NaN   \n",
       "3 -1.064793  0.107702 -0.126984 -0.044360 -0.181023 -0.691971       NaN   \n",
       "4  0.296360 -0.242201  0.120049 -0.048293  0.290658 -0.345816  0.249586   \n",
       "\n",
       "        I22       I23       I24       I25       I26       I27       I28  \\\n",
       "0 -0.052432 -0.114432 -0.104989  0.342845 -0.159417  0.006772 -0.303193   \n",
       "1 -0.235929 -0.073920 -0.063247 -0.798768 -0.899983  1.388771 -0.248677   \n",
       "2 -0.188988 -0.044158 -0.024550 -0.586562 -0.176292 -1.013037  0.066912   \n",
       "3  0.195138 -0.104877 -0.093976 -0.757725  0.004432 -1.471299  0.643575   \n",
       "4 -0.241812 -0.082055 -0.077706 -0.845163 -0.257777  0.919065 -0.522102   \n",
       "\n",
       "        I29       I30       I31       I32       I33       I34       I35  \\\n",
       "0 -0.163287 -0.080599 -0.828880 -1.064215 -0.547067 -0.540497 -0.676045   \n",
       "1 -0.058083 -0.014470  0.092095  0.561368  0.224819  0.223190  0.098852   \n",
       "2  0.219649  0.154490  2.370951  1.384675  0.489152  0.484715  0.367301   \n",
       "3 -0.067005 -0.006874 -0.087499  0.110638  0.046880  0.047141 -0.274713   \n",
       "4  0.146076  0.043851  1.281726  0.039106  0.135331  0.134652  0.654099   \n",
       "\n",
       "        I36       I37       I38       I39       I40       I41       I42  \\\n",
       "0 -0.305007 -0.507724 -0.191437 -0.087362 -0.856151  0.802525  0.733080   \n",
       "1 -0.128227 -0.215876 -0.007164 -0.035260 -0.123911 -0.089751 -0.094963   \n",
       "2  0.749572  0.669410  0.423228  0.226897  3.227283 -0.329997 -0.327579   \n",
       "3  0.169046 -0.179742  0.047391  0.015197  0.105158 -0.045135 -0.051329   \n",
       "4  1.437536  1.995784 -0.145004 -0.029483  0.252151  0.308723  0.293393   \n",
       "\n",
       "        I43       I44       I45       I46       I47       I48       I49  \\\n",
       "0  0.006512  0.533290  0.195197  0.058094 -0.228889 -0.150821 -0.104986   \n",
       "1  0.362818  0.011107 -1.506356 -0.573679 -0.955222 -0.818880 -1.063295   \n",
       "2 -1.033898  0.014531  0.211889 -1.197156  2.860444       NaN  3.584223   \n",
       "3  0.202098  0.034693  2.904519  4.514844 -0.241111       NaN -0.521576   \n",
       "4 -0.527888 -0.003680 -1.553644 -1.233945 -0.947111 -0.926073 -0.772468   \n",
       "\n",
       "        I50       I51       I52       I53       I54       I55       I56  \\\n",
       "0 -0.026743  0.188312 -0.250701 -0.101190 -0.357521 -0.527956  0.611385   \n",
       "1 -1.022679 -1.336188 -0.612039 -0.061357 -0.482805 -0.017077  1.192135   \n",
       "2       NaN  1.272375  7.427558 -0.182816 -2.713205 -1.877595 -0.568691   \n",
       "3       NaN -0.308812 -0.542532 -0.165028  1.490354 -1.550745 -0.918676   \n",
       "4 -0.636440 -0.833875 -0.527935 -0.014170 -0.142943  1.070523 -0.284682   \n",
       "\n",
       "        I57       I58       dI1       dI2       dI3       dI4       dI5  \\\n",
       "0 -0.092714 -0.055733 -0.065709 -0.002144 -0.004367 -0.079805  0.178280   \n",
       "1 -0.114981 -0.028074 -0.004451 -0.000536 -0.002288 -0.045597 -0.080639   \n",
       "2  0.224945  0.052749  0.377640  0.002656  0.001226  0.226060  0.207653   \n",
       "3  0.013484 -0.013198  0.050586  0.010356  0.007522  0.194792  0.010436   \n",
       "4 -0.155110 -0.026941  0.480767  0.021831 -0.003234 -0.041412  0.112513   \n",
       "\n",
       "        dI6       dI7       dI8       dI9      dI10      dI11      dI12  \\\n",
       "0  0.078155  0.072802  0.002090  0.211770 -0.003073 -0.188447  0.117769   \n",
       "1 -0.081924 -0.033862 -0.005111 -0.261836  0.000122 -0.045046  0.999854   \n",
       "2  0.270327  0.283061  0.002934  0.454366  0.004264  0.188623 -0.265918   \n",
       "3  0.107880  0.122549  0.017641  0.136566  0.010365  0.086853 -0.286395   \n",
       "4 -0.157224 -0.146180 -0.014677 -0.451950  0.034598 -0.114443 -0.307095   \n",
       "\n",
       "       dI13      dI14      dI15      dI16      dI17      dI18      dI19  \\\n",
       "0  0.001613 -0.024223  0.103204  0.032484  0.002688  0.000765 -0.004447   \n",
       "1 -0.008835 -0.122379 -0.199892  0.013615  0.014404 -0.000405  0.021573   \n",
       "2  0.000000  2.063796  1.076458  0.240011       NaN       NaN -0.028327   \n",
       "3 -0.014883  0.347297  0.017765  0.068701  0.015540  0.000208  0.016119   \n",
       "4 -0.346711  0.104144 -0.508920 -0.096666  0.044162  0.000159  0.085082   \n",
       "\n",
       "       dI20      dI21      dI22      dI23      dI24      dI25      dI26  \\\n",
       "0  0.148967 -0.018521 -0.014110 -0.001996 -0.002369 -0.120036  0.013172   \n",
       "1 -0.024160 -0.037420 -0.012610  0.003007  0.003617 -0.106893 -0.394834   \n",
       "2  1.764826       NaN  0.005847 -0.011166 -0.012626 -0.010822  0.056514   \n",
       "3  0.003992       NaN  0.043909 -0.000107  0.000099 -0.003895  0.002490   \n",
       "4  0.254664 -0.000408 -0.015390 -0.006226 -0.012542 -0.101059  0.091145   \n",
       "\n",
       "       dI27      dI28      dI29      dI30      dI31      dI32      dI33  \\\n",
       "0 -0.215571 -0.021999  0.001728 -0.000050 -0.012120 -0.040172 -0.060103   \n",
       "1 -0.132496 -0.027354 -0.129804 -0.066157 -0.494334  0.123781  0.284328   \n",
       "2 -0.100007 -0.216081 -0.127274 -0.056206  0.175751 -0.011770  0.493157   \n",
       "3 -0.003034 -0.015845  0.002377  0.001974  0.056340  0.010802  0.063094   \n",
       "4  0.282110 -0.005348  0.112377  0.036976  0.731570  0.050165  0.038419   \n",
       "\n",
       "       dI34      dI35      dI36      dI37      dI38      dI39      dI40  \\\n",
       "0 -0.059464 -0.044899  0.015735  0.022919 -0.003106  0.001233 -0.002339   \n",
       "1  0.281308  0.212767  0.192042  0.146926 -0.118826 -0.039203 -0.256107   \n",
       "2  0.487919  0.438576  0.574623  0.564379 -0.165933 -0.051256  0.410379   \n",
       "3  0.062424  0.057012  0.118399  0.116161 -0.017039  0.000839  0.054025   \n",
       "4  0.038011  0.265998  1.614120  1.806955 -0.122743 -0.001985  0.126103   \n",
       "\n",
       "       dI41      dI42      dI43      dI44      dI45      dI46      dI47  \\\n",
       "0  0.040628  0.411684  0.073090  0.526222  0.071060 -0.019531  0.359889   \n",
       "1  0.176622  0.168840  0.487752  0.029464  0.014232  0.039633  0.025667   \n",
       "2  0.056624  0.047592  0.000000 -0.020586  0.237539  0.017314  0.516667   \n",
       "3  0.030561  0.006389 -0.073937  0.764136 -0.076195 -0.114682  0.119667   \n",
       "4  0.630259  0.618027 -1.599633  0.032793 -0.126733 -0.163593 -0.225889   \n",
       "\n",
       "       dI48      dI49      dI50      dI51      dI52      dI53      dI54  \\\n",
       "0 -0.020476  0.057151  0.077110  0.102563  0.188481 -0.016027 -0.135451   \n",
       "1  0.006626  0.005180  0.006128 -0.016375  0.020727 -0.006525 -0.018790   \n",
       "2       NaN  0.404158       NaN  0.272937  0.774169 -0.007144  0.123954   \n",
       "3       NaN  0.001799       NaN  0.004938  0.018494 -0.003350 -0.029214   \n",
       "4 -0.026460 -0.080892 -0.095963 -0.014812 -0.324584 -0.019002 -0.379323   \n",
       "\n",
       "       dI55      dI56      dI57      dI58  \n",
       "0 -0.189667  0.250967  0.022171 -0.004265  \n",
       "1 -0.098543  0.317744 -0.180502 -0.009215  \n",
       "2  0.000000 -0.110103  0.186669 -0.030720  \n",
       "3  0.045747 -0.076884 -0.037859 -0.012046  \n",
       "4 -0.046024  0.282145  0.011008  0.010496  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0384-cbe0-44f2-8213-eb9593d584ad",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecc89f2-c2ee-4ea5-8684-821d96bb9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 116)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852d5c1-cd68-4fb6-a691-4436538012a3",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346abda1-a979-431a-b6a2-0219de1a8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    #('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000)) #hidden_layer_sizes=(20,20) for 2 hidden layers with 20 neurons each\n",
    "]\n",
    "\n",
    "vote_model = VotingClassifier(\n",
    "    estimators=voting_estimators, \n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000))\n",
    "]\n",
    "meta_stack_classifier = LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=stacking_estimators, \n",
    "    final_estimator=meta_stack_classifier, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    #('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME'))\n",
    "    #('Voting', vote_model),\n",
    "    #('Stacking', stacking_model),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000)) # 2 hidden layers with 20 neurons each\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b2aa3-2279-4c45-be46-3fa5b9668c35",
   "metadata": {},
   "source": [
    "# Create Pipeline with different combination of preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665f57b-10f3-482f-b28b-1b4fc3a5b25e",
   "metadata": {},
   "source": [
    "## Combination 6\n",
    "#### knn imputation, standard scaling, RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbdeca9f-8cf0-4c24-aa5d-8ba07d56c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8163e1ee-245b-4756-8baf-90c53beb631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Imputing Missing Values\n",
    "X_trainP, imp = handle_missing_vals_knn(\n",
    "    X_train, \n",
    "    n_neighbors=5\n",
    ")\n",
    "\n",
    "objs['miss'] = imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0cce5c68-50fc-4e82-8433-75619c28674a",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### Standard Scaler\n",
    "std_scale_cols = (\n",
    "    X_trainP\n",
    "    .loc[:, ~X_trainP.columns.str.contains('Group')]\n",
    "    .columns\n",
    ")\n",
    "\n",
    "X_trainP, std_scaler = apply_std_scaler(\n",
    "    X_trainP, \n",
    "    std_scale_cols\n",
    ")\n",
    "\n",
    "objs['scaler'] = std_scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080c298f-dbe4-4eb3-af13-c788fc03012d",
   "metadata": {},
   "source": [
    "### Select k best with F score, with 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a179444a-a424-4630-b336-e30c85d8e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainP, important_features = select_k_best(X_trainP, y_train, f_classif, 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1fdbb-5b7a-43b3-83af-7b48bee5c3b4",
   "metadata": {},
   "source": [
    "# Apply cross validation with f1, auc and accuracy\n",
    "\n",
    "#### *Weighted F1 Score: F1 score calculated by taking the average of F1 scores for each class. Average is weighted by support which is the number of true instances for each label. \n",
    "#### *AUC One vs One Weighted: By considering all pairwise combinations of classes, average AUC is calculated. Average is weighted by the support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79197db7-33cc-42da-b582-ecf52eba91a6",
   "metadata": {},
   "source": [
    "## Create Class object and apply cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fcb2a67-1bbe-4f97-8660-0c76963e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cost_matrix = np.array([[0, 1, 2],\n",
    "                              [1, 0, 1],\n",
    "                              [2, 1, 0]])\n",
    "model_selector = ModelSelection(\n",
    "    x_train=X_trainP, \n",
    "    y_train=y_train,\n",
    "    estimators=estimators,\n",
    "    cost_matrix=error_cost_matrix\n",
    ")\n",
    "model_selector.encode_y_train()\n",
    "model_selector.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "255d6131-2636-464d-91f0-d2cee3a3f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1, 1: 0, 2: 1}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selector.target_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ec19b29-b2f3-4d57-9c58-fbfe15b92c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Mean F1 Score:\n",
      "\n",
      "RandomForest = 0.432803\n",
      "GradientBoost = 0.433581\n",
      "XGBoost = 0.430447\n",
      "AdaBoost = 0.420735\n",
      "\n",
      "Best Estimator (F1): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean AUC Score:\n",
      "\n",
      "RandomForest = 0.538182\n",
      "GradientBoost = 0.543286\n",
      "XGBoost = 0.532127\n",
      "AdaBoost = 0.535623\n",
      "\n",
      "Best Estimator (AUC): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean Accuracy Score:\n",
      "\n",
      "RandomForest = 0.468750\n",
      "GradientBoost = 0.482125\n",
      "XGBoost = 0.459625\n",
      "AdaBoost = 0.484000\n",
      "\n",
      "Best Estimator (Accuracy): AdaBoost\n",
      "**********************************************\n",
      "CV Results for Cost Matrix Error Score:\n",
      "\n",
      "RandomForest = 0.913875\n",
      "GradientBoost = 0.889750\n",
      "XGBoost = 0.919875\n",
      "AdaBoost = 0.890000\n",
      "\n",
      "Best Estimator (Cost Metric Error Score): GradientBoost\n"
     ]
    }
   ],
   "source": [
    "model_selector.calculate_cv_f1(n_folds=5, scoring_average='f1_weighted')\n",
    "print('**********************************************')\n",
    "model_selector.calculate_cv_auc(n_folds=5, scoring_average='roc_auc_ovo_weighted')\n",
    "print('**********************************************')   \n",
    "model_selector.calculate_cv_accuracy(n_folds=5, scoring_average='accuracy')\n",
    "print('**********************************************')   \n",
    "model_selector.calculate_cost_matrix_error_cv(custom_cost_func=matrix_error_function, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff78202-7717-4a74-9c00-2bd9aac5fb64",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7f5076d9-0356-4356-86a4-f48bf781d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'ClassificationModel__n_estimators': [100, 200, 250],\n",
    "#     'ClassificationModel__max_depth': [3, 10, 20],\n",
    "#     'ClassificationModel__min_samples_split': [2, 10, 20],\n",
    "#     'ClassificationModel__learning_rate': [0.05, 0.1, 0.2],\n",
    "#     'ClassificationModel__random_state': [0]\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "78553300-8e68-4d61-bcb9-30d8b722128d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_selector.apply_grid_cv(\n",
    "#     estimator=GradientBoostingClassifier(),\n",
    "#     params=param_grid,\n",
    "#     cv=2,\n",
    "#     scoring=matrix_error_function\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a99075a-8db2-4799-8174-ed4fe4068d3a",
   "metadata": {},
   "source": [
    "### Predict on real dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "726643d5-4bec-41bb-a639-ee48dca3b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ######################################## Imputing Missing Values\n",
    "# X_test_compete = pd.DataFrame(objs['miss'].transform(X_test_compete), columns=X_test_compete.columns)\n",
    "# ######################################## Scaling\n",
    "# X_test_compete = pd.DataFrame(objs['scaler'].transform(X_test_compete[num_cols_test]), columns=num_cols_test).reset_index(drop=True)\n",
    "# ######################################## RFECV\n",
    "# subset_features_rfecv = X_trainP.columns.to_list()\n",
    "# X_test_compete = X_test_compete.loc[:, subset_features_rfecv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa79e9db-ddc1-40e1-a597-db00a813dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(grid_search2.grid_best_model.predict(X_test_compete)).replace(grid_search2.target_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2d7c604f-aae8-43d5-aa57-536dd27a7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"preds_comb4_5.txt\"\n",
    "preds.to_csv(file_path, index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81fb40ea-f364-4076-bc38-8dfd0defdaf4",
   "metadata": {},
   "source": [
    "### Select K-best with mutual_info_classif, with 40 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "11e73a51-d9ad-448d-9043-c98b0b593df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = dict()\n",
    "\n",
    "####################################### Imputing Missing Values\n",
    "X_trainP, imp = handle_missing_vals_knn(\n",
    "    X_train, \n",
    "    n_neighbors=5\n",
    ")\n",
    "\n",
    "objs['miss'] = imp\n",
    "\n",
    "####################################### Standard Scaler\n",
    "std_scale_cols = (\n",
    "    X_trainP\n",
    "    .loc[:, ~X_trainP.columns.str.contains('Group')]\n",
    "    .columns\n",
    ")\n",
    "\n",
    "X_trainP, std_scaler = apply_std_scaler(\n",
    "    X_trainP, \n",
    "    std_scale_cols\n",
    ")\n",
    "\n",
    "objs['scaler'] = std_scaler\n",
    "\n",
    "X_trainP, important_features = select_k_best(X_trainP, y_train, mutual_info_classif, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9c25ce9-1378-4633-8201-a11c9f9735cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector2 = ModelSelection(\n",
    "    x_train=X_trainP, \n",
    "    y_train=y_train,\n",
    "    estimators=estimators,\n",
    "    cost_matrix=error_cost_matrix\n",
    ")\n",
    "model_selector2.encode_y_train()\n",
    "model_selector2.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e3a5905-2bf1-4880-a3ba-68b3bb04344f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Mean F1 Score:\n",
      "\n",
      "RandomForest = 0.432847\n",
      "GradientBoost = 0.426424\n",
      "XGBoost = 0.438818\n",
      "AdaBoost = 0.409358\n",
      "\n",
      "Best Estimator (F1): XGBoost\n",
      "**********************************************\n",
      "CV Results for Mean AUC Score:\n",
      "\n",
      "RandomForest = 0.541991\n",
      "GradientBoost = 0.547004\n",
      "XGBoost = 0.539158\n",
      "AdaBoost = 0.530348\n",
      "\n",
      "Best Estimator (AUC): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean Accuracy Score:\n",
      "\n",
      "RandomForest = 0.470875\n",
      "GradientBoost = 0.474875\n",
      "XGBoost = 0.467500\n",
      "AdaBoost = 0.474125\n",
      "\n",
      "Best Estimator (Accuracy): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Cost Matrix Error Score:\n",
      "\n",
      "RandomForest = 0.911250\n",
      "GradientBoost = 0.903375\n",
      "XGBoost = 0.908125\n",
      "AdaBoost = 0.909750\n",
      "\n",
      "Best Estimator (Cost Metric Error Score): GradientBoost\n"
     ]
    }
   ],
   "source": [
    "model_selector2.calculate_cv_f1(n_folds=5, scoring_average='f1_weighted')\n",
    "print('**********************************************')\n",
    "model_selector2.calculate_cv_auc(n_folds=5, scoring_average='roc_auc_ovo_weighted')\n",
    "print('**********************************************')   \n",
    "model_selector2.calculate_cv_accuracy(n_folds=5, scoring_average='accuracy')\n",
    "print('**********************************************')   \n",
    "model_selector2.calculate_cost_matrix_error_cv(custom_cost_func=matrix_error_function, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00de105-cd5a-40fb-8059-b6efbc4c6423",
   "metadata": {},
   "source": [
    "### Select k best with F score, best 80 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1e10193b-dd3c-4894-aac7-1aba932cf33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = dict()\n",
    "\n",
    "####################################### Imputing Missing Values\n",
    "X_trainP, imp = handle_missing_vals_knn(\n",
    "    X_train, \n",
    "    n_neighbors=5\n",
    ")\n",
    "\n",
    "objs['miss'] = imp\n",
    "\n",
    "####################################### Standard Scaler\n",
    "std_scale_cols = (\n",
    "    X_trainP\n",
    "    .loc[:, ~X_trainP.columns.str.contains('Group')]\n",
    "    .columns\n",
    ")\n",
    "\n",
    "X_trainP, std_scaler = apply_std_scaler(\n",
    "    X_trainP, \n",
    "    std_scale_cols\n",
    ")\n",
    "\n",
    "objs['scaler'] = std_scaler\n",
    "\n",
    "X_trainP, important_features = select_k_best(X_trainP, y_train, mutual_info_classif, 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8bdf5958-9d25-447f-8174-4973d2bf2617",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_selector3 = ModelSelection(\n",
    "    x_train=X_trainP, \n",
    "    y_train=y_train,\n",
    "    estimators=estimators,\n",
    "    cost_matrix=error_cost_matrix\n",
    ")\n",
    "model_selector3.encode_y_train()\n",
    "model_selector3.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "321a2381-4d10-4d73-84ff-ac9082555cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Mean F1 Score:\n",
      "\n",
      "RandomForest = 0.437851\n",
      "GradientBoost = 0.435154\n",
      "XGBoost = 0.438532\n",
      "AdaBoost = 0.411039\n",
      "\n",
      "Best Estimator (F1): XGBoost\n",
      "**********************************************\n",
      "CV Results for Mean AUC Score:\n",
      "\n",
      "RandomForest = 0.543651\n",
      "GradientBoost = 0.550516\n",
      "XGBoost = 0.535113\n",
      "AdaBoost = 0.538311\n",
      "\n",
      "Best Estimator (AUC): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean Accuracy Score:\n",
      "\n",
      "RandomForest = 0.476250\n",
      "GradientBoost = 0.482000\n",
      "XGBoost = 0.469000\n",
      "AdaBoost = 0.477750\n",
      "\n",
      "Best Estimator (Accuracy): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Cost Matrix Error Score:\n",
      "\n",
      "RandomForest = 0.901500\n",
      "GradientBoost = 0.889500\n",
      "XGBoost = 0.904375\n",
      "AdaBoost = 0.902500\n",
      "\n",
      "Best Estimator (Cost Metric Error Score): GradientBoost\n"
     ]
    }
   ],
   "source": [
    "model_selector3.calculate_cv_f1(n_folds=5, scoring_average='f1_weighted')\n",
    "print('**********************************************')\n",
    "model_selector3.calculate_cv_auc(n_folds=5, scoring_average='roc_auc_ovo_weighted')\n",
    "print('**********************************************')   \n",
    "model_selector3.calculate_cv_accuracy(n_folds=5, scoring_average='accuracy')\n",
    "print('**********************************************')   \n",
    "model_selector3.calculate_cost_matrix_error_cv(custom_cost_func=matrix_error_function, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4112d2-284c-424b-8ae2-d2c3efcffd48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
