{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18872d5e-2ee7-4f1c-8374-c436aa9ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "from Functions_Classes import *\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "403b851f-60a3-4d92-838e-9d6f00c5cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/training_data.xls\")\n",
    "X_test_compete = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/test_data_no_target.xls\")\n",
    "\n",
    "df = df.loc[:, df.columns != 'Perform']\n",
    "df = df.loc[:, df.columns != 'Group']\n",
    "\n",
    "\n",
    "df_x = df.loc[:, df.columns != 'Class']\n",
    "df_y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.2,shuffle=True, stratify=df_y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "006456f7-eecd-4486-abb2-db0ff3ef51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = X_train.loc[:, ~X_train.columns.isin(['Group'])].columns.to_list()\n",
    "X_train[numeric_columns] = X_train[numeric_columns].replace(\n",
    "    {\n",
    "        'NA': np.nan, \n",
    "        '': np.nan,\n",
    "        ' ': np.nan\n",
    "    }\n",
    ").astype(float)\n",
    "\n",
    "X_test[numeric_columns] = X_test[numeric_columns].replace(\n",
    "    {\n",
    "        'NA': np.nan, \n",
    "        '': np.nan,\n",
    "        ' ': np.nan\n",
    "    }\n",
    ").astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88066b03-5993-437f-9af7-d0f10d3bf8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>I14</th>\n",
       "      <th>I15</th>\n",
       "      <th>I16</th>\n",
       "      <th>I17</th>\n",
       "      <th>I18</th>\n",
       "      <th>I19</th>\n",
       "      <th>I20</th>\n",
       "      <th>I21</th>\n",
       "      <th>I22</th>\n",
       "      <th>I23</th>\n",
       "      <th>I24</th>\n",
       "      <th>I25</th>\n",
       "      <th>I26</th>\n",
       "      <th>I27</th>\n",
       "      <th>I28</th>\n",
       "      <th>I29</th>\n",
       "      <th>I30</th>\n",
       "      <th>I31</th>\n",
       "      <th>I32</th>\n",
       "      <th>I33</th>\n",
       "      <th>I34</th>\n",
       "      <th>I35</th>\n",
       "      <th>I36</th>\n",
       "      <th>I37</th>\n",
       "      <th>I38</th>\n",
       "      <th>I39</th>\n",
       "      <th>I40</th>\n",
       "      <th>I41</th>\n",
       "      <th>I42</th>\n",
       "      <th>I43</th>\n",
       "      <th>I44</th>\n",
       "      <th>I45</th>\n",
       "      <th>I46</th>\n",
       "      <th>I47</th>\n",
       "      <th>I48</th>\n",
       "      <th>I49</th>\n",
       "      <th>I50</th>\n",
       "      <th>I51</th>\n",
       "      <th>I52</th>\n",
       "      <th>I53</th>\n",
       "      <th>I54</th>\n",
       "      <th>I55</th>\n",
       "      <th>I56</th>\n",
       "      <th>I57</th>\n",
       "      <th>I58</th>\n",
       "      <th>dI1</th>\n",
       "      <th>dI2</th>\n",
       "      <th>dI3</th>\n",
       "      <th>dI4</th>\n",
       "      <th>dI5</th>\n",
       "      <th>dI6</th>\n",
       "      <th>dI7</th>\n",
       "      <th>dI8</th>\n",
       "      <th>dI9</th>\n",
       "      <th>dI10</th>\n",
       "      <th>dI11</th>\n",
       "      <th>dI12</th>\n",
       "      <th>dI13</th>\n",
       "      <th>dI14</th>\n",
       "      <th>dI15</th>\n",
       "      <th>dI16</th>\n",
       "      <th>dI17</th>\n",
       "      <th>dI18</th>\n",
       "      <th>dI19</th>\n",
       "      <th>dI20</th>\n",
       "      <th>dI21</th>\n",
       "      <th>dI22</th>\n",
       "      <th>dI23</th>\n",
       "      <th>dI24</th>\n",
       "      <th>dI25</th>\n",
       "      <th>dI26</th>\n",
       "      <th>dI27</th>\n",
       "      <th>dI28</th>\n",
       "      <th>dI29</th>\n",
       "      <th>dI30</th>\n",
       "      <th>dI31</th>\n",
       "      <th>dI32</th>\n",
       "      <th>dI33</th>\n",
       "      <th>dI34</th>\n",
       "      <th>dI35</th>\n",
       "      <th>dI36</th>\n",
       "      <th>dI37</th>\n",
       "      <th>dI38</th>\n",
       "      <th>dI39</th>\n",
       "      <th>dI40</th>\n",
       "      <th>dI41</th>\n",
       "      <th>dI42</th>\n",
       "      <th>dI43</th>\n",
       "      <th>dI44</th>\n",
       "      <th>dI45</th>\n",
       "      <th>dI46</th>\n",
       "      <th>dI47</th>\n",
       "      <th>dI48</th>\n",
       "      <th>dI49</th>\n",
       "      <th>dI50</th>\n",
       "      <th>dI51</th>\n",
       "      <th>dI52</th>\n",
       "      <th>dI53</th>\n",
       "      <th>dI54</th>\n",
       "      <th>dI55</th>\n",
       "      <th>dI56</th>\n",
       "      <th>dI57</th>\n",
       "      <th>dI58</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7426</th>\n",
       "      <td>0.474336</td>\n",
       "      <td>0.010288</td>\n",
       "      <td>-0.000710</td>\n",
       "      <td>-0.606795</td>\n",
       "      <td>-0.132097</td>\n",
       "      <td>0.227654</td>\n",
       "      <td>0.407601</td>\n",
       "      <td>-0.002148</td>\n",
       "      <td>0.255976</td>\n",
       "      <td>0.010343</td>\n",
       "      <td>0.069800</td>\n",
       "      <td>-0.042908</td>\n",
       "      <td>-0.031332</td>\n",
       "      <td>-0.039776</td>\n",
       "      <td>1.244465</td>\n",
       "      <td>-0.281740</td>\n",
       "      <td>-0.034615</td>\n",
       "      <td>-0.046939</td>\n",
       "      <td>0.046508</td>\n",
       "      <td>-0.189993</td>\n",
       "      <td>-0.042645</td>\n",
       "      <td>-0.201074</td>\n",
       "      <td>-0.061445</td>\n",
       "      <td>-0.044316</td>\n",
       "      <td>-0.571520</td>\n",
       "      <td>-0.145953</td>\n",
       "      <td>2.143646</td>\n",
       "      <td>-0.514716</td>\n",
       "      <td>-0.097087</td>\n",
       "      <td>-0.035054</td>\n",
       "      <td>0.599826</td>\n",
       "      <td>1.021227</td>\n",
       "      <td>1.567838</td>\n",
       "      <td>1.551942</td>\n",
       "      <td>1.468628</td>\n",
       "      <td>1.229721</td>\n",
       "      <td>1.321067</td>\n",
       "      <td>-0.084339</td>\n",
       "      <td>-0.042190</td>\n",
       "      <td>0.476034</td>\n",
       "      <td>0.192107</td>\n",
       "      <td>0.178743</td>\n",
       "      <td>0.425832</td>\n",
       "      <td>0.051295</td>\n",
       "      <td>-1.429454</td>\n",
       "      <td>-0.915034</td>\n",
       "      <td>-0.818222</td>\n",
       "      <td>-0.859571</td>\n",
       "      <td>-0.831964</td>\n",
       "      <td>-0.737321</td>\n",
       "      <td>-0.953812</td>\n",
       "      <td>-0.423662</td>\n",
       "      <td>0.167951</td>\n",
       "      <td>0.982461</td>\n",
       "      <td>1.507053</td>\n",
       "      <td>-0.872424</td>\n",
       "      <td>-0.117816</td>\n",
       "      <td>-0.031497</td>\n",
       "      <td>-0.008751</td>\n",
       "      <td>0.009108</td>\n",
       "      <td>0.008876</td>\n",
       "      <td>-0.008488</td>\n",
       "      <td>-0.122003</td>\n",
       "      <td>-0.150909</td>\n",
       "      <td>-0.168123</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>-0.143467</td>\n",
       "      <td>0.005262</td>\n",
       "      <td>-0.017017</td>\n",
       "      <td>-0.045596</td>\n",
       "      <td>0.007570</td>\n",
       "      <td>0.026585</td>\n",
       "      <td>-0.414900</td>\n",
       "      <td>-0.012459</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>-0.000270</td>\n",
       "      <td>0.020325</td>\n",
       "      <td>-0.037350</td>\n",
       "      <td>0.034140</td>\n",
       "      <td>0.004292</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.036921</td>\n",
       "      <td>-0.025854</td>\n",
       "      <td>0.210788</td>\n",
       "      <td>0.002333</td>\n",
       "      <td>-0.046517</td>\n",
       "      <td>-0.028944</td>\n",
       "      <td>-0.756167</td>\n",
       "      <td>-0.166527</td>\n",
       "      <td>-0.281711</td>\n",
       "      <td>-0.278719</td>\n",
       "      <td>-0.292282</td>\n",
       "      <td>-0.167453</td>\n",
       "      <td>-0.215679</td>\n",
       "      <td>-0.088958</td>\n",
       "      <td>-0.036857</td>\n",
       "      <td>-0.895317</td>\n",
       "      <td>0.229733</td>\n",
       "      <td>0.222789</td>\n",
       "      <td>0.529174</td>\n",
       "      <td>0.039070</td>\n",
       "      <td>0.005165</td>\n",
       "      <td>0.065233</td>\n",
       "      <td>0.024111</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>-0.040331</td>\n",
       "      <td>-0.037055</td>\n",
       "      <td>-0.049063</td>\n",
       "      <td>0.009558</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>-0.044080</td>\n",
       "      <td>0.118029</td>\n",
       "      <td>-0.006498</td>\n",
       "      <td>-0.015567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4264</th>\n",
       "      <td>0.735381</td>\n",
       "      <td>-0.019519</td>\n",
       "      <td>-0.029400</td>\n",
       "      <td>1.373814</td>\n",
       "      <td>-0.242825</td>\n",
       "      <td>0.006498</td>\n",
       "      <td>-0.218154</td>\n",
       "      <td>-0.029641</td>\n",
       "      <td>1.226153</td>\n",
       "      <td>-0.014224</td>\n",
       "      <td>0.696016</td>\n",
       "      <td>0.219364</td>\n",
       "      <td>-0.037933</td>\n",
       "      <td>0.711385</td>\n",
       "      <td>0.669523</td>\n",
       "      <td>-0.165935</td>\n",
       "      <td>-0.110332</td>\n",
       "      <td>-0.044059</td>\n",
       "      <td>-0.110881</td>\n",
       "      <td>0.283741</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006747</td>\n",
       "      <td>-0.100089</td>\n",
       "      <td>-0.088502</td>\n",
       "      <td>0.737501</td>\n",
       "      <td>-0.100008</td>\n",
       "      <td>0.507143</td>\n",
       "      <td>1.584636</td>\n",
       "      <td>-0.123937</td>\n",
       "      <td>-0.048261</td>\n",
       "      <td>-0.549628</td>\n",
       "      <td>-0.592205</td>\n",
       "      <td>-0.239093</td>\n",
       "      <td>-0.235795</td>\n",
       "      <td>-0.505718</td>\n",
       "      <td>-0.186124</td>\n",
       "      <td>-0.501709</td>\n",
       "      <td>-0.046078</td>\n",
       "      <td>-0.036884</td>\n",
       "      <td>-0.457416</td>\n",
       "      <td>0.115378</td>\n",
       "      <td>0.104334</td>\n",
       "      <td>0.187516</td>\n",
       "      <td>0.077230</td>\n",
       "      <td>0.179328</td>\n",
       "      <td>-0.317715</td>\n",
       "      <td>0.771000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.230770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.487625</td>\n",
       "      <td>0.532675</td>\n",
       "      <td>-0.113977</td>\n",
       "      <td>-0.502087</td>\n",
       "      <td>-0.072377</td>\n",
       "      <td>-0.530395</td>\n",
       "      <td>-0.119678</td>\n",
       "      <td>-0.035370</td>\n",
       "      <td>-0.020779</td>\n",
       "      <td>-0.001515</td>\n",
       "      <td>-0.001271</td>\n",
       "      <td>0.566549</td>\n",
       "      <td>0.112948</td>\n",
       "      <td>0.148753</td>\n",
       "      <td>0.146571</td>\n",
       "      <td>0.010503</td>\n",
       "      <td>0.791676</td>\n",
       "      <td>-0.004064</td>\n",
       "      <td>0.011731</td>\n",
       "      <td>0.351006</td>\n",
       "      <td>0.012634</td>\n",
       "      <td>-0.178190</td>\n",
       "      <td>0.477856</td>\n",
       "      <td>0.039291</td>\n",
       "      <td>-0.003576</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>-0.013169</td>\n",
       "      <td>0.026280</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>-0.000972</td>\n",
       "      <td>-0.001111</td>\n",
       "      <td>-0.055436</td>\n",
       "      <td>-0.005346</td>\n",
       "      <td>-0.177226</td>\n",
       "      <td>-0.033158</td>\n",
       "      <td>0.017723</td>\n",
       "      <td>0.011541</td>\n",
       "      <td>0.098352</td>\n",
       "      <td>0.119397</td>\n",
       "      <td>0.039128</td>\n",
       "      <td>0.038712</td>\n",
       "      <td>0.037939</td>\n",
       "      <td>0.013403</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.045437</td>\n",
       "      <td>0.016490</td>\n",
       "      <td>0.133769</td>\n",
       "      <td>-0.149494</td>\n",
       "      <td>-0.144975</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.077788</td>\n",
       "      <td>0.149001</td>\n",
       "      <td>-0.015912</td>\n",
       "      <td>0.704333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.135784</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.193875</td>\n",
       "      <td>0.478701</td>\n",
       "      <td>-0.024174</td>\n",
       "      <td>-0.148020</td>\n",
       "      <td>-0.161484</td>\n",
       "      <td>-0.100311</td>\n",
       "      <td>0.069675</td>\n",
       "      <td>-0.004375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5670</th>\n",
       "      <td>0.150030</td>\n",
       "      <td>-0.030559</td>\n",
       "      <td>-0.041371</td>\n",
       "      <td>-0.180490</td>\n",
       "      <td>0.023586</td>\n",
       "      <td>-0.143670</td>\n",
       "      <td>-0.301218</td>\n",
       "      <td>-0.039838</td>\n",
       "      <td>0.782315</td>\n",
       "      <td>-0.021184</td>\n",
       "      <td>-0.104858</td>\n",
       "      <td>0.603132</td>\n",
       "      <td>-0.063976</td>\n",
       "      <td>0.786947</td>\n",
       "      <td>0.997630</td>\n",
       "      <td>-0.155514</td>\n",
       "      <td>-0.070871</td>\n",
       "      <td>-0.045427</td>\n",
       "      <td>-0.083950</td>\n",
       "      <td>0.062908</td>\n",
       "      <td>-0.163518</td>\n",
       "      <td>-0.015327</td>\n",
       "      <td>-0.072948</td>\n",
       "      <td>-0.057468</td>\n",
       "      <td>0.767850</td>\n",
       "      <td>0.173958</td>\n",
       "      <td>-0.128118</td>\n",
       "      <td>-0.310119</td>\n",
       "      <td>-0.164138</td>\n",
       "      <td>-0.079906</td>\n",
       "      <td>-0.883528</td>\n",
       "      <td>-1.344381</td>\n",
       "      <td>-0.716429</td>\n",
       "      <td>-0.708061</td>\n",
       "      <td>-0.897027</td>\n",
       "      <td>-0.443576</td>\n",
       "      <td>-0.724724</td>\n",
       "      <td>-0.170219</td>\n",
       "      <td>-0.079356</td>\n",
       "      <td>-0.867710</td>\n",
       "      <td>0.626772</td>\n",
       "      <td>0.600270</td>\n",
       "      <td>0.559442</td>\n",
       "      <td>-0.010800</td>\n",
       "      <td>0.906108</td>\n",
       "      <td>-0.712064</td>\n",
       "      <td>0.854778</td>\n",
       "      <td>1.128683</td>\n",
       "      <td>1.737216</td>\n",
       "      <td>1.107266</td>\n",
       "      <td>1.701688</td>\n",
       "      <td>1.132818</td>\n",
       "      <td>-0.104753</td>\n",
       "      <td>-0.537718</td>\n",
       "      <td>0.410211</td>\n",
       "      <td>0.797962</td>\n",
       "      <td>-0.112652</td>\n",
       "      <td>-0.057491</td>\n",
       "      <td>-1.246969</td>\n",
       "      <td>-0.017840</td>\n",
       "      <td>-0.014250</td>\n",
       "      <td>-0.627947</td>\n",
       "      <td>0.082116</td>\n",
       "      <td>-0.027240</td>\n",
       "      <td>0.043286</td>\n",
       "      <td>-0.003879</td>\n",
       "      <td>-0.281142</td>\n",
       "      <td>-0.029354</td>\n",
       "      <td>-1.055325</td>\n",
       "      <td>0.657791</td>\n",
       "      <td>0.038651</td>\n",
       "      <td>-0.156749</td>\n",
       "      <td>-0.189418</td>\n",
       "      <td>0.051770</td>\n",
       "      <td>0.036775</td>\n",
       "      <td>-0.000911</td>\n",
       "      <td>0.020150</td>\n",
       "      <td>-0.690801</td>\n",
       "      <td>-0.011164</td>\n",
       "      <td>-0.048206</td>\n",
       "      <td>-0.008495</td>\n",
       "      <td>-0.009713</td>\n",
       "      <td>-0.455277</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>-0.364856</td>\n",
       "      <td>-0.055686</td>\n",
       "      <td>0.034768</td>\n",
       "      <td>0.016951</td>\n",
       "      <td>0.031098</td>\n",
       "      <td>-0.228416</td>\n",
       "      <td>-0.394980</td>\n",
       "      <td>-0.390785</td>\n",
       "      <td>-0.315390</td>\n",
       "      <td>-0.323573</td>\n",
       "      <td>-0.280804</td>\n",
       "      <td>0.043982</td>\n",
       "      <td>0.018647</td>\n",
       "      <td>0.038100</td>\n",
       "      <td>-1.159936</td>\n",
       "      <td>-1.124875</td>\n",
       "      <td>0.182791</td>\n",
       "      <td>-0.007366</td>\n",
       "      <td>-0.049387</td>\n",
       "      <td>-0.078541</td>\n",
       "      <td>0.660222</td>\n",
       "      <td>-0.383559</td>\n",
       "      <td>0.192662</td>\n",
       "      <td>0.380037</td>\n",
       "      <td>0.006750</td>\n",
       "      <td>0.726753</td>\n",
       "      <td>-0.008247</td>\n",
       "      <td>0.087142</td>\n",
       "      <td>0.101860</td>\n",
       "      <td>-0.054802</td>\n",
       "      <td>-0.133887</td>\n",
       "      <td>0.001193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2110</th>\n",
       "      <td>-0.392277</td>\n",
       "      <td>-0.036396</td>\n",
       "      <td>-0.017427</td>\n",
       "      <td>0.000440</td>\n",
       "      <td>0.194470</td>\n",
       "      <td>0.427458</td>\n",
       "      <td>0.290762</td>\n",
       "      <td>-0.039829</td>\n",
       "      <td>-0.240505</td>\n",
       "      <td>-0.022131</td>\n",
       "      <td>-0.481441</td>\n",
       "      <td>1.195059</td>\n",
       "      <td>-0.133470</td>\n",
       "      <td>0.360923</td>\n",
       "      <td>0.753503</td>\n",
       "      <td>-0.114618</td>\n",
       "      <td>-0.059685</td>\n",
       "      <td>-0.046232</td>\n",
       "      <td>-0.002248</td>\n",
       "      <td>0.065763</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.221062</td>\n",
       "      <td>-0.119099</td>\n",
       "      <td>-0.110237</td>\n",
       "      <td>-0.664177</td>\n",
       "      <td>0.052724</td>\n",
       "      <td>-0.336943</td>\n",
       "      <td>0.021138</td>\n",
       "      <td>-0.160624</td>\n",
       "      <td>-0.099285</td>\n",
       "      <td>-0.440708</td>\n",
       "      <td>0.383819</td>\n",
       "      <td>0.641210</td>\n",
       "      <td>0.635158</td>\n",
       "      <td>2.653875</td>\n",
       "      <td>0.184460</td>\n",
       "      <td>2.560986</td>\n",
       "      <td>-0.241563</td>\n",
       "      <td>-0.109047</td>\n",
       "      <td>-0.653331</td>\n",
       "      <td>0.699936</td>\n",
       "      <td>0.671222</td>\n",
       "      <td>-1.033898</td>\n",
       "      <td>-0.016909</td>\n",
       "      <td>-0.901262</td>\n",
       "      <td>-1.001319</td>\n",
       "      <td>-0.371000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.127813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.241250</td>\n",
       "      <td>0.242688</td>\n",
       "      <td>-0.002609</td>\n",
       "      <td>0.139621</td>\n",
       "      <td>1.329443</td>\n",
       "      <td>2.329796</td>\n",
       "      <td>0.123187</td>\n",
       "      <td>-0.060640</td>\n",
       "      <td>-0.201366</td>\n",
       "      <td>-0.004628</td>\n",
       "      <td>-0.008292</td>\n",
       "      <td>-0.039354</td>\n",
       "      <td>-0.068543</td>\n",
       "      <td>0.074813</td>\n",
       "      <td>0.130321</td>\n",
       "      <td>-0.000238</td>\n",
       "      <td>0.128159</td>\n",
       "      <td>-0.007429</td>\n",
       "      <td>-0.154127</td>\n",
       "      <td>4.104377</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.260225</td>\n",
       "      <td>0.465077</td>\n",
       "      <td>0.126900</td>\n",
       "      <td>-0.000810</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>-0.009085</td>\n",
       "      <td>-0.026615</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.000076</td>\n",
       "      <td>-0.000358</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>-0.022250</td>\n",
       "      <td>-0.034998</td>\n",
       "      <td>-0.308995</td>\n",
       "      <td>-0.103373</td>\n",
       "      <td>-0.001026</td>\n",
       "      <td>-0.002704</td>\n",
       "      <td>-0.091684</td>\n",
       "      <td>-0.075261</td>\n",
       "      <td>-0.258189</td>\n",
       "      <td>-0.255447</td>\n",
       "      <td>-0.126716</td>\n",
       "      <td>-0.276211</td>\n",
       "      <td>-0.155800</td>\n",
       "      <td>-0.006981</td>\n",
       "      <td>-0.003381</td>\n",
       "      <td>-0.101877</td>\n",
       "      <td>-0.032694</td>\n",
       "      <td>-0.031706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.012951</td>\n",
       "      <td>0.167433</td>\n",
       "      <td>-0.029549</td>\n",
       "      <td>0.353778</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.336252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.227687</td>\n",
       "      <td>0.519870</td>\n",
       "      <td>-0.019833</td>\n",
       "      <td>-0.040847</td>\n",
       "      <td>-0.052477</td>\n",
       "      <td>1.004543</td>\n",
       "      <td>-0.061889</td>\n",
       "      <td>-0.005486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2116</th>\n",
       "      <td>-0.826989</td>\n",
       "      <td>-0.041629</td>\n",
       "      <td>-0.046757</td>\n",
       "      <td>-0.905674</td>\n",
       "      <td>0.165526</td>\n",
       "      <td>-0.942051</td>\n",
       "      <td>0.391674</td>\n",
       "      <td>-0.050908</td>\n",
       "      <td>-0.869245</td>\n",
       "      <td>-0.040587</td>\n",
       "      <td>-0.942128</td>\n",
       "      <td>-1.920122</td>\n",
       "      <td>0.109571</td>\n",
       "      <td>-0.295938</td>\n",
       "      <td>-0.482493</td>\n",
       "      <td>-0.302145</td>\n",
       "      <td>0.150220</td>\n",
       "      <td>-0.048025</td>\n",
       "      <td>0.346086</td>\n",
       "      <td>-0.710210</td>\n",
       "      <td>0.268111</td>\n",
       "      <td>-0.243370</td>\n",
       "      <td>-0.101431</td>\n",
       "      <td>-0.101193</td>\n",
       "      <td>-0.887292</td>\n",
       "      <td>-0.111807</td>\n",
       "      <td>0.080874</td>\n",
       "      <td>-0.527018</td>\n",
       "      <td>0.058842</td>\n",
       "      <td>0.002634</td>\n",
       "      <td>0.692140</td>\n",
       "      <td>0.591906</td>\n",
       "      <td>0.089524</td>\n",
       "      <td>0.089332</td>\n",
       "      <td>0.645773</td>\n",
       "      <td>-0.197195</td>\n",
       "      <td>0.443340</td>\n",
       "      <td>-0.100691</td>\n",
       "      <td>-0.055519</td>\n",
       "      <td>-0.113597</td>\n",
       "      <td>-0.179029</td>\n",
       "      <td>-0.461493</td>\n",
       "      <td>0.674174</td>\n",
       "      <td>-0.022750</td>\n",
       "      <td>-1.457341</td>\n",
       "      <td>-0.498530</td>\n",
       "      <td>-0.956111</td>\n",
       "      <td>-0.924697</td>\n",
       "      <td>-1.034101</td>\n",
       "      <td>-0.916229</td>\n",
       "      <td>-1.340563</td>\n",
       "      <td>-0.615273</td>\n",
       "      <td>0.028934</td>\n",
       "      <td>0.375269</td>\n",
       "      <td>0.591471</td>\n",
       "      <td>0.527293</td>\n",
       "      <td>-0.159744</td>\n",
       "      <td>-0.047051</td>\n",
       "      <td>-0.277401</td>\n",
       "      <td>-0.005865</td>\n",
       "      <td>-0.000638</td>\n",
       "      <td>-0.025115</td>\n",
       "      <td>0.030998</td>\n",
       "      <td>0.254083</td>\n",
       "      <td>0.314438</td>\n",
       "      <td>0.002076</td>\n",
       "      <td>-0.007482</td>\n",
       "      <td>-0.009609</td>\n",
       "      <td>-0.051676</td>\n",
       "      <td>-0.920798</td>\n",
       "      <td>0.125177</td>\n",
       "      <td>-0.170878</td>\n",
       "      <td>-0.094073</td>\n",
       "      <td>-0.004514</td>\n",
       "      <td>0.070196</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.115591</td>\n",
       "      <td>-0.187369</td>\n",
       "      <td>-0.197957</td>\n",
       "      <td>-0.014884</td>\n",
       "      <td>-0.013678</td>\n",
       "      <td>-0.015323</td>\n",
       "      <td>-0.098531</td>\n",
       "      <td>0.060141</td>\n",
       "      <td>-0.458352</td>\n",
       "      <td>-0.005823</td>\n",
       "      <td>0.060331</td>\n",
       "      <td>0.022833</td>\n",
       "      <td>0.580822</td>\n",
       "      <td>0.380013</td>\n",
       "      <td>0.235347</td>\n",
       "      <td>0.232847</td>\n",
       "      <td>0.416106</td>\n",
       "      <td>-0.428854</td>\n",
       "      <td>-0.192951</td>\n",
       "      <td>0.042064</td>\n",
       "      <td>-0.001595</td>\n",
       "      <td>0.154025</td>\n",
       "      <td>-0.323833</td>\n",
       "      <td>-0.594364</td>\n",
       "      <td>0.129474</td>\n",
       "      <td>-0.047782</td>\n",
       "      <td>-0.026050</td>\n",
       "      <td>0.222922</td>\n",
       "      <td>-0.022889</td>\n",
       "      <td>0.003734</td>\n",
       "      <td>-0.097475</td>\n",
       "      <td>-0.119853</td>\n",
       "      <td>-0.220500</td>\n",
       "      <td>-0.030727</td>\n",
       "      <td>0.039075</td>\n",
       "      <td>0.105650</td>\n",
       "      <td>0.332904</td>\n",
       "      <td>0.011538</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>-0.004688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            I1        I2        I3        I4        I5        I6        I7  \\\n",
       "7426  0.474336  0.010288 -0.000710 -0.606795 -0.132097  0.227654  0.407601   \n",
       "4264  0.735381 -0.019519 -0.029400  1.373814 -0.242825  0.006498 -0.218154   \n",
       "5670  0.150030 -0.030559 -0.041371 -0.180490  0.023586 -0.143670 -0.301218   \n",
       "2110 -0.392277 -0.036396 -0.017427  0.000440  0.194470  0.427458  0.290762   \n",
       "2116 -0.826989 -0.041629 -0.046757 -0.905674  0.165526 -0.942051  0.391674   \n",
       "\n",
       "            I8        I9       I10       I11       I12       I13       I14  \\\n",
       "7426 -0.002148  0.255976  0.010343  0.069800 -0.042908 -0.031332 -0.039776   \n",
       "4264 -0.029641  1.226153 -0.014224  0.696016  0.219364 -0.037933  0.711385   \n",
       "5670 -0.039838  0.782315 -0.021184 -0.104858  0.603132 -0.063976  0.786947   \n",
       "2110 -0.039829 -0.240505 -0.022131 -0.481441  1.195059 -0.133470  0.360923   \n",
       "2116 -0.050908 -0.869245 -0.040587 -0.942128 -1.920122  0.109571 -0.295938   \n",
       "\n",
       "           I15       I16       I17       I18       I19       I20       I21  \\\n",
       "7426  1.244465 -0.281740 -0.034615 -0.046939  0.046508 -0.189993 -0.042645   \n",
       "4264  0.669523 -0.165935 -0.110332 -0.044059 -0.110881  0.283741       NaN   \n",
       "5670  0.997630 -0.155514 -0.070871 -0.045427 -0.083950  0.062908 -0.163518   \n",
       "2110  0.753503 -0.114618 -0.059685 -0.046232 -0.002248  0.065763       NaN   \n",
       "2116 -0.482493 -0.302145  0.150220 -0.048025  0.346086 -0.710210  0.268111   \n",
       "\n",
       "           I22       I23       I24       I25       I26       I27       I28  \\\n",
       "7426 -0.201074 -0.061445 -0.044316 -0.571520 -0.145953  2.143646 -0.514716   \n",
       "4264  0.006747 -0.100089 -0.088502  0.737501 -0.100008  0.507143  1.584636   \n",
       "5670 -0.015327 -0.072948 -0.057468  0.767850  0.173958 -0.128118 -0.310119   \n",
       "2110 -0.221062 -0.119099 -0.110237 -0.664177  0.052724 -0.336943  0.021138   \n",
       "2116 -0.243370 -0.101431 -0.101193 -0.887292 -0.111807  0.080874 -0.527018   \n",
       "\n",
       "           I29       I30       I31       I32       I33       I34       I35  \\\n",
       "7426 -0.097087 -0.035054  0.599826  1.021227  1.567838  1.551942  1.468628   \n",
       "4264 -0.123937 -0.048261 -0.549628 -0.592205 -0.239093 -0.235795 -0.505718   \n",
       "5670 -0.164138 -0.079906 -0.883528 -1.344381 -0.716429 -0.708061 -0.897027   \n",
       "2110 -0.160624 -0.099285 -0.440708  0.383819  0.641210  0.635158  2.653875   \n",
       "2116  0.058842  0.002634  0.692140  0.591906  0.089524  0.089332  0.645773   \n",
       "\n",
       "           I36       I37       I38       I39       I40       I41       I42  \\\n",
       "7426  1.229721  1.321067 -0.084339 -0.042190  0.476034  0.192107  0.178743   \n",
       "4264 -0.186124 -0.501709 -0.046078 -0.036884 -0.457416  0.115378  0.104334   \n",
       "5670 -0.443576 -0.724724 -0.170219 -0.079356 -0.867710  0.626772  0.600270   \n",
       "2110  0.184460  2.560986 -0.241563 -0.109047 -0.653331  0.699936  0.671222   \n",
       "2116 -0.197195  0.443340 -0.100691 -0.055519 -0.113597 -0.179029 -0.461493   \n",
       "\n",
       "           I43       I44       I45       I46       I47       I48       I49  \\\n",
       "7426  0.425832  0.051295 -1.429454 -0.915034 -0.818222 -0.859571 -0.831964   \n",
       "4264  0.187516  0.077230  0.179328 -0.317715  0.771000       NaN  0.230770   \n",
       "5670  0.559442 -0.010800  0.906108 -0.712064  0.854778  1.128683  1.737216   \n",
       "2110 -1.033898 -0.016909 -0.901262 -1.001319 -0.371000       NaN  0.127813   \n",
       "2116  0.674174 -0.022750 -1.457341 -0.498530 -0.956111 -0.924697 -1.034101   \n",
       "\n",
       "           I50       I51       I52       I53       I54       I55       I56  \\\n",
       "7426 -0.737321 -0.953812 -0.423662  0.167951  0.982461  1.507053 -0.872424   \n",
       "4264       NaN  0.487625  0.532675 -0.113977 -0.502087 -0.072377 -0.530395   \n",
       "5670  1.107266  1.701688  1.132818 -0.104753 -0.537718  0.410211  0.797962   \n",
       "2110       NaN -0.241250  0.242688 -0.002609  0.139621  1.329443  2.329796   \n",
       "2116 -0.916229 -1.340563 -0.615273  0.028934  0.375269  0.591471  0.527293   \n",
       "\n",
       "           I57       I58       dI1       dI2       dI3       dI4       dI5  \\\n",
       "7426 -0.117816 -0.031497 -0.008751  0.009108  0.008876 -0.008488 -0.122003   \n",
       "4264 -0.119678 -0.035370 -0.020779 -0.001515 -0.001271  0.566549  0.112948   \n",
       "5670 -0.112652 -0.057491 -1.246969 -0.017840 -0.014250 -0.627947  0.082116   \n",
       "2110  0.123187 -0.060640 -0.201366 -0.004628 -0.008292 -0.039354 -0.068543   \n",
       "2116 -0.159744 -0.047051 -0.277401 -0.005865 -0.000638 -0.025115  0.030998   \n",
       "\n",
       "           dI6       dI7       dI8       dI9      dI10      dI11      dI12  \\\n",
       "7426 -0.150909 -0.168123  0.007286 -0.143467  0.005262 -0.017017 -0.045596   \n",
       "4264  0.148753  0.146571  0.010503  0.791676 -0.004064  0.011731  0.351006   \n",
       "5670 -0.027240  0.043286 -0.003879 -0.281142 -0.029354 -1.055325  0.657791   \n",
       "2110  0.074813  0.130321 -0.000238  0.128159 -0.007429 -0.154127  4.104377   \n",
       "2116  0.254083  0.314438  0.002076 -0.007482 -0.009609 -0.051676 -0.920798   \n",
       "\n",
       "          dI13      dI14      dI15      dI16      dI17      dI18      dI19  \\\n",
       "7426  0.007570  0.026585 -0.414900 -0.012459  0.012176 -0.000270  0.020325   \n",
       "4264  0.012634 -0.178190  0.477856  0.039291 -0.003576  0.001576 -0.013169   \n",
       "5670  0.038651 -0.156749 -0.189418  0.051770  0.036775 -0.000911  0.020150   \n",
       "2110  0.000000 -0.260225  0.465077  0.126900 -0.000810  0.000136 -0.009085   \n",
       "2116  0.125177 -0.170878 -0.094073 -0.004514  0.070196  0.000679  0.115591   \n",
       "\n",
       "          dI20      dI21      dI22      dI23      dI24      dI25      dI26  \\\n",
       "7426 -0.037350  0.034140  0.004292  0.000737  0.000842  0.036921 -0.025854   \n",
       "4264  0.026280       NaN  0.002315 -0.000972 -0.001111 -0.055436 -0.005346   \n",
       "5670 -0.690801 -0.011164 -0.048206 -0.008495 -0.009713 -0.455277  0.046544   \n",
       "2110 -0.026615       NaN -0.000076 -0.000358  0.000831 -0.022250 -0.034998   \n",
       "2116 -0.187369 -0.197957 -0.014884 -0.013678 -0.015323 -0.098531  0.060141   \n",
       "\n",
       "          dI27      dI28      dI29      dI30      dI31      dI32      dI33  \\\n",
       "7426  0.210788  0.002333 -0.046517 -0.028944 -0.756167 -0.166527 -0.281711   \n",
       "4264 -0.177226 -0.033158  0.017723  0.011541  0.098352  0.119397  0.039128   \n",
       "5670 -0.364856 -0.055686  0.034768  0.016951  0.031098 -0.228416 -0.394980   \n",
       "2110 -0.308995 -0.103373 -0.001026 -0.002704 -0.091684 -0.075261 -0.258189   \n",
       "2116 -0.458352 -0.005823  0.060331  0.022833  0.580822  0.380013  0.235347   \n",
       "\n",
       "          dI34      dI35      dI36      dI37      dI38      dI39      dI40  \\\n",
       "7426 -0.278719 -0.292282 -0.167453 -0.215679 -0.088958 -0.036857 -0.895317   \n",
       "4264  0.038712  0.037939  0.013403  0.017530  0.045437  0.016490  0.133769   \n",
       "5670 -0.390785 -0.315390 -0.323573 -0.280804  0.043982  0.018647  0.038100   \n",
       "2110 -0.255447 -0.126716 -0.276211 -0.155800 -0.006981 -0.003381 -0.101877   \n",
       "2116  0.232847  0.416106 -0.428854 -0.192951  0.042064 -0.001595  0.154025   \n",
       "\n",
       "          dI41      dI42      dI43      dI44      dI45      dI46      dI47  \\\n",
       "7426  0.229733  0.222789  0.529174  0.039070  0.005165  0.065233  0.024111   \n",
       "4264 -0.149494 -0.144975       NaN  0.077788  0.149001 -0.015912  0.704333   \n",
       "5670 -1.159936 -1.124875  0.182791 -0.007366 -0.049387 -0.078541  0.660222   \n",
       "2110 -0.032694 -0.031706  0.000000 -0.012951  0.167433 -0.029549  0.353778   \n",
       "2116 -0.323833 -0.594364  0.129474 -0.047782 -0.026050  0.222922 -0.022889   \n",
       "\n",
       "          dI48      dI49      dI50      dI51      dI52      dI53      dI54  \\\n",
       "7426 -0.003079 -0.040331 -0.037055 -0.049063  0.009558 -0.000408 -0.016245   \n",
       "4264       NaN  0.135784       NaN  0.193875  0.478701 -0.024174 -0.148020   \n",
       "5670 -0.383559  0.192662  0.380037  0.006750  0.726753 -0.008247  0.087142   \n",
       "2110       NaN  0.336252       NaN  0.227687  0.519870 -0.019833 -0.040847   \n",
       "2116  0.003734 -0.097475 -0.119853 -0.220500 -0.030727  0.039075  0.105650   \n",
       "\n",
       "          dI55      dI56      dI57      dI58  \n",
       "7426 -0.044080  0.118029 -0.006498 -0.015567  \n",
       "4264 -0.161484 -0.100311  0.069675 -0.004375  \n",
       "5670  0.101860 -0.054802 -0.133887  0.001193  \n",
       "2110 -0.052477  1.004543 -0.061889 -0.005486  \n",
       "2116  0.332904  0.011538  0.000010 -0.004688  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0384-cbe0-44f2-8213-eb9593d584ad",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aecc89f2-c2ee-4ea5-8684-821d96bb9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6400, 116)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852d5c1-cd68-4fb6-a691-4436538012a3",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "346abda1-a979-431a-b6a2-0219de1a8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('SVM', SVC(random_state=0, probability=True)),\n",
    "    ('NaiveBayes', GaussianNB()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000)) #hidden_layer_sizes=(20,20) for 2 hidden layers with 20 neurons each\n",
    "]\n",
    "\n",
    "vote_model = VotingClassifier(\n",
    "    estimators=voting_estimators, \n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('SVM', SVC(random_state=0, probability=True)),\n",
    "    ('NaiveBayes', GaussianNB()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME')),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000))\n",
    "]\n",
    "meta_stack_classifier = LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=stacking_estimators, \n",
    "    final_estimator=meta_stack_classifier, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\", max_iter=1000)),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0, algorithm='SAMME'))\n",
    "    #('Voting', vote_model),\n",
    "    #('Stacking', stacking_model),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0, max_iter=1000)) # 2 hidden layers with 20 neurons each\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3b2aa3-2279-4c45-be46-3fa5b9668c35",
   "metadata": {},
   "source": [
    "# Create Pipeline with different combination of preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665f57b-10f3-482f-b28b-1b4fc3a5b25e",
   "metadata": {},
   "source": [
    "## Combination 4\n",
    "#### knn imputation, replace outliers with IQR thresholds, Smote oversampling, standard scaling, RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d91f42e-aedb-4430-b6dc-fedea5e7ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "objs = dict()\n",
    "####################################### Imputing Missing Values\n",
    "X_trainP, imp = handle_missing_vals_knn(\n",
    "    X_train, \n",
    "    n_neighbors=5\n",
    ")\n",
    "\n",
    "objs['miss'] = imp\n",
    "####################################### Remove outliers\n",
    "X_trainP_df = pd.concat(\n",
    "    [\n",
    "        X_trainP.reset_index(drop=True), \n",
    "        pd.Series(y_train, name='Class').reset_index(drop=True)\n",
    "    ], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "X_trainP_df = replace_outliers_with_thresholds(\n",
    "    dataframe=X_trainP_df,\n",
    "    columns=X_trainP_df.columns,\n",
    "    target='Class'\n",
    ")\n",
    "\n",
    "X_trainP = (\n",
    "    X_trainP_df\n",
    "    .loc[:, X_trainP_df.columns != 'Class']\n",
    ")\n",
    "\n",
    "y_trainP = X_trainP_df['Class']\n",
    "####################################### Oversampling\n",
    "X_trainP, y_trainP = apply_smote(X_trainP, y_trainP)\n",
    "####################################### Standard Scaler\n",
    "std_scale_cols = (\n",
    "    X_trainP\n",
    "    .loc[:, ~X_trainP.columns.str.contains('Group')]\n",
    "    .columns\n",
    ")\n",
    "\n",
    "X_trainP, std_scaler = apply_std_scaler(\n",
    "    X_trainP, \n",
    "    std_scale_cols\n",
    ")\n",
    "\n",
    "objs['scaler'] = std_scaler\n",
    "####################################### RFECV\n",
    "classifier = DecisionTreeClassifier()\n",
    "X_trainP = select_features_rfecv(\n",
    "    X=X_trainP, \n",
    "    y=y_trainP, \n",
    "    classifier=classifier, \n",
    "    cv=3, \n",
    "    scoring='f1_weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e230017-e092-4e62-8a14-5345a8c7ede4",
   "metadata": {},
   "source": [
    "### Prepare the test set according to combination 3 above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fb7be12-40c6-4249-acff-7c11bbd72d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols_test = X_test.loc[:, X_test.columns != 'Group'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c48ba29-ffe3-4158-8153-8beb3e417e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## Imputing Missing Values\n",
    "X_testP = pd.DataFrame(objs['miss'].transform(X_test), columns=X_test.columns)\n",
    "######################################## Scaling\n",
    "X_testP = pd.DataFrame(objs['scaler'].transform(X_testP[num_cols_test]), columns=num_cols_test).reset_index(drop=True)\n",
    "######################################## RFECV\n",
    "subset_features_rfecv = X_trainP.columns.to_list()\n",
    "X_testP = X_testP.loc[:, subset_features_rfecv]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1fdbb-5b7a-43b3-83af-7b48bee5c3b4",
   "metadata": {},
   "source": [
    "# Apply cross validation with f1, auc and accuracy\n",
    "\n",
    "#### *Weighted F1 Score: F1 score calculated by taking the average of F1 scores for each class. Average is weighted by support which is the number of true instances for each label. \n",
    "#### *AUC One vs One Weighted: By considering all pairwise combinations of classes, average AUC is calculated. Average is weighted by the support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79197db7-33cc-42da-b582-ecf52eba91a6",
   "metadata": {},
   "source": [
    "## Create Class object and apply cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fcb2a67-1bbe-4f97-8660-0c76963e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cost_matrix = np.array([[0, 1, 2],\n",
    "                                      [1, 0, 1],\n",
    "                                      [2, 1, 0]])\n",
    "model_selector = ModelSelection(\n",
    "    x_train=X_trainP, \n",
    "    y_train=y_trainP,\n",
    "    estimators=estimators,\n",
    "    x_test=X_testP,\n",
    "    y_test=y_test,\n",
    "    cost_matrix=error_cost_matrix\n",
    ")\n",
    "model_selector.encode_y_train()\n",
    "model_selector.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "255d6131-2636-464d-91f0-d2cee3a3f369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: -1, 1: 0, 2: 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selector.target_label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ec19b29-b2f3-4d57-9c58-fbfe15b92c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Mean F1 Score:\n",
      "\n",
      "RandomForest = 0.639336\n",
      "SVM = 0.481659\n",
      "LogisticRegression = 0.400893\n",
      "GradientBoost = 0.488286\n",
      "XGBoost = 0.623610\n",
      "AdaBoost = 0.401376\n",
      "\n",
      "Best Estimator (F1): RandomForest\n",
      "**********************************************\n",
      "CV Results for Mean AUC Score:\n",
      "\n",
      "RandomForest = 0.819615\n",
      "SVM = 0.694049\n",
      "LogisticRegression = 0.569860\n",
      "GradientBoost = 0.679838\n",
      "XGBoost = 0.799632\n",
      "AdaBoost = 0.579882\n",
      "\n",
      "Best Estimator (AUC): RandomForest\n",
      "**********************************************\n",
      "CV Results for Mean Accuracy Score:\n",
      "\n",
      "RandomForest = 0.642778\n",
      "SVM = 0.496903\n",
      "LogisticRegression = 0.404443\n",
      "GradientBoost = 0.495466\n",
      "XGBoost = 0.627737\n",
      "AdaBoost = 0.406989\n",
      "\n",
      "Best Estimator (Accuracy): RandomForest\n",
      "**********************************************\n",
      "CV Results for Cost Matrix Error Score:\n",
      "\n",
      "RandomForest = 0.584384\n",
      "SVM = 0.683035\n",
      "LogisticRegression = 0.781796\n",
      "GradientBoost = 0.701062\n",
      "XGBoost = 0.600420\n",
      "AdaBoost = 0.777151\n",
      "\n",
      "Best Estimator (Cost Metric Error Score): RandomForest\n"
     ]
    }
   ],
   "source": [
    "model_selector.calculate_cv_f1(n_folds=5, scoring_average='f1_weighted')\n",
    "print('**********************************************')\n",
    "model_selector.calculate_cv_auc(n_folds=5, scoring_average='roc_auc_ovo_weighted')\n",
    "print('**********************************************')   \n",
    "model_selector.calculate_cv_accuracy(n_folds=5, scoring_average='accuracy')\n",
    "print('**********************************************')   \n",
    "model_selector.calculate_cost_matrix_cv2(n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0e9b3d4-87d3-496c-9a15-db65103730b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Cost Matrix Error Score:\n",
      "\n",
      "RandomForest = 0.584385\n",
      "SVM = 0.683034\n",
      "LogisticRegression = 0.781802\n",
      "GradientBoost = 0.701061\n",
      "XGBoost = 0.600420\n",
      "AdaBoost = 0.777150\n",
      "\n",
      "Best Estimator (Cost Metric Error Score): RandomForest\n"
     ]
    }
   ],
   "source": [
    "model_selector.calculate_cost_matrix_error_cv(custom_cost_func=matrix_error_function, n_folds=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff78202-7717-4a74-9c00-2bd9aac5fb64",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7f5076d9-0356-4356-86a4-f48bf781d89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'ClassificationModel__n_estimators': [50, 100, 150],\n",
    "    'ClassificationModel__criterion': ['gini', 'entropy'],\n",
    "    'ClassificationModel__max_depth': [None, 5, 10],\n",
    "    'ClassificationModel__min_samples_split': [2, 5],\n",
    "    'ClassificationModel__min_samples_leaf': [1, 2],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78553300-8e68-4d61-bcb9-30d8b722128d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 72 candidates, totalling 216 fits\n",
      "Best parameters found by GridSearchCV (f1_weighted):\n",
      "{'ClassificationModel__criterion': 'gini', 'ClassificationModel__max_depth': None, 'ClassificationModel__min_samples_leaf': 1, 'ClassificationModel__min_samples_split': 2, 'ClassificationModel__n_estimators': 150}\n",
      "\n",
      "Best score found by GridSearchCV (f1_weighted):\n",
      "0.6424143429370669\n"
     ]
    }
   ],
   "source": [
    "model_selector.apply_grid_cv(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    params=param_grid,\n",
    "    cv=3,\n",
    "    scoring='f1_weighted'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d44c14aa-fb23-4797-a563-1549a0eb1379",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: -1, 1: 0, 2: 1}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.45      0.44       619\n",
      "           1       0.17      0.14      0.16       227\n",
      "           2       0.49      0.51      0.50       754\n",
      "\n",
      "    accuracy                           0.43      1600\n",
      "   macro avg       0.37      0.37      0.37      1600\n",
      "weighted avg       0.42      0.43      0.43      1600\n",
      "\n",
      "{0: -1, 1: 0, 2: 1}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_selector.predict_on_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab05565-bbb6-4896-8694-86653a52203f",
   "metadata": {},
   "source": [
    "### Second Grid Search CV with custom cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afff708d-c42b-4389-ab85-051497af022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search2 = ModelSelection(\n",
    "    x_train=X_trainP, \n",
    "    y_train=y_trainP,\n",
    "    estimators=estimators,\n",
    "    x_test=X_testP,\n",
    "    y_test=y_test,\n",
    "    cost_matrix=error_cost_matrix\n",
    ")\n",
    "\n",
    "grid_search2.encode_y_train()\n",
    "grid_search2.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a10329b-491f-419d-8656-8dddbd6c9528",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'ClassificationModel__n_estimators': [50, 100, 150, 200, 250, 300],\n",
    "    'ClassificationModel__max_depth': [None, 10, 20, 30],\n",
    "    'ClassificationModel__min_samples_split': [2, 5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5a84cc-1072-4157-804a-5ce7bc1b35a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mgrid_search2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_grid_cv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRandomForestClassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmatrix_error_function\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\ProjectDataMining2\\DM2_DataCraft\\Functions_Classes.py:424\u001b[0m, in \u001b[0;36mModelSelection.apply_grid_cv\u001b[1;34m(self, estimator, params, cv, scoring)\u001b[0m\n\u001b[0;32m    408\u001b[0m     pipe \u001b[38;5;241m=\u001b[39m Pipeline(\n\u001b[0;32m    409\u001b[0m         [\n\u001b[0;32m    410\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumnTransformers\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcol_transformer), \n\u001b[0;32m    411\u001b[0m             (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClassificationModel\u001b[39m\u001b[38;5;124m'\u001b[39m, estimator)\n\u001b[0;32m    412\u001b[0m         ]\n\u001b[0;32m    413\u001b[0m     )\n\u001b[0;32m    415\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m    416\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mpipe, \n\u001b[0;32m    417\u001b[0m     param_grid\u001b[38;5;241m=\u001b[39mparams, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m     error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    423\u001b[0m )\n\u001b[1;32m--> 424\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoded_y_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_object \u001b[38;5;241m=\u001b[39m grid_search\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_best_parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrid_object\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    964\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    965\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    966\u001b[0m     )\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 970\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    974\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1525\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1526\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1527\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    909\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    910\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    911\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    912\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    913\u001b[0m         )\n\u001b[0;32m    914\u001b[0m     )\n\u001b[1;32m--> 916\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    921\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    929\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    930\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    931\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    932\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    935\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    937\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    938\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    939\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     66\u001b[0m )\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.01\u001b[39m)\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search2.apply_grid_cv(\n",
    "    estimator=RandomForestClassifier(random_state=0),\n",
    "    params=param_grid,\n",
    "    cv=5,\n",
    "    scoring=matrix_error_function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d06213de-3c34-4f04-ad57-8fc62bf9cfef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.62652068, -0.62353462, -0.62320283, -0.63879673, -0.61700951,\n",
       "       -0.60462287])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search2.grid_cv_results['mean_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25c302e2-4fb4-4bb4-947c-b23a64d0b636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.6046228710462287"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search2.grid_best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638079cb-ec8e-492e-972c-95aa90ec408e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
