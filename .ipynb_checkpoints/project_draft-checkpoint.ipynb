{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18872d5e-2ee7-4f1c-8374-c436aa9ea3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier, StackingClassifier\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold, train_test_split, cross_val_predict, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer, roc_auc_score, confusion_matrix, classification_report\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006456f7-eecd-4486-abb2-db0ff3ef51a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/training_data.xls\")\n",
    "df = df.loc[:, df.columns != 'Perform']\n",
    "X_test = pd.read_excel(\"C:/Users/Cagan Deliktas/Desktop/ProjectDataMining2/DM2_DataCraft/data/test_data_no_target.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88066b03-5993-437f-9af7-d0f10d3bf8ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Group</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>I10</th>\n",
       "      <th>I11</th>\n",
       "      <th>I12</th>\n",
       "      <th>I13</th>\n",
       "      <th>I14</th>\n",
       "      <th>I15</th>\n",
       "      <th>I16</th>\n",
       "      <th>I17</th>\n",
       "      <th>I18</th>\n",
       "      <th>I19</th>\n",
       "      <th>I20</th>\n",
       "      <th>I21</th>\n",
       "      <th>I22</th>\n",
       "      <th>I23</th>\n",
       "      <th>I24</th>\n",
       "      <th>I25</th>\n",
       "      <th>I26</th>\n",
       "      <th>I27</th>\n",
       "      <th>I28</th>\n",
       "      <th>I29</th>\n",
       "      <th>I30</th>\n",
       "      <th>I31</th>\n",
       "      <th>I32</th>\n",
       "      <th>I33</th>\n",
       "      <th>I34</th>\n",
       "      <th>I35</th>\n",
       "      <th>I36</th>\n",
       "      <th>I37</th>\n",
       "      <th>I38</th>\n",
       "      <th>I39</th>\n",
       "      <th>I40</th>\n",
       "      <th>I41</th>\n",
       "      <th>I42</th>\n",
       "      <th>I43</th>\n",
       "      <th>I44</th>\n",
       "      <th>I45</th>\n",
       "      <th>I46</th>\n",
       "      <th>I47</th>\n",
       "      <th>I48</th>\n",
       "      <th>I49</th>\n",
       "      <th>I50</th>\n",
       "      <th>I51</th>\n",
       "      <th>I52</th>\n",
       "      <th>I53</th>\n",
       "      <th>I54</th>\n",
       "      <th>I55</th>\n",
       "      <th>I56</th>\n",
       "      <th>I57</th>\n",
       "      <th>I58</th>\n",
       "      <th>dI1</th>\n",
       "      <th>dI2</th>\n",
       "      <th>dI3</th>\n",
       "      <th>dI4</th>\n",
       "      <th>dI5</th>\n",
       "      <th>dI6</th>\n",
       "      <th>dI7</th>\n",
       "      <th>dI8</th>\n",
       "      <th>dI9</th>\n",
       "      <th>dI10</th>\n",
       "      <th>dI11</th>\n",
       "      <th>dI12</th>\n",
       "      <th>dI13</th>\n",
       "      <th>dI14</th>\n",
       "      <th>dI15</th>\n",
       "      <th>dI16</th>\n",
       "      <th>dI17</th>\n",
       "      <th>dI18</th>\n",
       "      <th>dI19</th>\n",
       "      <th>dI20</th>\n",
       "      <th>dI21</th>\n",
       "      <th>dI22</th>\n",
       "      <th>dI23</th>\n",
       "      <th>dI24</th>\n",
       "      <th>dI25</th>\n",
       "      <th>dI26</th>\n",
       "      <th>dI27</th>\n",
       "      <th>dI28</th>\n",
       "      <th>dI29</th>\n",
       "      <th>dI30</th>\n",
       "      <th>dI31</th>\n",
       "      <th>dI32</th>\n",
       "      <th>dI33</th>\n",
       "      <th>dI34</th>\n",
       "      <th>dI35</th>\n",
       "      <th>dI36</th>\n",
       "      <th>dI37</th>\n",
       "      <th>dI38</th>\n",
       "      <th>dI39</th>\n",
       "      <th>dI40</th>\n",
       "      <th>dI41</th>\n",
       "      <th>dI42</th>\n",
       "      <th>dI43</th>\n",
       "      <th>dI44</th>\n",
       "      <th>dI45</th>\n",
       "      <th>dI46</th>\n",
       "      <th>dI47</th>\n",
       "      <th>dI48</th>\n",
       "      <th>dI49</th>\n",
       "      <th>dI50</th>\n",
       "      <th>dI51</th>\n",
       "      <th>dI52</th>\n",
       "      <th>dI53</th>\n",
       "      <th>dI54</th>\n",
       "      <th>dI55</th>\n",
       "      <th>dI56</th>\n",
       "      <th>dI57</th>\n",
       "      <th>dI58</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>G9</td>\n",
       "      <td>0.136495</td>\n",
       "      <td>-0.028429</td>\n",
       "      <td>-0.037772</td>\n",
       "      <td>-0.232459</td>\n",
       "      <td>-0.016222</td>\n",
       "      <td>-0.187506</td>\n",
       "      <td>-0.322545</td>\n",
       "      <td>-0.043743</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>-0.014757</td>\n",
       "      <td>-0.033105</td>\n",
       "      <td>0.303035</td>\n",
       "      <td>-0.093811</td>\n",
       "      <td>-0.598917</td>\n",
       "      <td>-0.271292</td>\n",
       "      <td>-0.256749</td>\n",
       "      <td>-0.100146</td>\n",
       "      <td>-0.045525</td>\n",
       "      <td>-0.078422</td>\n",
       "      <td>-0.060129</td>\n",
       "      <td>-0.069528</td>\n",
       "      <td>-0.052432</td>\n",
       "      <td>-0.114432</td>\n",
       "      <td>-0.104989</td>\n",
       "      <td>0.342845</td>\n",
       "      <td>-0.159417</td>\n",
       "      <td>0.006772</td>\n",
       "      <td>-0.303193</td>\n",
       "      <td>-0.163287</td>\n",
       "      <td>-0.080599</td>\n",
       "      <td>-0.828880</td>\n",
       "      <td>-1.064215</td>\n",
       "      <td>-0.547067</td>\n",
       "      <td>-0.540497</td>\n",
       "      <td>-0.676045</td>\n",
       "      <td>-0.305007</td>\n",
       "      <td>-0.507724</td>\n",
       "      <td>-0.191437</td>\n",
       "      <td>-0.087362</td>\n",
       "      <td>-0.856151</td>\n",
       "      <td>0.802525</td>\n",
       "      <td>0.733080</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.533290</td>\n",
       "      <td>0.195197</td>\n",
       "      <td>0.058094</td>\n",
       "      <td>-0.228889</td>\n",
       "      <td>-0.150821</td>\n",
       "      <td>-0.104986</td>\n",
       "      <td>-0.026743</td>\n",
       "      <td>0.188312</td>\n",
       "      <td>-0.250701</td>\n",
       "      <td>-0.101190</td>\n",
       "      <td>-0.357521</td>\n",
       "      <td>-0.527956</td>\n",
       "      <td>0.611385</td>\n",
       "      <td>-0.092714</td>\n",
       "      <td>-0.055733</td>\n",
       "      <td>-0.065709</td>\n",
       "      <td>-0.002144</td>\n",
       "      <td>-0.004367</td>\n",
       "      <td>-0.079805</td>\n",
       "      <td>0.178280</td>\n",
       "      <td>0.078155</td>\n",
       "      <td>0.072802</td>\n",
       "      <td>0.002090</td>\n",
       "      <td>0.211770</td>\n",
       "      <td>-0.003073</td>\n",
       "      <td>-0.188447</td>\n",
       "      <td>0.117769</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>-0.024223</td>\n",
       "      <td>0.103204</td>\n",
       "      <td>0.032484</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000765</td>\n",
       "      <td>-0.004447</td>\n",
       "      <td>0.148967</td>\n",
       "      <td>-0.018521</td>\n",
       "      <td>-0.014110</td>\n",
       "      <td>-0.001996</td>\n",
       "      <td>-0.002369</td>\n",
       "      <td>-0.120036</td>\n",
       "      <td>0.013172</td>\n",
       "      <td>-0.215571</td>\n",
       "      <td>-0.021999</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>-0.000050</td>\n",
       "      <td>-0.012120</td>\n",
       "      <td>-0.040172</td>\n",
       "      <td>-0.060103</td>\n",
       "      <td>-0.059464</td>\n",
       "      <td>-0.044899</td>\n",
       "      <td>0.015735</td>\n",
       "      <td>0.022919</td>\n",
       "      <td>-0.003106</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>-0.002339</td>\n",
       "      <td>0.040628</td>\n",
       "      <td>0.411684</td>\n",
       "      <td>0.073090</td>\n",
       "      <td>0.526222</td>\n",
       "      <td>0.071060</td>\n",
       "      <td>-0.019531</td>\n",
       "      <td>0.359889</td>\n",
       "      <td>-0.020476</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.077110</td>\n",
       "      <td>0.102563</td>\n",
       "      <td>0.188481</td>\n",
       "      <td>-0.016027</td>\n",
       "      <td>-0.135451</td>\n",
       "      <td>-0.189667</td>\n",
       "      <td>0.250967</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>-0.004265</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>G5</td>\n",
       "      <td>-0.714522</td>\n",
       "      <td>-0.042137</td>\n",
       "      <td>-0.052968</td>\n",
       "      <td>-0.796862</td>\n",
       "      <td>-0.018394</td>\n",
       "      <td>0.070102</td>\n",
       "      <td>-0.076321</td>\n",
       "      <td>-0.063864</td>\n",
       "      <td>-1.045521</td>\n",
       "      <td>-0.037353</td>\n",
       "      <td>-0.792515</td>\n",
       "      <td>-1.082483</td>\n",
       "      <td>0.025798</td>\n",
       "      <td>-0.833652</td>\n",
       "      <td>-0.625088</td>\n",
       "      <td>-0.333608</td>\n",
       "      <td>0.072579</td>\n",
       "      <td>-0.046963</td>\n",
       "      <td>0.223022</td>\n",
       "      <td>-0.605902</td>\n",
       "      <td>-0.131099</td>\n",
       "      <td>-0.235929</td>\n",
       "      <td>-0.073920</td>\n",
       "      <td>-0.063247</td>\n",
       "      <td>-0.798768</td>\n",
       "      <td>-0.899983</td>\n",
       "      <td>1.388771</td>\n",
       "      <td>-0.248677</td>\n",
       "      <td>-0.058083</td>\n",
       "      <td>-0.014470</td>\n",
       "      <td>0.092095</td>\n",
       "      <td>0.561368</td>\n",
       "      <td>0.224819</td>\n",
       "      <td>0.223190</td>\n",
       "      <td>0.098852</td>\n",
       "      <td>-0.128227</td>\n",
       "      <td>-0.215876</td>\n",
       "      <td>-0.007164</td>\n",
       "      <td>-0.035260</td>\n",
       "      <td>-0.123911</td>\n",
       "      <td>-0.089751</td>\n",
       "      <td>-0.094963</td>\n",
       "      <td>0.362818</td>\n",
       "      <td>0.011107</td>\n",
       "      <td>-1.506356</td>\n",
       "      <td>-0.573679</td>\n",
       "      <td>-0.955222</td>\n",
       "      <td>-0.818880</td>\n",
       "      <td>-1.063295</td>\n",
       "      <td>-1.022679</td>\n",
       "      <td>-1.336188</td>\n",
       "      <td>-0.612039</td>\n",
       "      <td>-0.061357</td>\n",
       "      <td>-0.482805</td>\n",
       "      <td>-0.017077</td>\n",
       "      <td>1.192135</td>\n",
       "      <td>-0.114981</td>\n",
       "      <td>-0.028074</td>\n",
       "      <td>-0.004451</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.002288</td>\n",
       "      <td>-0.045597</td>\n",
       "      <td>-0.080639</td>\n",
       "      <td>-0.081924</td>\n",
       "      <td>-0.033862</td>\n",
       "      <td>-0.005111</td>\n",
       "      <td>-0.261836</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>-0.045046</td>\n",
       "      <td>0.999854</td>\n",
       "      <td>-0.008835</td>\n",
       "      <td>-0.122379</td>\n",
       "      <td>-0.199892</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.014404</td>\n",
       "      <td>-0.000405</td>\n",
       "      <td>0.021573</td>\n",
       "      <td>-0.024160</td>\n",
       "      <td>-0.037420</td>\n",
       "      <td>-0.012610</td>\n",
       "      <td>0.003007</td>\n",
       "      <td>0.003617</td>\n",
       "      <td>-0.106893</td>\n",
       "      <td>-0.394834</td>\n",
       "      <td>-0.132496</td>\n",
       "      <td>-0.027354</td>\n",
       "      <td>-0.129804</td>\n",
       "      <td>-0.066157</td>\n",
       "      <td>-0.494334</td>\n",
       "      <td>0.123781</td>\n",
       "      <td>0.284328</td>\n",
       "      <td>0.281308</td>\n",
       "      <td>0.212767</td>\n",
       "      <td>0.192042</td>\n",
       "      <td>0.146926</td>\n",
       "      <td>-0.118826</td>\n",
       "      <td>-0.039203</td>\n",
       "      <td>-0.256107</td>\n",
       "      <td>0.176622</td>\n",
       "      <td>0.168840</td>\n",
       "      <td>0.487752</td>\n",
       "      <td>0.029464</td>\n",
       "      <td>0.014232</td>\n",
       "      <td>0.039633</td>\n",
       "      <td>0.025667</td>\n",
       "      <td>0.006626</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.006128</td>\n",
       "      <td>-0.016375</td>\n",
       "      <td>0.020727</td>\n",
       "      <td>-0.006525</td>\n",
       "      <td>-0.018790</td>\n",
       "      <td>-0.098543</td>\n",
       "      <td>0.317744</td>\n",
       "      <td>-0.180502</td>\n",
       "      <td>-0.009215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>G10</td>\n",
       "      <td>0.104791</td>\n",
       "      <td>-0.038188</td>\n",
       "      <td>-0.053191</td>\n",
       "      <td>0.620233</td>\n",
       "      <td>0.148587</td>\n",
       "      <td>0.489875</td>\n",
       "      <td>0.319274</td>\n",
       "      <td>-0.060246</td>\n",
       "      <td>0.053174</td>\n",
       "      <td>-0.025008</td>\n",
       "      <td>-0.456840</td>\n",
       "      <td>1.284450</td>\n",
       "      <td>-0.133470</td>\n",
       "      <td>3.207672</td>\n",
       "      <td>2.373230</td>\n",
       "      <td>1.304427</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.361293</td>\n",
       "      <td>2.995661</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.188988</td>\n",
       "      <td>-0.044158</td>\n",
       "      <td>-0.024550</td>\n",
       "      <td>-0.586562</td>\n",
       "      <td>-0.176292</td>\n",
       "      <td>-1.013037</td>\n",
       "      <td>0.066912</td>\n",
       "      <td>0.219649</td>\n",
       "      <td>0.154490</td>\n",
       "      <td>2.370951</td>\n",
       "      <td>1.384675</td>\n",
       "      <td>0.489152</td>\n",
       "      <td>0.484715</td>\n",
       "      <td>0.367301</td>\n",
       "      <td>0.749572</td>\n",
       "      <td>0.669410</td>\n",
       "      <td>0.423228</td>\n",
       "      <td>0.226897</td>\n",
       "      <td>3.227283</td>\n",
       "      <td>-0.329997</td>\n",
       "      <td>-0.327579</td>\n",
       "      <td>-1.033898</td>\n",
       "      <td>0.014531</td>\n",
       "      <td>0.211889</td>\n",
       "      <td>-1.197156</td>\n",
       "      <td>2.860444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.584223</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.272375</td>\n",
       "      <td>7.427558</td>\n",
       "      <td>-0.182816</td>\n",
       "      <td>-2.713205</td>\n",
       "      <td>-1.877595</td>\n",
       "      <td>-0.568691</td>\n",
       "      <td>0.224945</td>\n",
       "      <td>0.052749</td>\n",
       "      <td>0.377640</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.226060</td>\n",
       "      <td>0.207653</td>\n",
       "      <td>0.270327</td>\n",
       "      <td>0.283061</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.454366</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.188623</td>\n",
       "      <td>-0.265918</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.063796</td>\n",
       "      <td>1.076458</td>\n",
       "      <td>0.240011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.028327</td>\n",
       "      <td>1.764826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.005847</td>\n",
       "      <td>-0.011166</td>\n",
       "      <td>-0.012626</td>\n",
       "      <td>-0.010822</td>\n",
       "      <td>0.056514</td>\n",
       "      <td>-0.100007</td>\n",
       "      <td>-0.216081</td>\n",
       "      <td>-0.127274</td>\n",
       "      <td>-0.056206</td>\n",
       "      <td>0.175751</td>\n",
       "      <td>-0.011770</td>\n",
       "      <td>0.493157</td>\n",
       "      <td>0.487919</td>\n",
       "      <td>0.438576</td>\n",
       "      <td>0.574623</td>\n",
       "      <td>0.564379</td>\n",
       "      <td>-0.165933</td>\n",
       "      <td>-0.051256</td>\n",
       "      <td>0.410379</td>\n",
       "      <td>0.056624</td>\n",
       "      <td>0.047592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.020586</td>\n",
       "      <td>0.237539</td>\n",
       "      <td>0.017314</td>\n",
       "      <td>0.516667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.404158</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272937</td>\n",
       "      <td>0.774169</td>\n",
       "      <td>-0.007144</td>\n",
       "      <td>0.123954</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.110103</td>\n",
       "      <td>0.186669</td>\n",
       "      <td>-0.030720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G2</td>\n",
       "      <td>-0.532847</td>\n",
       "      <td>-0.006582</td>\n",
       "      <td>-0.023377</td>\n",
       "      <td>1.306702</td>\n",
       "      <td>-0.068909</td>\n",
       "      <td>0.048024</td>\n",
       "      <td>-0.119481</td>\n",
       "      <td>-0.021057</td>\n",
       "      <td>-1.012916</td>\n",
       "      <td>-0.011783</td>\n",
       "      <td>1.206727</td>\n",
       "      <td>0.311773</td>\n",
       "      <td>-0.005928</td>\n",
       "      <td>3.869459</td>\n",
       "      <td>-1.064793</td>\n",
       "      <td>0.107702</td>\n",
       "      <td>-0.126984</td>\n",
       "      <td>-0.044360</td>\n",
       "      <td>-0.181023</td>\n",
       "      <td>-0.691971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.195138</td>\n",
       "      <td>-0.104877</td>\n",
       "      <td>-0.093976</td>\n",
       "      <td>-0.757725</td>\n",
       "      <td>0.004432</td>\n",
       "      <td>-1.471299</td>\n",
       "      <td>0.643575</td>\n",
       "      <td>-0.067005</td>\n",
       "      <td>-0.006874</td>\n",
       "      <td>-0.087499</td>\n",
       "      <td>0.110638</td>\n",
       "      <td>0.046880</td>\n",
       "      <td>0.047141</td>\n",
       "      <td>-0.274713</td>\n",
       "      <td>0.169046</td>\n",
       "      <td>-0.179742</td>\n",
       "      <td>0.047391</td>\n",
       "      <td>0.015197</td>\n",
       "      <td>0.105158</td>\n",
       "      <td>-0.045135</td>\n",
       "      <td>-0.051329</td>\n",
       "      <td>0.202098</td>\n",
       "      <td>0.034693</td>\n",
       "      <td>2.904519</td>\n",
       "      <td>4.514844</td>\n",
       "      <td>-0.241111</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.521576</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.308812</td>\n",
       "      <td>-0.542532</td>\n",
       "      <td>-0.165028</td>\n",
       "      <td>1.490354</td>\n",
       "      <td>-1.550745</td>\n",
       "      <td>-0.918676</td>\n",
       "      <td>0.013484</td>\n",
       "      <td>-0.013198</td>\n",
       "      <td>0.050586</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.007522</td>\n",
       "      <td>0.194792</td>\n",
       "      <td>0.010436</td>\n",
       "      <td>0.107880</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.017641</td>\n",
       "      <td>0.136566</td>\n",
       "      <td>0.010365</td>\n",
       "      <td>0.086853</td>\n",
       "      <td>-0.286395</td>\n",
       "      <td>-0.014883</td>\n",
       "      <td>0.347297</td>\n",
       "      <td>0.017765</td>\n",
       "      <td>0.068701</td>\n",
       "      <td>0.015540</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.016119</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.043909</td>\n",
       "      <td>-0.000107</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.003895</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>-0.003034</td>\n",
       "      <td>-0.015845</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.056340</td>\n",
       "      <td>0.010802</td>\n",
       "      <td>0.063094</td>\n",
       "      <td>0.062424</td>\n",
       "      <td>0.057012</td>\n",
       "      <td>0.118399</td>\n",
       "      <td>0.116161</td>\n",
       "      <td>-0.017039</td>\n",
       "      <td>0.000839</td>\n",
       "      <td>0.054025</td>\n",
       "      <td>0.030561</td>\n",
       "      <td>0.006389</td>\n",
       "      <td>-0.073937</td>\n",
       "      <td>0.764136</td>\n",
       "      <td>-0.076195</td>\n",
       "      <td>-0.114682</td>\n",
       "      <td>0.119667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004938</td>\n",
       "      <td>0.018494</td>\n",
       "      <td>-0.003350</td>\n",
       "      <td>-0.029214</td>\n",
       "      <td>0.045747</td>\n",
       "      <td>-0.076884</td>\n",
       "      <td>-0.037859</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>G3</td>\n",
       "      <td>-0.200815</td>\n",
       "      <td>-0.016334</td>\n",
       "      <td>-0.036754</td>\n",
       "      <td>-0.886675</td>\n",
       "      <td>0.484495</td>\n",
       "      <td>-1.148744</td>\n",
       "      <td>0.152517</td>\n",
       "      <td>-0.043580</td>\n",
       "      <td>-0.935537</td>\n",
       "      <td>-0.023262</td>\n",
       "      <td>-0.908986</td>\n",
       "      <td>-0.525121</td>\n",
       "      <td>0.015492</td>\n",
       "      <td>-0.347325</td>\n",
       "      <td>0.296360</td>\n",
       "      <td>-0.242201</td>\n",
       "      <td>0.120049</td>\n",
       "      <td>-0.048293</td>\n",
       "      <td>0.290658</td>\n",
       "      <td>-0.345816</td>\n",
       "      <td>0.249586</td>\n",
       "      <td>-0.241812</td>\n",
       "      <td>-0.082055</td>\n",
       "      <td>-0.077706</td>\n",
       "      <td>-0.845163</td>\n",
       "      <td>-0.257777</td>\n",
       "      <td>0.919065</td>\n",
       "      <td>-0.522102</td>\n",
       "      <td>0.146076</td>\n",
       "      <td>0.043851</td>\n",
       "      <td>1.281726</td>\n",
       "      <td>0.039106</td>\n",
       "      <td>0.135331</td>\n",
       "      <td>0.134652</td>\n",
       "      <td>0.654099</td>\n",
       "      <td>1.437536</td>\n",
       "      <td>1.995784</td>\n",
       "      <td>-0.145004</td>\n",
       "      <td>-0.029483</td>\n",
       "      <td>0.252151</td>\n",
       "      <td>0.308723</td>\n",
       "      <td>0.293393</td>\n",
       "      <td>-0.527888</td>\n",
       "      <td>-0.003680</td>\n",
       "      <td>-1.553644</td>\n",
       "      <td>-1.233945</td>\n",
       "      <td>-0.947111</td>\n",
       "      <td>-0.926073</td>\n",
       "      <td>-0.772468</td>\n",
       "      <td>-0.636440</td>\n",
       "      <td>-0.833875</td>\n",
       "      <td>-0.527935</td>\n",
       "      <td>-0.014170</td>\n",
       "      <td>-0.142943</td>\n",
       "      <td>1.070523</td>\n",
       "      <td>-0.284682</td>\n",
       "      <td>-0.155110</td>\n",
       "      <td>-0.026941</td>\n",
       "      <td>0.480767</td>\n",
       "      <td>0.021831</td>\n",
       "      <td>-0.003234</td>\n",
       "      <td>-0.041412</td>\n",
       "      <td>0.112513</td>\n",
       "      <td>-0.157224</td>\n",
       "      <td>-0.146180</td>\n",
       "      <td>-0.014677</td>\n",
       "      <td>-0.451950</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>-0.114443</td>\n",
       "      <td>-0.307095</td>\n",
       "      <td>-0.346711</td>\n",
       "      <td>0.104144</td>\n",
       "      <td>-0.508920</td>\n",
       "      <td>-0.096666</td>\n",
       "      <td>0.044162</td>\n",
       "      <td>0.000159</td>\n",
       "      <td>0.085082</td>\n",
       "      <td>0.254664</td>\n",
       "      <td>-0.000408</td>\n",
       "      <td>-0.015390</td>\n",
       "      <td>-0.006226</td>\n",
       "      <td>-0.012542</td>\n",
       "      <td>-0.101059</td>\n",
       "      <td>0.091145</td>\n",
       "      <td>0.282110</td>\n",
       "      <td>-0.005348</td>\n",
       "      <td>0.112377</td>\n",
       "      <td>0.036976</td>\n",
       "      <td>0.731570</td>\n",
       "      <td>0.050165</td>\n",
       "      <td>0.038419</td>\n",
       "      <td>0.038011</td>\n",
       "      <td>0.265998</td>\n",
       "      <td>1.614120</td>\n",
       "      <td>1.806955</td>\n",
       "      <td>-0.122743</td>\n",
       "      <td>-0.001985</td>\n",
       "      <td>0.126103</td>\n",
       "      <td>0.630259</td>\n",
       "      <td>0.618027</td>\n",
       "      <td>-1.599633</td>\n",
       "      <td>0.032793</td>\n",
       "      <td>-0.126733</td>\n",
       "      <td>-0.163593</td>\n",
       "      <td>-0.225889</td>\n",
       "      <td>-0.026460</td>\n",
       "      <td>-0.080892</td>\n",
       "      <td>-0.095963</td>\n",
       "      <td>-0.014812</td>\n",
       "      <td>-0.324584</td>\n",
       "      <td>-0.019002</td>\n",
       "      <td>-0.379323</td>\n",
       "      <td>-0.046024</td>\n",
       "      <td>0.282145</td>\n",
       "      <td>0.011008</td>\n",
       "      <td>0.010496</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Group        I1        I2        I3        I4        I5        I6        I7  \\\n",
       "0    G9  0.136495 -0.028429 -0.037772 -0.232459 -0.016222 -0.187506 -0.322545   \n",
       "1    G5 -0.714522 -0.042137 -0.052968 -0.796862 -0.018394  0.070102 -0.076321   \n",
       "2   G10  0.104791 -0.038188 -0.053191  0.620233  0.148587  0.489875  0.319274   \n",
       "3    G2 -0.532847 -0.006582 -0.023377  1.306702 -0.068909  0.048024 -0.119481   \n",
       "4    G3 -0.200815 -0.016334 -0.036754 -0.886675  0.484495 -1.148744  0.152517   \n",
       "\n",
       "         I8        I9       I10       I11       I12       I13       I14  \\\n",
       "0 -0.043743  0.125389 -0.014757 -0.033105  0.303035 -0.093811 -0.598917   \n",
       "1 -0.063864 -1.045521 -0.037353 -0.792515 -1.082483  0.025798 -0.833652   \n",
       "2 -0.060246  0.053174 -0.025008 -0.456840  1.284450 -0.133470  3.207672   \n",
       "3 -0.021057 -1.012916 -0.011783  1.206727  0.311773 -0.005928  3.869459   \n",
       "4 -0.043580 -0.935537 -0.023262 -0.908986 -0.525121  0.015492 -0.347325   \n",
       "\n",
       "        I15       I16       I17       I18       I19       I20       I21  \\\n",
       "0 -0.271292 -0.256749 -0.100146 -0.045525 -0.078422 -0.060129 -0.069528   \n",
       "1 -0.625088 -0.333608  0.072579 -0.046963  0.223022 -0.605902 -0.131099   \n",
       "2  2.373230  1.304427       NaN       NaN -0.361293  2.995661       NaN   \n",
       "3 -1.064793  0.107702 -0.126984 -0.044360 -0.181023 -0.691971       NaN   \n",
       "4  0.296360 -0.242201  0.120049 -0.048293  0.290658 -0.345816  0.249586   \n",
       "\n",
       "        I22       I23       I24       I25       I26       I27       I28  \\\n",
       "0 -0.052432 -0.114432 -0.104989  0.342845 -0.159417  0.006772 -0.303193   \n",
       "1 -0.235929 -0.073920 -0.063247 -0.798768 -0.899983  1.388771 -0.248677   \n",
       "2 -0.188988 -0.044158 -0.024550 -0.586562 -0.176292 -1.013037  0.066912   \n",
       "3  0.195138 -0.104877 -0.093976 -0.757725  0.004432 -1.471299  0.643575   \n",
       "4 -0.241812 -0.082055 -0.077706 -0.845163 -0.257777  0.919065 -0.522102   \n",
       "\n",
       "        I29       I30       I31       I32       I33       I34       I35  \\\n",
       "0 -0.163287 -0.080599 -0.828880 -1.064215 -0.547067 -0.540497 -0.676045   \n",
       "1 -0.058083 -0.014470  0.092095  0.561368  0.224819  0.223190  0.098852   \n",
       "2  0.219649  0.154490  2.370951  1.384675  0.489152  0.484715  0.367301   \n",
       "3 -0.067005 -0.006874 -0.087499  0.110638  0.046880  0.047141 -0.274713   \n",
       "4  0.146076  0.043851  1.281726  0.039106  0.135331  0.134652  0.654099   \n",
       "\n",
       "        I36       I37       I38       I39       I40       I41       I42  \\\n",
       "0 -0.305007 -0.507724 -0.191437 -0.087362 -0.856151  0.802525  0.733080   \n",
       "1 -0.128227 -0.215876 -0.007164 -0.035260 -0.123911 -0.089751 -0.094963   \n",
       "2  0.749572  0.669410  0.423228  0.226897  3.227283 -0.329997 -0.327579   \n",
       "3  0.169046 -0.179742  0.047391  0.015197  0.105158 -0.045135 -0.051329   \n",
       "4  1.437536  1.995784 -0.145004 -0.029483  0.252151  0.308723  0.293393   \n",
       "\n",
       "        I43       I44       I45       I46       I47       I48       I49  \\\n",
       "0  0.006512  0.533290  0.195197  0.058094 -0.228889 -0.150821 -0.104986   \n",
       "1  0.362818  0.011107 -1.506356 -0.573679 -0.955222 -0.818880 -1.063295   \n",
       "2 -1.033898  0.014531  0.211889 -1.197156  2.860444       NaN  3.584223   \n",
       "3  0.202098  0.034693  2.904519  4.514844 -0.241111       NaN -0.521576   \n",
       "4 -0.527888 -0.003680 -1.553644 -1.233945 -0.947111 -0.926073 -0.772468   \n",
       "\n",
       "        I50       I51       I52       I53       I54       I55       I56  \\\n",
       "0 -0.026743  0.188312 -0.250701 -0.101190 -0.357521 -0.527956  0.611385   \n",
       "1 -1.022679 -1.336188 -0.612039 -0.061357 -0.482805 -0.017077  1.192135   \n",
       "2       NaN  1.272375  7.427558 -0.182816 -2.713205 -1.877595 -0.568691   \n",
       "3       NaN -0.308812 -0.542532 -0.165028  1.490354 -1.550745 -0.918676   \n",
       "4 -0.636440 -0.833875 -0.527935 -0.014170 -0.142943  1.070523 -0.284682   \n",
       "\n",
       "        I57       I58       dI1       dI2       dI3       dI4       dI5  \\\n",
       "0 -0.092714 -0.055733 -0.065709 -0.002144 -0.004367 -0.079805  0.178280   \n",
       "1 -0.114981 -0.028074 -0.004451 -0.000536 -0.002288 -0.045597 -0.080639   \n",
       "2  0.224945  0.052749  0.377640  0.002656  0.001226  0.226060  0.207653   \n",
       "3  0.013484 -0.013198  0.050586  0.010356  0.007522  0.194792  0.010436   \n",
       "4 -0.155110 -0.026941  0.480767  0.021831 -0.003234 -0.041412  0.112513   \n",
       "\n",
       "        dI6       dI7       dI8       dI9      dI10      dI11      dI12  \\\n",
       "0  0.078155  0.072802  0.002090  0.211770 -0.003073 -0.188447  0.117769   \n",
       "1 -0.081924 -0.033862 -0.005111 -0.261836  0.000122 -0.045046  0.999854   \n",
       "2  0.270327  0.283061  0.002934  0.454366  0.004264  0.188623 -0.265918   \n",
       "3  0.107880  0.122549  0.017641  0.136566  0.010365  0.086853 -0.286395   \n",
       "4 -0.157224 -0.146180 -0.014677 -0.451950  0.034598 -0.114443 -0.307095   \n",
       "\n",
       "       dI13      dI14      dI15      dI16      dI17      dI18      dI19  \\\n",
       "0  0.001613 -0.024223  0.103204  0.032484  0.002688  0.000765 -0.004447   \n",
       "1 -0.008835 -0.122379 -0.199892  0.013615  0.014404 -0.000405  0.021573   \n",
       "2  0.000000  2.063796  1.076458  0.240011       NaN       NaN -0.028327   \n",
       "3 -0.014883  0.347297  0.017765  0.068701  0.015540  0.000208  0.016119   \n",
       "4 -0.346711  0.104144 -0.508920 -0.096666  0.044162  0.000159  0.085082   \n",
       "\n",
       "       dI20      dI21      dI22      dI23      dI24      dI25      dI26  \\\n",
       "0  0.148967 -0.018521 -0.014110 -0.001996 -0.002369 -0.120036  0.013172   \n",
       "1 -0.024160 -0.037420 -0.012610  0.003007  0.003617 -0.106893 -0.394834   \n",
       "2  1.764826       NaN  0.005847 -0.011166 -0.012626 -0.010822  0.056514   \n",
       "3  0.003992       NaN  0.043909 -0.000107  0.000099 -0.003895  0.002490   \n",
       "4  0.254664 -0.000408 -0.015390 -0.006226 -0.012542 -0.101059  0.091145   \n",
       "\n",
       "       dI27      dI28      dI29      dI30      dI31      dI32      dI33  \\\n",
       "0 -0.215571 -0.021999  0.001728 -0.000050 -0.012120 -0.040172 -0.060103   \n",
       "1 -0.132496 -0.027354 -0.129804 -0.066157 -0.494334  0.123781  0.284328   \n",
       "2 -0.100007 -0.216081 -0.127274 -0.056206  0.175751 -0.011770  0.493157   \n",
       "3 -0.003034 -0.015845  0.002377  0.001974  0.056340  0.010802  0.063094   \n",
       "4  0.282110 -0.005348  0.112377  0.036976  0.731570  0.050165  0.038419   \n",
       "\n",
       "       dI34      dI35      dI36      dI37      dI38      dI39      dI40  \\\n",
       "0 -0.059464 -0.044899  0.015735  0.022919 -0.003106  0.001233 -0.002339   \n",
       "1  0.281308  0.212767  0.192042  0.146926 -0.118826 -0.039203 -0.256107   \n",
       "2  0.487919  0.438576  0.574623  0.564379 -0.165933 -0.051256  0.410379   \n",
       "3  0.062424  0.057012  0.118399  0.116161 -0.017039  0.000839  0.054025   \n",
       "4  0.038011  0.265998  1.614120  1.806955 -0.122743 -0.001985  0.126103   \n",
       "\n",
       "       dI41      dI42      dI43      dI44      dI45      dI46      dI47  \\\n",
       "0  0.040628  0.411684  0.073090  0.526222  0.071060 -0.019531  0.359889   \n",
       "1  0.176622  0.168840  0.487752  0.029464  0.014232  0.039633  0.025667   \n",
       "2  0.056624  0.047592  0.000000 -0.020586  0.237539  0.017314  0.516667   \n",
       "3  0.030561  0.006389 -0.073937  0.764136 -0.076195 -0.114682  0.119667   \n",
       "4  0.630259  0.618027 -1.599633  0.032793 -0.126733 -0.163593 -0.225889   \n",
       "\n",
       "       dI48      dI49      dI50      dI51      dI52      dI53      dI54  \\\n",
       "0 -0.020476  0.057151  0.077110  0.102563  0.188481 -0.016027 -0.135451   \n",
       "1  0.006626  0.005180  0.006128 -0.016375  0.020727 -0.006525 -0.018790   \n",
       "2       NaN  0.404158       NaN  0.272937  0.774169 -0.007144  0.123954   \n",
       "3       NaN  0.001799       NaN  0.004938  0.018494 -0.003350 -0.029214   \n",
       "4 -0.026460 -0.080892 -0.095963 -0.014812 -0.324584 -0.019002 -0.379323   \n",
       "\n",
       "       dI55      dI56      dI57      dI58  Class  \n",
       "0 -0.189667  0.250967  0.022171 -0.004265     -1  \n",
       "1 -0.098543  0.317744 -0.180502 -0.009215      1  \n",
       "2  0.000000 -0.110103  0.186669 -0.030720      1  \n",
       "3  0.045747 -0.076884 -0.037859 -0.012046     -1  \n",
       "4 -0.046024  0.282145  0.011008  0.010496      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44bc0384-cbe0-44f2-8213-eb9593d584ad",
   "metadata": {},
   "source": [
    "## Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aecc89f2-c2ee-4ea5-8684-821d96bb9233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 118)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78880b8-001f-4b1b-83ce-517ca37e82ab",
   "metadata": {},
   "source": [
    "# Only the two columns below have a different type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0dc447-dce1-4a48-8dfa-aa37c6a418e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Group    object\n",
       "Class     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes[df.dtypes!='float64']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50f016-d605-4f68-8a30-98434df59e4f",
   "metadata": {},
   "source": [
    "## we can conclude that all of the columns are float64 besides the group and class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "517e987d-fb06-4d7f-a279-8db111aaa902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([dtype('O'), dtype('float64'), dtype('int64')], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed39c2b0-3582-46d7-8b8b-8929f48b8383",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>50%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I1</th>\n",
       "      <td>0.073392</td>\n",
       "      <td>-0.141608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I2</th>\n",
       "      <td>-0.013688</td>\n",
       "      <td>-0.029587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I3</th>\n",
       "      <td>-0.021178</td>\n",
       "      <td>-0.038679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I4</th>\n",
       "      <td>0.005549</td>\n",
       "      <td>-0.247530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I5</th>\n",
       "      <td>-0.012115</td>\n",
       "      <td>-0.077468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean       50%\n",
       "I1  0.073392 -0.141608\n",
       "I2 -0.013688 -0.029587\n",
       "I3 -0.021178 -0.038679\n",
       "I4  0.005549 -0.247530\n",
       "I5 -0.012115 -0.077468"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='number').describe().T.loc[:,['mean', '50%']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0648db1-6391-4cfa-bcde-6f38ac235bb3",
   "metadata": {},
   "source": [
    "### It seems that the difference between median and mean values for each feature are not so high (not very skewed). the max difference is: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd7293cc-f98a-4665-a28c-6e60265da9c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2869852781880433"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='number').describe().T.loc[:,['mean', '50%']].assign(diff=lambda x: x['mean'] - x['50%'])['diff'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f341669-8432-4d93-834f-0b526ac74dfa",
   "metadata": {},
   "source": [
    "# we can see that some of the columns' max and min values are extreme. it might indicate outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddfecdde-ae5e-457d-94da-e511d8264de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I1</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.073392</td>\n",
       "      <td>0.905966</td>\n",
       "      <td>-2.432857</td>\n",
       "      <td>-0.591216</td>\n",
       "      <td>-0.141608</td>\n",
       "      <td>0.489964</td>\n",
       "      <td>5.013767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I2</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>-0.013688</td>\n",
       "      <td>0.182037</td>\n",
       "      <td>-0.076729</td>\n",
       "      <td>-0.036932</td>\n",
       "      <td>-0.029587</td>\n",
       "      <td>-0.017655</td>\n",
       "      <td>9.338686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I3</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>-0.021178</td>\n",
       "      <td>0.199068</td>\n",
       "      <td>-0.060811</td>\n",
       "      <td>-0.046093</td>\n",
       "      <td>-0.038679</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>9.323656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I4</th>\n",
       "      <td>7475.0</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.948536</td>\n",
       "      <td>-1.038730</td>\n",
       "      <td>-0.568159</td>\n",
       "      <td>-0.247530</td>\n",
       "      <td>0.290979</td>\n",
       "      <td>10.504271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I5</th>\n",
       "      <td>7995.0</td>\n",
       "      <td>-0.012115</td>\n",
       "      <td>0.986441</td>\n",
       "      <td>-15.806570</td>\n",
       "      <td>-0.192743</td>\n",
       "      <td>-0.077468</td>\n",
       "      <td>0.075142</td>\n",
       "      <td>22.755955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI55</th>\n",
       "      <td>7830.0</td>\n",
       "      <td>0.015563</td>\n",
       "      <td>0.404426</td>\n",
       "      <td>-2.345764</td>\n",
       "      <td>-0.176976</td>\n",
       "      <td>-0.015549</td>\n",
       "      <td>0.163862</td>\n",
       "      <td>4.079167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI56</th>\n",
       "      <td>7980.0</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.498026</td>\n",
       "      <td>-5.744188</td>\n",
       "      <td>-0.154206</td>\n",
       "      <td>-0.024760</td>\n",
       "      <td>0.124599</td>\n",
       "      <td>8.336605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI57</th>\n",
       "      <td>7865.0</td>\n",
       "      <td>0.003348</td>\n",
       "      <td>1.315116</td>\n",
       "      <td>-79.799539</td>\n",
       "      <td>-0.032287</td>\n",
       "      <td>0.000374</td>\n",
       "      <td>0.036215</td>\n",
       "      <td>71.825953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI58</th>\n",
       "      <td>7751.0</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.267788</td>\n",
       "      <td>-10.055223</td>\n",
       "      <td>-0.007101</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.009304</td>\n",
       "      <td>10.114502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <td>8000.0</td>\n",
       "      <td>0.084000</td>\n",
       "      <td>0.922524</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>117 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        count      mean       std        min       25%       50%       75%  \\\n",
       "I1     8000.0  0.073392  0.905966  -2.432857 -0.591216 -0.141608  0.489964   \n",
       "I2     8000.0 -0.013688  0.182037  -0.076729 -0.036932 -0.029587 -0.017655   \n",
       "I3     8000.0 -0.021178  0.199068  -0.060811 -0.046093 -0.038679 -0.027411   \n",
       "I4     7475.0  0.005549  0.948536  -1.038730 -0.568159 -0.247530  0.290979   \n",
       "I5     7995.0 -0.012115  0.986441 -15.806570 -0.192743 -0.077468  0.075142   \n",
       "...       ...       ...       ...        ...       ...       ...       ...   \n",
       "dI55   7830.0  0.015563  0.404426  -2.345764 -0.176976 -0.015549  0.163862   \n",
       "dI56   7980.0  0.002955  0.498026  -5.744188 -0.154206 -0.024760  0.124599   \n",
       "dI57   7865.0  0.003348  1.315116 -79.799539 -0.032287  0.000374  0.036215   \n",
       "dI58   7751.0  0.001651  0.267788 -10.055223 -0.007101  0.000889  0.009304   \n",
       "Class  8000.0  0.084000  0.922524  -1.000000 -1.000000  0.000000  1.000000   \n",
       "\n",
       "             max  \n",
       "I1      5.013767  \n",
       "I2      9.338686  \n",
       "I3      9.323656  \n",
       "I4     10.504271  \n",
       "I5     22.755955  \n",
       "...          ...  \n",
       "dI55    4.079167  \n",
       "dI56    8.336605  \n",
       "dI57   71.825953  \n",
       "dI58   10.114502  \n",
       "Class   1.000000  \n",
       "\n",
       "[117 rows x 8 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select_dtypes(include='number').describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995dfe3e-7f19-45a7-8b42-dce172c4fcbf",
   "metadata": {},
   "source": [
    "# Missing Value Percentages for each column in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5da26c6f-3509-42f8-94b0-0aca7336000b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>missing_perc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dI21</th>\n",
       "      <td>19.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI48</th>\n",
       "      <td>19.4125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI50</th>\n",
       "      <td>19.4125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I21</th>\n",
       "      <td>19.4125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I48</th>\n",
       "      <td>19.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I50</th>\n",
       "      <td>19.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dI24</th>\n",
       "      <td>9.4625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I24</th>\n",
       "      <td>9.1250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      missing_perc\n",
       "dI21       19.5500\n",
       "dI48       19.4125\n",
       "dI50       19.4125\n",
       "I21        19.4125\n",
       "I48        19.2500\n",
       "I50        19.2500\n",
       "dI24        9.4625\n",
       "I24         9.1250"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pd.DataFrame(\n",
    "    df.isna().sum()\n",
    ").rename(columns={0: 'missing_perc'}).\n",
    "  sort_values(['missing_perc'], ascending=False) / df.shape[0])*100).head(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf46ebd-69ed-4c0d-84ce-82553d72a1f5",
   "metadata": {},
   "source": [
    "## The Target Class seems not balanced. The class 0 is way less than other two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "269d0ac6-329c-4c50-b9b1-def87204d544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Class\n",
       " 1    3768\n",
       "-1    3096\n",
       " 0    1136\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts() #buy or hold or sell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d79847c1-1753-4336-8b82-9ad38818a148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAd0AAAE8CAYAAACFJWtJAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABAq0lEQVR4nO3deVwVZf8//hfbOawHRFmTCFARFFww7VghKgGKFqm3oaaUqGlQIaVGuVdaWlKZRffXOzFvybTbpdyQRVzRFCERl9BQLAVMkwMuIHD9/ujHfDyCyjqH4PV8PObx8Mx1zcz7YsSXc2bTE0IIEBERUbPT13UBREREbQVDl4iISCYMXSIiIpkwdImIiGTC0CUiIpIJQ5eIiEgmDF0iIiKZMHSJiIhkwtAlIiKSCUOXWpXz589DT08P8fHxui6F7iM3NxcBAQGwtLSEnp4eNm/eXO91+Pn5oXv37k1fHFEzY+iSzjz77LMwNTVFSUnJffuMGzcOCoUCV69elbGyxsnKysKLL74IJycnKJVKWFtbw9/fH6tWrUJlZaWuywMALFq0qEFh1xTCwsKQnZ2NDz74AGvWrEGfPn1q7Xfp0iXMnz8fWVlZ8hYIIC0tDXp6enWaWpqEhAR8+umnui6D7sNQ1wVQ2zVu3Dj89NNP2LRpEyZMmFCj/ebNm9iyZQuCgoLQvn17HVRYfytXrsTUqVNhZ2eH8ePHo3PnzigpKUFKSgrCw8Nx+fJlvPPOO7ouE4sWLcKoUaMQEhIi63Zv3bqF9PR0vPvuu4iMjHxg30uXLmHBggV47LHH0LNnT3kK/P95eHhgzZo1WvNiYmJgbm6Od999V9Za6ishIQEnTpxAVFSUrkuhWjB0SWeeffZZWFhYICEhodbQ3bJlC27cuIFx48bpoLr6O3ToEKZOnQq1Wo3t27fDwsJCaouKisLRo0dx4sQJHVaoe1euXAEAWFlZ6baQh7Czs8OLL76oNe/DDz9Ehw4dasxviIqKClRVVUGhUDR6XfQPI4h0KCwsTBgaGorCwsIabcOGDRMWFhbi5s2b4urVq+LNN98U3bt3F2ZmZsLCwkIEBQWJrKwsrWXy8vIEALFq1Spp3oABA8SAAQNq3bazs7PWvMrKShEbGys8PT2FUqkUtra2YsqUKeLatWsPHUtQUJAwNDQUFy5cqNPYS0tLRXR0tOjYsaNQKBSiS5cuYunSpaKqquqB46kGQMybN0/6PG/ePAFA5ObmirCwMGFpaSlUKpV46aWXxI0bN7SWu3cKCwsTQgih0WjEG2+8IZydnYVCoRA2NjbC399fZGRkPHQ8x44dE0FBQcLCwkKYmZmJQYMGifT09Br13T3d+/Ovtnv37lrrrP45DBgwQHTr1k3k5OQIPz8/YWJiIhwdHcVHH31UY123b98Wc+fOFW5ubkKhUIiOHTuKGTNmiNu3bz90THfr1q2b1t+jsrIyMWfOHNG7d2+hUqmEqampeOqpp0RqaqrWctX7cOnSpSI2Nla4uroKfX19kZmZKY3Vx8dHKJVK4erqKuLi4qSf1b3WrFkjevfuLYyNjUW7du3ECy+8IPLz86X2AQMG1PlnTLrBI13SqXHjxmH16tVYv3691teN165dQ2JiIsaMGQMTExPk5ORg8+bN+Ne//gUXFxcUFhbi66+/xoABA3Dy5Ek4Ojo2ST2vvPIK4uPj8fLLL+P1119HXl4evvjiC2RmZuLAgQMwMjKqdbmbN28iJSUFvr6+ePTRRx+6HSEEnn32WezevRvh4eHo2bMnEhMTMWPGDPzxxx+IjY1t8BhGjx4NFxcXLF68GMeOHcPKlStha2uLjz76CACwZs0aTJo0CX379sWUKVMAAG5ubgCAqVOn4ocffkBkZCQ8PT1x9epV7N+/H6dOnULv3r3vu82cnBw8/fTTUKlUmDlzJoyMjPD111/Dz88Pe/bsQb9+/TBixAhYWVlh+vTpGDNmDIYOHQpzc/Na1+fh4YGFCxdi7ty5mDJlCp5++mkAQP/+/aU+f/31F4KCgjBixAiMHj0aP/zwA2bNmgUvLy8MGTIEAFBVVYVnn30W+/fvx5QpU+Dh4YHs7GzExsbi119/bdR5bY1Gg5UrV2LMmDGYPHkySkpK8J///AeBgYH4+eefa3wlvmrVKty+fRtTpkyRzvVnZmYiKCgIDg4OWLBgASorK7Fw4ULY2NjU2N4HH3yAOXPmYPTo0Zg0aRKuXLmC5cuXw9fXF5mZmbCyssK7776L4uJi/P7779Lfofv9jElHdJ361LZVVFQIBwcHoVartebHxcUJACIxMVEI8ffRSmVlpVafvLw8oVQqxcKFC7XmoYFHuvv27RMAxNq1a7X67dy5s9b5d/vll18EAPHGG288ZMR/27x5swAg3n//fa35o0aNEnp6euLs2bP3HU813OdId+LEiVr9nn/+edG+fXuteWZmZtLR7d0sLS1FREREncZwt5CQEKFQKMS5c+ekeZcuXRIWFhbC19dXmnf3Ud/DHDly5L5jrz6i+/bbb6V5ZWVlwt7eXowcOVKat2bNGqGvry/27duntXz1368DBw7UeYz3HulWVFSIsrIyrT5//fWXsLOz09oH1WNWqVSiqKhIq//w4cOFqamp+OOPP6R5ubm5wtDQUOtI9/z588LAwEB88MEHWstnZ2cLQ0NDrfnBwcE8um3BePUy6ZSBgQFCQ0ORnp6O8+fPS/MTEhJgZ2eHwYMHAwCUSiX09f/+61pZWYmrV6/C3Nwc7u7uOHbsWJPUsmHDBlhaWuKZZ57Bn3/+KU0+Pj4wNzfH7t2777usRqMBAK3zuA+yfft2GBgY4PXXX9ea/+abb0IIgR07djR4HFOnTtX6/PTTT+Pq1atSjQ9iZWWFw4cP49KlS3XeXmVlJXbt2oWQkBC4urpK8x0cHDB27Fjs37+/TtuuL3Nzc63zqwqFAn379sVvv/0mzduwYQM8PDzQtWtXrX06aNAgAHjgPn0YAwMD6ZxsVVUVrl27hoqKCvTp06fWv5MjR47UOoKtrKxEcnIyQkJCtL6p6dSpk3SkXm3jxo2oqqrC6NGjtcZhb2+Pzp07N2ocJC+GLulc9YVSCQkJAIDff/8d+/btQ2hoKAwMDAD8/Y9abGwsOnfuDKVSiQ4dOsDGxgbHjx9HcXFxk9SRm5uL4uJi2NrawsbGRmsqLS1FUVHRfZdVqVQA8MDbn+524cIFODo61ghpDw8Pqb2h7v16u127dgD+/jr2YZYsWYITJ07AyckJffv2xfz587VCrDZXrlzBzZs34e7uXqPNw8MDVVVVuHjxYj1GUDcdO3ascctOu3bttMaZm5uLnJycGvuzS5cuAPDAfVoXq1evhre3N4yNjdG+fXvY2Nhg27Zttf6ddHFx0fpcVFSEW7duoVOnTjX63jsvNzcXQgh07ty5xlhOnTrV6HGQfHhOl3TOx8cHXbt2xXfffYd33nkH3333HYQQWlctL1q0CHPmzMHEiRPx3nvvwdraGvr6+oiKikJVVdUD16+npwchRI35994zW1VVBVtbW6xdu7bW9dR2nq1ap06dYGhoiOzs7AfWUl/3uw/0Qff7Vv9H5V61/QzuNXr0aDz99NPYtGkTdu3ahaVLl+Kjjz7Cxo0baxx96VpdxllVVQUvLy8sW7as1r5OTk4N3v5///tfvPTSSwgJCcGMGTNga2sLAwMDLF68GOfOnavR38TEpMHbqqqqgp6eHnbs2FHruHne9p+DoUstwrhx4zBnzhwcP34cCQkJ6Ny5Mx5//HGp/YcffsDAgQPxn//8R2u569evo0OHDg9cd7t27Wo9Wrv3aNLNzQ3Jycl48skn6/0PpKmpKQYNGoTU1FRcvHjxof+YOzs7Izk5GSUlJVpHu6dPn5baq2sH/h7ng2qvrwc91MHBwQGvvvoqXn31VRQVFaF379744IMP7hu6NjY2MDU1xZkzZ2q0nT59Gvr6+g0Kt6Z48ISbmxt++eUXDB48uMkfZPHDDz/A1dUVGzdu1Fr3vHnz6rS8ra0tjI2Ncfbs2Rpt985zc3ODEAIuLi7SUfr9tMQHdtD/4dfL1CJUH9XOnTsXWVlZNe7NNTAwqHGktmHDBvzxxx8PXbebmxtOnz4t3SMKAL/88gsOHDig1W/06NGorKzEe++9V2MdFRUVNYLvXvPmzYMQAuPHj0dpaWmN9oyMDKxevRoAMHToUFRWVuKLL77Q6hMbGws9PT0p4FQqFTp06IC9e/dq9fvyyy8fWMvDmJmZ1RhPZWVlja9FbW1t4ejoiLKysvuuy8DAAAEBAdiyZYvWefnCwkIkJCTgqaeekr5+r2+NQM3/cNTH6NGj8ccff+D//b//V6Pt1q1buHHjRoPXXX3Eefffy8OHDyM9Pb3Oy/v7+2Pz5s1a59DPnj1b45z+iBEjYGBggAULFtT4PRBCaD2xzczMrMlOuVDT45EutQguLi7o378/tmzZAgA1QnfYsGFYuHAhXn75ZfTv3x/Z2dlYu3at1oU79zNx4kQsW7YMgYGBCA8PR1FREeLi4tCtWzetC3wGDBiAV155BYsXL0ZWVhYCAgJgZGSE3NxcbNiwAZ999hlGjRp13+30798fK1aswKuvvoquXbtqPZEqLS0NP/74I95//30AwPDhwzFw4EC8++67OH/+PHr06IFdu3Zhy5YtiIqKkm7hAYBJkybhww8/xKRJk9CnTx/s3bsXv/76a71+vvfy8fFBcnIyli1bBkdHR7i4uMDd3R0dO3bEqFGj0KNHD5ibmyM5ORlHjhzBJ5988sD1vf/++0hKSsJTTz2FV199FYaGhvj6669RVlaGJUuWNKhGNzc3WFlZIS4uDhYWFjAzM0O/fv1qnBt9kPHjx2P9+vWYOnUqdu/ejSeffBKVlZU4ffo01q9fj8TExPs+hvJhhg0bho0bN+L5559HcHAw8vLyEBcXB09Pz1r/01Wb+fPnY9euXXjyyScxbdo06T9i3bt313r8pZubG95//33ExMTg/PnzCAkJgYWFBfLy8rBp0yZMmTIFb731FoC/9+3333+P6OhoPP744zA3N8fw4cMbNEZqBrq5aJqophUrVggAom/fvjXabt++Ld58803h4OAgTExMxJNPPinS09Nr3A50v1ts/vvf/wpXV1ehUChEz549RWJiYq0PxxBCiH//+9/Cx8dHmJiYCAsLC+Hl5SVmzpwpLl26VKdxZGRkiLFjxwpHR0dhZGQk2rVrJwYPHixWr16tddtTSUmJmD59utSvc+fONR6OIYQQN2/eFOHh4cLS0lJYWFiI0aNHi6KiovveMnTlyhWt5VetWiUAiLy8PGne6dOnha+vrzAxMZEejlFWViZmzJghevToIT3gokePHuLLL7+s07iPHTsmAgMDhbm5uTA1NRUDBw4UBw8e1OpTn1uGhBBiy5YtwtPTU7qF5t6HY9yrtn1aXl4uPvroI9GtWzehVCpFu3bthI+Pj1iwYIEoLi6uUx1C1LxlqKqqSixatEg4OzsLpVIpevXqJbZu3VqjhoeNOSUlRfTq1UsoFArh5uYmVq5cKd58801hbGxco+///vc/8dRTTwkzMzNhZmYmunbtKiIiIsSZM2ekPqWlpWLs2LHCysqKD8dogfSEqMPVFUREJJuQkBDk5OQgNzdX16VQE+M5XSIiHbp165bW59zcXGzfvh1+fn66KYiaFY90iYh0yMHBAS+99BJcXV1x4cIFfPXVVygrK0NmZiY6d+6s6/KoifFCKiIiHQoKCsJ3332HgoICKJVKqNVqLFq0iIHbSvFIl4iISCY8p0tERCQThi4REZFMeE63DqqqqnDp0iVYWFjwEWtERG2YEAIlJSVwdHSU3nxWHwzdOrh06VKjHoxORESty8WLF9GxY8d6L8fQrYPqB9JfvHixQc+QJSKi1kGj0cDJyanO786+F0O3Dqq/UlapVAxdIiJq8KlGXkhFREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcmED8cgImomj729Tdcl0F3Ofxis6xJ4pEtERCQXhi4REZFMGLpEREQyYegSERHJhKFLREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUx0GrpfffUVvL29oVKpoFKpoFarsWPHDqndz88Penp6WtPUqVO11pGfn4/g4GCYmprC1tYWM2bMQEVFhVaftLQ09O7dG0qlEp06dUJ8fLwcwyMiItKi02cvd+zYER9++CE6d+4MIQRWr16N5557DpmZmejWrRsAYPLkyVi4cKG0jKmpqfTnyspKBAcHw97eHgcPHsTly5cxYcIEGBkZYdGiRQCAvLw8BAcHY+rUqVi7di1SUlIwadIkODg4IDAwUN4BExFRm6YnhBC6LuJu1tbWWLp0KcLDw+Hn54eePXvi008/rbXvjh07MGzYMFy6dAl2dnYAgLi4OMyaNQtXrlyBQqHArFmzsG3bNpw4cUJaLjQ0FNevX8fOnTvrVJNGo4GlpSWKi4uhUqkaPUYiahv4woOWpSleeNDYPGgx53QrKyuxbt063LhxA2q1Wpq/du1adOjQAd27d0dMTAxu3rwptaWnp8PLy0sKXAAIDAyERqNBTk6O1Mff319rW4GBgUhPT79vLWVlZdBoNFoTERFRY+n81X7Z2dlQq9W4ffs2zM3NsWnTJnh6egIAxo4dC2dnZzg6OuL48eOYNWsWzpw5g40bNwIACgoKtAIXgPS5oKDggX00Gg1u3boFExOTGjUtXrwYCxYsaPKxEhFR26bz0HV3d0dWVhaKi4vxww8/ICwsDHv27IGnpyemTJki9fPy8oKDgwMGDx6Mc+fOwc3NrdlqiomJQXR0tPRZo9HAycmp2bZHRERtg86/XlYoFOjUqRN8fHywePFi9OjRA5999lmtffv16wcAOHv2LADA3t4ehYWFWn2qP9vb2z+wj0qlqvUoFwCUSqV0RXX1RERE1Fg6D917VVVVoaysrNa2rKwsAICDgwMAQK1WIzs7G0VFRVKfpKQkqFQq6StqtVqNlJQUrfUkJSVpnTcmIiKSg06/Xo6JicGQIUPw6KOPoqSkBAkJCUhLS0NiYiLOnTuHhIQEDB06FO3bt8fx48cxffp0+Pr6wtvbGwAQEBAAT09PjB8/HkuWLEFBQQFmz56NiIgIKJVKAMDUqVPxxRdfYObMmZg4cSJSU1Oxfv16bNvGqwqJiEheOg3doqIiTJgwAZcvX4alpSW8vb2RmJiIZ555BhcvXkRycjI+/fRT3LhxA05OThg5ciRmz54tLW9gYICtW7di2rRpUKvVMDMzQ1hYmNZ9vS4uLti2bRumT5+Ozz77DB07dsTKlSt5jy4REcmuxd2n2xLxPl0iagjep9uy8D5dIiKiNoShS0REJBOGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFMGLpEREQyYegSERHJhKFLREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcnEUNcFtDWPvb1N1yXQPc5/GKzrEoiojeCRLhERkUx0GrpfffUVvL29oVKpoFKpoFarsWPHDqn99u3biIiIQPv27WFubo6RI0eisLBQax35+fkIDg6GqakpbG1tMWPGDFRUVGj1SUtLQ+/evaFUKtGpUyfEx8fLMTwiIiItOg3djh074sMPP0RGRgaOHj2KQYMG4bnnnkNOTg4AYPr06fjpp5+wYcMG7NmzB5cuXcKIESOk5SsrKxEcHIzy8nIcPHgQq1evRnx8PObOnSv1ycvLQ3BwMAYOHIisrCxERUVh0qRJSExMlH28RETUtukJIYSui7ibtbU1li5dilGjRsHGxgYJCQkYNWoUAOD06dPw8PBAeno6nnjiCezYsQPDhg3DpUuXYGdnBwCIi4vDrFmzcOXKFSgUCsyaNQvbtm3DiRMnpG2Ehobi+vXr2LlzZ51q0mg0sLS0RHFxMVQqVaPGx3O6LQ/P6VJz4e97y9IUv+uNzYMWc063srIS69atw40bN6BWq5GRkYE7d+7A399f6tO1a1c8+uijSE9PBwCkp6fDy8tLClwACAwMhEajkY6W09PTtdZR3ad6HbUpKyuDRqPRmoiIiBpL56GbnZ0Nc3NzKJVKTJ06FZs2bYKnpycKCgqgUChgZWWl1d/Ozg4FBQUAgIKCAq3ArW6vbntQH41Gg1u3btVa0+LFi2FpaSlNTk5OTTFUIiJq43Qeuu7u7sjKysLhw4cxbdo0hIWF4eTJkzqtKSYmBsXFxdJ08eJFndZDREStg87v01UoFOjUqRMAwMfHB0eOHMFnn32GF154AeXl5bh+/brW0W5hYSHs7e0BAPb29vj555+11ld9dfPdfe694rmwsBAqlQomJia11qRUKqFUKptkfERERNV0fqR7r6qqKpSVlcHHxwdGRkZISUmR2s6cOYP8/Hyo1WoAgFqtRnZ2NoqKiqQ+SUlJUKlU8PT0lPrcvY7qPtXrICIikotOj3RjYmIwZMgQPProoygpKUFCQgLS0tKQmJgIS0tLhIeHIzo6GtbW1lCpVHjttdegVqvxxBNPAAACAgLg6emJ8ePHY8mSJSgoKMDs2bMREREhHalOnToVX3zxBWbOnImJEyciNTUV69evx7ZtvKqQiIjkpdPQLSoqwoQJE3D58mVYWlrC29sbiYmJeOaZZwAAsbGx0NfXx8iRI1FWVobAwEB8+eWX0vIGBgbYunUrpk2bBrVaDTMzM4SFhWHhwoVSHxcXF2zbtg3Tp0/HZ599ho4dO2LlypUIDAyUfbxERNS2tbj7dFsi3qfbuvE+XWou/H1vWXifLhERURvC0CUiIpIJQ5eIiEgmDF0iIiKZMHSJiIhkwtAlIiKSCUOXiIhIJgxdIiIimTB0iYiIZMLQJSIikglDl4iISCYMXSIiIpkwdImIiGTC0CUiIpIJQ5eIiEgmDF0iIiKZMHSJiIhkwtAlIiKSiU5Dd/HixXj88cdhYWEBW1tbhISE4MyZM1p9/Pz8oKenpzVNnTpVq09+fj6Cg4NhamoKW1tbzJgxAxUVFVp90tLS0Lt3byiVSnTq1Anx8fHNPTwiIiItOg3dPXv2ICIiAocOHUJSUhLu3LmDgIAA3LhxQ6vf5MmTcfnyZWlasmSJ1FZZWYng4GCUl5fj4MGDWL16NeLj4zF37lypT15eHoKDgzFw4EBkZWUhKioKkyZNQmJiomxjJSIiMtTlxnfu3Kn1OT4+Hra2tsjIyICvr68039TUFPb29rWuY9euXTh58iSSk5NhZ2eHnj174r333sOsWbMwf/58KBQKxMXFwcXFBZ988gkAwMPDA/v370dsbCwCAwObb4BERER3aVHndIuLiwEA1tbWWvPXrl2LDh06oHv37oiJicHNmzeltvT0dHh5ecHOzk6aFxgYCI1Gg5ycHKmPv7+/1joDAwORnp5eax1lZWXQaDRaExERUWPp9Ej3blVVVYiKisKTTz6J7t27S/PHjh0LZ2dnODo64vjx45g1axbOnDmDjRs3AgAKCgq0AheA9LmgoOCBfTQaDW7dugUTExOttsWLF2PBggVNPkYiImrbWkzoRkRE4MSJE9i/f7/W/ClTpkh/9vLygoODAwYPHoxz587Bzc2tWWqJiYlBdHS09Fmj0cDJyalZtkVERG1Hi/h6OTIyElu3bsXu3bvRsWPHB/bt168fAODs2bMAAHt7exQWFmr1qf5cfR74fn1UKlWNo1wAUCqVUKlUWhMREVFj6TR0hRCIjIzEpk2bkJqaChcXl4cuk5WVBQBwcHAAAKjVamRnZ6OoqEjqk5SUBJVKBU9PT6lPSkqK1nqSkpKgVqubaCREREQPp9PQjYiIwH//+18kJCTAwsICBQUFKCgowK1btwAA586dw3vvvYeMjAycP38eP/74IyZMmABfX194e3sDAAICAuDp6Ynx48fjl19+QWJiImbPno2IiAgolUoAwNSpU/Hbb79h5syZOH36NL788kusX78e06dP19nYiYio7dFp6H711VcoLi6Gn58fHBwcpOn7778HACgUCiQnJyMgIABdu3bFm2++iZEjR+Knn36S1mFgYICtW7fCwMAAarUaL774IiZMmICFCxdKfVxcXLBt2zYkJSWhR48e+OSTT7By5UreLkRERLLS6YVUQogHtjs5OWHPnj0PXY+zszO2b9/+wD5+fn7IzMysV31ERERNqUVcSEVERNQWMHSJiIhk0qDQPXbsGLKzs6XPW7ZsQUhICN555x2Ul5c3WXFEREStSYNC95VXXsGvv/4KAPjtt98QGhoKU1NTbNiwATNnzmzSAomIiFqLBoXur7/+ip49ewIANmzYAF9fXyQkJCA+Ph7/+9//mrI+IiKiVqNBoSuEQFVVFQAgOTkZQ4cOBfD31cZ//vln01VHRETUijQodPv06YP3338fa9aswZ49exAcHAzg7/fW3vtiASIiIvpbg0I3NjYWx44dQ2RkJN5991106tQJAPDDDz+gf//+TVogERFRa9Ggh2P06NFD6+rlakuXLoWhYYt5cREREVGL0qAjXVdXV1y9erXG/Nu3b6NLly6NLoqIiKg1alDonj9/HpWVlTXml5WV4ffff290UURERK1Rvb4L/vHHH6U/JyYmwtLSUvpcWVmJlJSUOr2ej4iIqC2qV+iGhIQAAPT09BAWFqbVZmRkhMceewyffPJJkxVHRETUmtQrdKvvzXVxccGRI0fQoUOHZimKiIioNWrQpcZ5eXlNXQcREVGr1+D7e1JSUpCSkoKioiLpCLjaN9980+jCiIiIWpsGhe6CBQuwcOFC9OnTBw4ODtDT02vquoiIiFqdBoVuXFwc4uPjMX78+Kauh4iIqNVq0H265eXlTfK4x8WLF+Pxxx+HhYUFbG1tERISgjNnzmj1uX37NiIiItC+fXuYm5tj5MiRKCws1OqTn5+P4OBgmJqawtbWFjNmzEBFRYVWn7S0NPTu3RtKpRKdOnVCfHx8o+snIiKqjwaF7qRJk5CQkNDoje/ZswcRERE4dOgQkpKScOfOHQQEBODGjRtSn+nTp+Onn37Chg0bsGfPHly6dAkjRoyQ2isrKxEcHIzy8nIcPHgQq1evRnx8PObOnSv1ycvLQ3BwMAYOHIisrCxERUVh0qRJSExMbPQYiIiI6kpPCCHqu9Abb7yBb7/9Ft7e3vD29oaRkZFW+7JlyxpUzJUrV2Bra4s9e/bA19cXxcXFsLGxQUJCAkaNGgUAOH36NDw8PJCeno4nnngCO3bswLBhw3Dp0iXpDUdxcXGYNWsWrly5AoVCgVmzZmHbtm04ceKEtK3Q0FBcv34dO3fufGhdGo0GlpaWKC4uhkqlatDYqj329rZGLU9N7/yHwbougVop/r63LE3xu97YPGjQke7x48fRs2dP6Ovr48SJE8jMzJSmrKyshqwSAFBcXAwAsLa2BgBkZGTgzp078Pf3l/p07doVjz76KNLT0wEA6enp8PLy0nqlYGBgIDQaDXJycqQ+d6+juk/1Ou5VVlYGjUajNRERETVWgy6k2r17d1PXgaqqKkRFReHJJ59E9+7dAQAFBQVQKBSwsrLS6mtnZ4eCggKpz73v8K3+/LA+Go0Gt27dgomJiVbb4sWLsWDBgiYbGxEREdDAI93mEBERgRMnTmDdunW6LgUxMTEoLi6WposXL+q6JCIiagUadKQ7cODAB96bm5qaWq/1RUZGYuvWrdi7dy86duwozbe3t0d5eTmuX7+udbRbWFgIe3t7qc/PP/+stb7qq5vv7nPvFc+FhYVQqVQ1jnIBQKlUQqlU1msMRERED9OgI92ePXuiR48e0uTp6Yny8nIcO3YMXl5edV6PEAKRkZHYtGkTUlNTa7yhyMfHB0ZGRkhJSZHmnTlzBvn5+VCr1QAAtVqN7OxsFBUVSX2SkpKgUqng6ekp9bl7HdV9qtdBREQkhwYd6cbGxtY6f/78+SgtLa3zeiIiIpCQkIAtW7bAwsJCOgdraWkJExMTWFpaIjw8HNHR0bC2toZKpcJrr70GtVqNJ554AgAQEBAAT09PjB8/HkuWLEFBQQFmz56NiIgI6Wh16tSp+OKLLzBz5kxMnDgRqampWL9+PbZt45WFREQknyY9p/viiy/W67nLX331FYqLi+Hn5wcHBwdp+v7776U+sbGxGDZsGEaOHAlfX1/Y29tj48aNUruBgQG2bt0KAwMDqNVqvPjii5gwYQIWLlwo9XFxccG2bduQlJSEHj164JNPPsHKlSsRGBjYNAMnIiKqgwa/8KA26enpMDY2rnP/utwibGxsjBUrVmDFihX37ePs7Izt27c/cD1+fn7IzMysc21ERERNrUGhe/cToYC/w/Py5cs4evQo5syZ0ySFERERtTYNCl1LS0utz/r6+nB3d8fChQsREBDQJIURERG1Ng0K3VWrVjV1HURERK1eo87pZmRk4NSpUwCAbt26oVevXk1SFBERUWvUoNAtKipCaGgo0tLSpIdWXL9+HQMHDsS6detgY2PTlDUSERG1Cg26Zei1115DSUkJcnJycO3aNVy7dg0nTpyARqPB66+/3tQ1EhERtQoNOtLduXMnkpOT4eHhIc3z9PTEihUreCEVERHRfTToSLeqqqrGO3QBwMjICFVVVY0uioiIqDVqUOgOGjQIb7zxBi5duiTN++OPPzB9+nQMHjy4yYojIiJqTRoUul988QU0Gg0ee+wxuLm5wc3NDS4uLtBoNFi+fHlT10hERNQqNOicrpOTE44dO4bk5GScPn0aAODh4QF/f/8mLY6IiKg1qdeRbmpqKjw9PaHRaKCnp4dnnnkGr732Gl577TU8/vjj6NatG/bt29dctRIREf2j1St0P/30U0yePBkqlapGm6WlJV555RUsW7asyYojIiJqTeoVur/88guCgoLu2x4QEICMjIxGF0VERNQa1St0CwsLa71VqJqhoSGuXLnS6KKIiIhao3qF7iOPPIITJ07ct/348eNwcHBodFFEREStUb1Cd+jQoZgzZw5u375do+3WrVuYN28ehg0b1mTFERERtSb1umVo9uzZ2LhxI7p06YLIyEi4u7sDAE6fPo0VK1agsrIS7777brMUSkRE9E9XryNdOzs7HDx4EN27d0dMTAyef/55PP/883jnnXfQvXt37N+/H3Z2dnVe3969ezF8+HA4OjpCT08Pmzdv1mp/6aWXoKenpzXdeyHXtWvXMG7cOKhUKlhZWSE8PBylpaVafY4fP46nn34axsbGcHJywpIlS+ozbCIioiZR74djODs7Y/v27fjrr79w9uxZCCHQuXNntGvXrt4bv3HjBnr06IGJEydixIgRtfYJCgrCqlWrpM9KpVKrfdy4cbh8+TKSkpJw584dvPzyy5gyZQoSEhIAABqNBgEBAfD390dcXByys7MxceJEWFlZYcqUKfWumYiIqKEa/BL7du3a4fHHH2/UxocMGYIhQ4Y8sI9SqYS9vX2tbadOncLOnTtx5MgR9OnTBwCwfPlyDB06FB9//DEcHR2xdu1alJeX45tvvoFCoUC3bt2QlZWFZcuWMXSJiEhWDXr2spzS0tJga2sLd3d3TJs2DVevXpXa0tPTYWVlJQUuAPj7+0NfXx+HDx+W+vj6+kKhUEh9AgMDcebMGfz111+1brOsrAwajUZrIiIiaqwWHbpBQUH49ttvkZKSgo8++gh79uzBkCFDUFlZCQAoKCiAra2t1jKGhoawtrZGQUGB1Ofe88zVn6v73Gvx4sWwtLSUJicnp6YeGhERtUEN/npZDqGhodKfvby84O3tDTc3N6SlpTXrKwRjYmIQHR0tfdZoNAxeIiJqtBZ9pHsvV1dXdOjQAWfPngUA2Nvbo6ioSKtPRUUFrl27Jp0Htre3R2FhoVaf6s/3O1esVCqhUqm0JiIiosb6R4Xu77//jqtXr0pPvVKr1bh+/brW855TU1NRVVWFfv36SX327t2LO3fuSH2SkpLg7u7eoCuuiYiIGkqnoVtaWoqsrCxkZWUBAPLy8pCVlYX8/HyUlpZixowZOHToEM6fP4+UlBQ899xz6NSpEwIDAwH8/Q7foKAgTJ48GT///DMOHDiAyMhIhIaGwtHREQAwduxYKBQKhIeHIycnB99//z0+++wzra+PiYiI5KDT0D169Ch69eqFXr16AQCio6PRq1cvzJ07FwYGBjh+/DieffZZdOnSBeHh4fDx8cG+ffu07tVdu3YtunbtisGDB2Po0KF46qmn8O9//1tqt7S0xK5du5CXlwcfHx+8+eabmDt3Lm8XIiIi2en0Qio/Pz8IIe7bnpiY+NB1WFtbSw/CuB9vb2/s27ev3vURERE1pX/UOV0iIqJ/MoYuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFMGLpEREQyYegSERHJhKFLREQkE4YuERGRTHT6aj+ituKxt7fpugS6y/kPg3VdArVRPNIlIiKSCUOXiIhIJgxdIiIimeg0dPfu3Yvhw4fD0dERenp62Lx5s1a7EAJz586Fg4MDTExM4O/vj9zcXK0+165dw7hx46BSqWBlZYXw8HCUlpZq9Tl+/DiefvppGBsbw8nJCUuWLGnuoREREdWg09C9ceMGevTogRUrVtTavmTJEnz++eeIi4vD4cOHYWZmhsDAQNy+fVvqM27cOOTk5CApKQlbt27F3r17MWXKFKldo9EgICAAzs7OyMjIwNKlSzF//nz8+9//bvbxERER3U2nVy8PGTIEQ4YMqbVNCIFPP/0Us2fPxnPPPQcA+Pbbb2FnZ4fNmzcjNDQUp06dws6dO3HkyBH06dMHALB8+XIMHToUH3/8MRwdHbF27VqUl5fjm2++gUKhQLdu3ZCVlYVly5ZphTMREVFza7HndPPy8lBQUAB/f39pnqWlJfr164f09HQAQHp6OqysrKTABQB/f3/o6+vj8OHDUh9fX18oFAqpT2BgIM6cOYO//vqr1m2XlZVBo9FoTURERI3VYkO3oKAAAGBnZ6c1387OTmorKCiAra2tVruhoSGsra21+tS2jru3ca/FixfD0tJSmpycnBo/ICIiavNabOjqUkxMDIqLi6Xp4sWLui6JiIhagRYbuvb29gCAwsJCrfmFhYVSm729PYqKirTaKyoqcO3aNa0+ta3j7m3cS6lUQqVSaU1ERESN1WJD18XFBfb29khJSZHmaTQaHD58GGq1GgCgVqtx/fp1ZGRkSH1SU1NRVVWFfv36SX327t2LO3fuSH2SkpLg7u6Odu3ayTQaIiIiHYduaWkpsrKykJWVBeDvi6eysrKQn58PPT09REVF4f3338ePP/6I7OxsTJgwAY6OjggJCQEAeHh4ICgoCJMnT8bPP/+MAwcOIDIyEqGhoXB0dAQAjB07FgqFAuHh4cjJycH333+Pzz77DNHR0ToaNRERtVU6vWXo6NGjGDhwoPS5OgjDwsIQHx+PmTNn4saNG5gyZQquX7+Op556Cjt37oSxsbG0zNq1axEZGYnBgwdDX18fI0eOxOeffy61W1paYteuXYiIiICPjw86dOiAuXPn8nYhIiKSnU5D18/PD0KI+7br6elh4cKFWLhw4X37WFtbIyEh4YHb8fb2xr59+xpcJxERUVNosed0iYiIWhuGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFMGLpEREQyYegSERHJhKFLREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFMGLpEREQyadGhO3/+fOjp6WlNXbt2ldpv376NiIgItG/fHubm5hg5ciQKCwu11pGfn4/g4GCYmprC1tYWM2bMQEVFhdxDISIigqGuC3iYbt26ITk5WfpsaPh/JU+fPh3btm3Dhg0bYGlpicjISIwYMQIHDhwAAFRWViI4OBj29vY4ePAgLl++jAkTJsDIyAiLFi2SfSxERNS2tfjQNTQ0hL29fY35xcXF+M9//oOEhAQMGjQIALBq1Sp4eHjg0KFDeOKJJ7Br1y6cPHkSycnJsLOzQ8+ePfHee+9h1qxZmD9/PhQKhdzDISKiNqxFf70MALm5uXB0dISrqyvGjRuH/Px8AEBGRgbu3LkDf39/qW/Xrl3x6KOPIj09HQCQnp4OLy8v2NnZSX0CAwOh0WiQk5Nz322WlZVBo9FoTURERI3VokO3X79+iI+Px86dO/HVV18hLy8PTz/9NEpKSlBQUACFQgErKyutZezs7FBQUAAAKCgo0Arc6vbqtvtZvHgxLC0tpcnJyalpB0ZERG1Si/56eciQIdKfvb290a9fPzg7O2P9+vUwMTFptu3GxMQgOjpa+qzRaBi8RETUaC36SPdeVlZW6NKlC86ePQt7e3uUl5fj+vXrWn0KCwulc8D29vY1rmau/lzbeeJqSqUSKpVKayIiImqsf1TolpaW4ty5c3BwcICPjw+MjIyQkpIitZ85cwb5+flQq9UAALVajezsbBQVFUl9kpKSoFKp4OnpKXv9RETUtrXor5ffeustDB8+HM7Ozrh06RLmzZsHAwMDjBkzBpaWlggPD0d0dDSsra2hUqnw2muvQa1W44knngAABAQEwNPTE+PHj8eSJUtQUFCA2bNnIyIiAkqlUsejIyKitqZFh+7vv/+OMWPG4OrVq7CxscFTTz2FQ4cOwcbGBgAQGxsLfX19jBw5EmVlZQgMDMSXX34pLW9gYICtW7di2rRpUKvVMDMzQ1hYGBYuXKirIRERURvWokN33bp1D2w3NjbGihUrsGLFivv2cXZ2xvbt25u6NCIionr7R53TJSIi+idj6BIREcmEoUtERCQThi4REZFMGLpEREQyYegSERHJhKFLREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFMGLpEREQyYegSERHJpE2F7ooVK/DYY4/B2NgY/fr1w88//6zrkoiIqA1pM6H7/fffIzo6GvPmzcOxY8fQo0cPBAYGoqioSNelERFRG9FmQnfZsmWYPHkyXn75ZXh6eiIuLg6mpqb45ptvdF0aERG1EYa6LkAO5eXlyMjIQExMjDRPX18f/v7+SE9Pr9G/rKwMZWVl0ufi4mIAgEajaXQtVWU3G70OalpNsV8fhvu9ZZFjnwPc7y1NU+z36nUIIRq0fJsI3T///BOVlZWws7PTmm9nZ4fTp0/X6L948WIsWLCgxnwnJ6dmq5F0x/JTXVdAcuM+b5uacr+XlJTA0tKy3su1idCtr5iYGERHR0ufq6qqcO3aNbRv3x56enoA/v7fjpOTEy5evAiVSqWrUnWCY+fYOfa2g2PXHrsQAiUlJXB0dGzQOttE6Hbo0AEGBgYoLCzUml9YWAh7e/sa/ZVKJZRKpdY8KyurWtetUqna3F/Eahw7x97WcOwcO4AGHeFWaxMXUikUCvj4+CAlJUWaV1VVhZSUFKjVah1WRkREbUmbONIFgOjoaISFhaFPnz7o27cvPv30U9y4cQMvv/yyrksjIqI2os2E7gsvvIArV65g7ty5KCgoQM+ePbFz584aF1fVlVKpxLx582p8Dd0WcOwce1vDsXPsTUVPNPS6ZyIiIqqXNnFOl4iIqCVg6BIREcmEoUtERCQThi4REZFMGLr18MEHH6B///4wNTW978My7vXSSy9BT09PawoKCmreQptBQ8YuhMDcuXPh4OAAExMT+Pv7Izc3t3kLbQbXrl3DuHHjoFKpYGVlhfDwcJSWlj5wGT8/vxr7ferUqTJV3HD1ff3lhg0b0LVrVxgbG8PLywvbt2+XqdKmV5+xx8fH19i/xsbGMlbbdPbu3Yvhw4fD0dERenp62Lx580OXSUtLQ+/evaFUKtGpUyfEx8c3e51Nrb7jTktLq7HP9fT0UFBQUK/tMnTroby8HP/6178wbdq0ei0XFBSEy5cvS9N3333XTBU2n4aMfcmSJfj8888RFxeHw4cPw8zMDIGBgbh9+3YzVtr0xo0bh5ycHCQlJWHr1q3Yu3cvpkyZ8tDlJk+erLXflyxZIkO1DVff118ePHgQY8aMQXh4ODIzMxESEoKQkBCcOHFC5sobryGv/lSpVFr798KFCzJW3HRu3LiBHj16YMWKFXXqn5eXh+DgYAwcOBBZWVmIiorCpEmTkJiY2MyVNq36jrvamTNntPa7ra1t/TYsqN5WrVolLC0t69Q3LCxMPPfcc81aj5zqOvaqqiphb28vli5dKs27fv26UCqV4rvvvmvGCpvWyZMnBQBx5MgRad6OHTuEnp6e+OOPP+673IABA8Qbb7whQ4VNp2/fviIiIkL6XFlZKRwdHcXixYtr7T969GgRHBysNa9fv37ilVdeadY6m0N9x16ffwP+SQCITZs2PbDPzJkzRbdu3bTmvfDCCyIwMLAZK2tedRn37t27BQDx119/NWpbPNKVQVpaGmxtbeHu7o5p06bh6tWrui6p2eXl5aGgoAD+/v7SPEtLS/Tr16/W1ym2VOnp6bCyskKfPn2kef7+/tDX18fhw4cfuOzatWvRoUMHdO/eHTExMbh5s+W+5q369Zd3768Hvf4S+Ptnc3d/AAgMDPxH7V+gYWMHgNLSUjg7O8PJyQnPPfcccnJy5ChX51rLfm+onj17wsHBAc888wwOHDhQ7+XbzBOpdCUoKAgjRoyAi4sLzp07h3feeQdDhgxBeno6DAwMdF1es6k+z1Hb6xTrew5ElwoKCmp8fWRoaAhra+sHjmPs2LFwdnaGo6Mjjh8/jlmzZuHMmTPYuHFjc5fcIPV9/SXw98/mn75/gYaN3d3dHd988w28vb1RXFyMjz/+GP3790dOTg46duwoR9k6c7/9rtFocOvWLZiYmOiosubl4OCAuLg49OnTB2VlZVi5ciX8/Pxw+PBh9O7du87rafOh+/bbb+Ojjz56YJ9Tp06ha9euDVp/aGio9GcvLy94e3vDzc0NaWlpGDx4cIPW2VSae+wtWV3H3lB3n/P18vKCg4MDBg8ejHPnzsHNza3B66WWQa1Wa70spX///vDw8MDXX3+N9957T4eVUXNxd3eHu7u79Ll///44d+4cYmNjsWbNmjqvp82H7ptvvomXXnrpgX1cXV2bbHuurq7o0KEDzp49q/PQbc6xV78ysbCwEA4ODtL8wsJC9OzZs0HrbEp1Hbu9vX2Ni2kqKipw7dq1Wl8LeT/9+vUDAJw9e7ZFhm59X38J/L2P69O/pWrI2O9lZGSEXr164ezZs81RYotyv/2uUqla7VHu/fTt2xf79++v1zJtPnRtbGxgY2Mj2/Z+//13XL16VSuIdKU5x+7i4gJ7e3ukpKRIIavRaHD48OF6X/3dHOo6drVajevXryMjIwM+Pj4AgNTUVFRVVUlBWhdZWVkA0CL2e23ufv1lSEgIgP97/WVkZGSty6jVaqSkpCAqKkqal5SU9I97XWZDxn6vyspKZGdnY+jQoc1YacugVqtr3Br2T9zvTSErK6v+v9ONugyrjblw4YLIzMwUCxYsEObm5iIzM1NkZmaKkpISqY+7u7vYuHGjEEKIkpIS8dZbb4n09HSRl5cnkpOTRe/evUXnzp3F7du3dTWMBqnv2IUQ4sMPPxRWVlZiy5Yt4vjx4+K5554TLi4u4tatW7oYQoMFBQWJXr16icOHD4v9+/eLzp07izFjxkjtv//+u3B3dxeHDx8WQghx9uxZsXDhQnH06FGRl5cntmzZIlxdXYWvr6+uhlAn69atE0qlUsTHx4uTJ0+KKVOmCCsrK1FQUCCEEGL8+PHi7bfflvofOHBAGBoaio8//licOnVKzJs3TxgZGYns7GxdDaHB6jv2BQsWiMTERHHu3DmRkZEhQkNDhbGxscjJydHVEBqspKRE+n0GIJYtWyYyMzPFhQsXhBBCvP3222L8+PFS/99++02YmpqKGTNmiFOnTokVK1YIAwMDsXPnTl0NoUHqO+7Y2FixefNmkZubK7Kzs8Ubb7wh9PX1RXJycr22y9Cth7CwMAGgxrR7926pDwCxatUqIYQQN2/eFAEBAcLGxkYYGRkJZ2dnMXnyZOkX+Z+kvmMX4u/bhubMmSPs7OyEUqkUgwcPFmfOnJG/+Ea6evWqGDNmjDA3NxcqlUq8/PLLWv/ZyMvL0/pZ5OfnC19fX2FtbS2USqXo1KmTmDFjhiguLtbRCOpu+fLl4tFHHxUKhUL07dtXHDp0SGobMGCACAsL0+q/fv160aVLF6FQKES3bt3Etm3bZK646dRn7FFRUVJfOzs7MXToUHHs2DEdVN141bfC3DtVjzcsLEwMGDCgxjI9e/YUCoVCuLq6av3e/1PUd9wfffSRcHNzE8bGxsLa2lr4+fmJ1NTUem+Xr/YjIiKSCe/TJSIikglDl4iISCYMXSIiIpkwdImIiGTC0CUiIpIJQ5eIiEgmDF0iIiKZMHSJiIhkwtAlamP09PSwefNmXZdB1CYxdIlamYKCArz22mtwdXWFUqmEk5MThg8fjpSUFF2XRtTmtfm3DBG1JufPn8eTTz4JKysrLF26FF5eXrhz5w4SExMRERFx35eyE5E8eKRL1Iq8+uqr0NPTw88//4yRI0eiS5cu6NatG6Kjo3Ho0KFal5k1axa6dOkCU1NTuLq6Ys6cObhz547U/ssvv2DgwIGwsLCASqWCj48Pjh49CgC4cOEChg8fjnbt2sHMzAzdunXTeu3biRMnMGTIEJibm8POzg7jx4/Hn3/+KbX/8MMP8PLygomJCdq3bw9/f3/cuHGjmX46RLrHI12iVuLatWvYuXMnPvjgA5iZmdVot7KyqnU5CwsLxMfHw9HREdnZ2Zg8eTIsLCwwc+ZMAMC4cePQq1cvfPXVVzAwMEBWVhaMjIwAABERESgvL8fevXthZmaGkydPwtzcHABw/fp1DBo0CJMmTUJsbCxu3bqFWbNmYfTo0UhNTcXly5cxZswYLFmyBM8//zxKSkqwb98+8B0s1JoxdIlaibNnz0IIga5du9ZrudmzZ0t/fuyxx/DWW29h3bp1Uujm5+djxowZ0no7d+4s9c/Pz8fIkSPh5eUFAHB1dZXavvjiC/Tq1QuLFi2S5n3zzTdwcnLCr7/+itLSUlRUVGDEiBFwdnYGAGk9RK0VQ5eolWjoEeL333+Pzz//HOfOnZOCUKVSSe3R0dGYNGkS1qxZA39/f/zrX/+Cm5sbAOD111/HtGnTsGvXLvj7+2PkyJHw9vYG8PfX0rt375aOfO927tw5BAQEYPDgwfDy8kJgYCACAgIwatQotGvXrkHjIPon4Dldolaic+fO0NPTq9fFUunp6Rg3bhyGDh2KrVu3IjMzE++++y7Ky8ulPvPnz0dOTg6Cg4ORmpoKT09PbNq0CQAwadIk/Pbbbxg/fjyys7PRp08fLF++HABQWlqK4cOHIysrS2vKzc2Fr68vDAwMkJSUhB07dsDT0xPLly+Hu7s78vLymvYHQ9SS1Pu190TUYgUFBYlHHnlElJaW1mj766+/hBBCABCbNm0SQgjx8ccfC1dXV61+4eHhwtLS8r7bCA0NFcOHD6+17e233xZeXl5CCCHeeecd4e7uLu7cuVOn2isqKsQjjzwiPvnkkzr1J/on4pEuUSuyYsUKVFZWom/fvvjf//6H3NxcnDp1Cp9//jnUanWN/p07d0Z+fj7WrVuHc+fO4fPPP5eOYgHg1q1biIyMRFpaGi5cuIADBw7gyJEj8PDwAABERUUhMTEReXl5OHbsGHbv3i21RURE4Nq1axgzZgyOHDmCc+fOITExES+//DIqKytx+PBhLFq0CEePHkV+fj42btyIK1euSMsTtUq6Tn0ialqXLl0SERERwtnZWSgUCvHII4+IZ599VuzevVsIoX2kK4QQM2bMEO3btxfm5ubihRdeELGxsdKRbllZmQgNDRVOTk5CoVAIR0dHERkZKW7duiWEECIyMlK4ubkJpVIpbGxsxPjx48Wff/4prfvXX38Vzz//vLCyshImJiaia9euIioqSlRVVYmTJ0+KwMBAYWNjI5RKpejSpYtYvny5XD8mIp3QE4LX5xMREcmBXy8TERHJhKFLREQkE4YuERGRTBi6REREMmHoEhERyYShS0REJBOGLhERkUwYukRERDJh6BIREcmEoUtERCQThi4REZFM/j9hvKq4BRHoKQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bar_plot = pd.DataFrame(df.Class.value_counts()).reset_index()\n",
    "\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.bar(x=bar_plot['Class'], height=bar_plot['count'])\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Value Counts of the Target')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0a4ea1-cf1c-476e-a5dd-c3dfaa80e600",
   "metadata": {},
   "source": [
    "## There are many highly correlated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b22a65b-5f83-4925-9741-813be0207b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs with correlation greater than 0.6:\n",
      "[('I2', 'I3', 0.9818), ('I2', 'I8', 0.9738), ('I2', 'I10', 0.9249), ('I3', 'I2', 0.9818), ('I3', 'I8', 0.986)]\n"
     ]
    }
   ],
   "source": [
    "correlation_matrix = df.loc[:, ~df.columns.isin(['Group', 'Class'])].corr()\n",
    "\n",
    "high_correlation_pairs = (correlation_matrix.abs() > 0.9) & (correlation_matrix != 1)\n",
    "\n",
    "pairs = [(col1, col2, round(correlation_matrix.loc[col1, col2],4)) for col1 in correlation_matrix.columns for col2 in correlation_matrix.columns if high_correlation_pairs.loc[col1, col2]]\n",
    "\n",
    "print(\"Pairs with correlation greater than 0.6:\")\n",
    "print(pairs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a5df07-532a-44cb-aa98-ea1906000de3",
   "metadata": {},
   "source": [
    "## Model Selection Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "575bd56f-398f-41e5-bd49-4e078da153e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection:\n",
    "    def __init__(self,\n",
    "                 x_train: pd.DataFrame,\n",
    "                 y_train: pd.DataFrame,\n",
    "                 x_test: pd.DataFrame = None,\n",
    "                 y_test: pd.DataFrame = None,\n",
    "                 numerical_cols: list = None,\n",
    "                 one_hot_cols: list = None,\n",
    "                 freq_encod_cols: list = None,\n",
    "                 ordinal_cols: list = None,\n",
    "                 estimators: list = None,\n",
    "                 cost_matrix: np.array = None\n",
    "                ):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.numerical_cols = numerical_cols\n",
    "        self.one_hot_cols = one_hot_cols \n",
    "        self.freq_encod_cols = freq_encod_cols\n",
    "        self.ordinal_cols = ordinal_cols\n",
    "        self.estimators = estimators\n",
    "        self.cost_matrix = cost_matrix\n",
    "        \n",
    "        self.grid_object = None\n",
    "        self.grid_best_model = None\n",
    "        self.grid_best_parameters = None\n",
    "        self.grid_cv_results = None\n",
    "        self.grid_best_score = None\n",
    "        \n",
    "        self.encoded_y_train = None\n",
    "        \n",
    "        self.col_transformer = None\n",
    "        self.frequency_encoder = None\n",
    "        \n",
    "        self.f1_cv_results = dict()\n",
    "        self.accuracy_cv_results = dict()\n",
    "        self.auc_cv_results = dict()\n",
    "        \n",
    "        self.mean_result_f1 = dict()\n",
    "        self.mean_result_accuracy = dict()\n",
    "        self.mean_result_auc = dict()\n",
    "        self.mean_result_cost_matrix = dict()\n",
    "        \n",
    "        self.best_estimator_name_f1 = None\n",
    "        self.best_estimator_name_accuracy = None\n",
    "        self.best_estimator_name_auc = None\n",
    "        self.best_estimator_name_cost_matrix = None\n",
    "        self.best_estimator_obj_f1 = None\n",
    "        self.best_estimator_obj_accuracy = None\n",
    "        self.best_estimator_obj_auc = None\n",
    "        self.best_estimator_obj_cost_matrix = None\n",
    "        \n",
    "        self.target_label_mapping = None\n",
    "        self.target_label_encoder = None\n",
    "        \n",
    "        self.test_preds = None\n",
    "        self.test_f1_score = None\n",
    "        self.test_auc_score = None\n",
    "        self.test_accuracy_score = None\n",
    "        \n",
    "        self.error_cost_matrix_results = dict()\n",
    "        self.only_train_model = None\n",
    "        self.only_x_train_preds = None\n",
    "        \n",
    "    def encode_y_train(self):\n",
    "        lab_encoder = LabelEncoder()\n",
    "        self.encoded_y_train = lab_encoder.fit_transform(self.y_train.values.ravel())\n",
    "        self.target_label_encoder = lab_encoder\n",
    "        self.target_label_mapping = {\n",
    "            label: category for label, category in enumerate(lab_encoder.classes_)\n",
    "        }\n",
    "\n",
    "    def create_numerical_col_pipe(self):\n",
    "        return (\"scaler\", StandardScaler(), self.numerical_cols)\n",
    "        \n",
    "    def create_nominal_onehot_col_pipe(self):\n",
    "        return (\"ohe_encoder\", OneHotEncoder(handle_unknown='ignore', drop=\"first\"), self.one_hot_cols)\n",
    "        \n",
    "    def create_ordinal_col_pipe(self):\n",
    "        return (\"binary_encoder\", LabelEncoder(), self.ordinal_cols)\n",
    "\n",
    "    def create_col_transformer(self):\n",
    "        transformers = []\n",
    "        if self.numerical_cols is not None:\n",
    "            transformers.append(self.create_numerical_col_pipe())\n",
    "        if self.one_hot_cols is not None:\n",
    "            transformers.append(self.create_nominal_onehot_col_pipe())\n",
    "        if self.ordinal_cols is not None:\n",
    "            transformers.append(self.create_ordinal_col_pipe())\n",
    "        \n",
    "        if self.freq_encod_cols is not None:\n",
    "            freq_encoder = CountFrequencyEncoder(\n",
    "                encoding_method='frequency',\n",
    "                variables=freq_encod_cols\n",
    "            )\n",
    "            freq_encoder.fit(self.x_train)\n",
    "            self.x_train = freq_encoder.transform(self.x_train)\n",
    "            self.frequency_encoder = freq_encoder\n",
    "            \n",
    "\n",
    "        self.col_transformer = ColumnTransformer(\n",
    "            transformers=transformers\n",
    "        )\n",
    "        \n",
    "    def get_estimator_by_name(self, estimator_name):\n",
    "        for name, estimator in self.estimators:\n",
    "            if name == estimator_name:\n",
    "                return estimator\n",
    "        return None\n",
    "        \n",
    "    def calculate_cv_f1(self, n_folds, scoring_average='f1_weighted'): # 'f1_micro' or 'f1_macro'\n",
    "        for name, estimator in self.estimators:\n",
    "            f1_pipe = Pipeline(\n",
    "                [\n",
    "                    ('ColumnTransformers', self.col_transformer), \n",
    "                    ('ClassificationModel', estimator)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "            \n",
    "            f1_cv_score = cross_val_score(\n",
    "                f1_pipe, \n",
    "                self.x_train, \n",
    "                self.encoded_y_train, \n",
    "                cv=stratif_cv, \n",
    "                scoring=scoring_average,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            self.f1_cv_results[name] = f1_cv_score\n",
    "            self.mean_result_f1[name] = f1_cv_score.mean()\n",
    "        \n",
    "        self.best_estimator_name_f1 = max(self.mean_result_f1, key=self.mean_result_f1.get)\n",
    "        self.best_estimator_obj_f1 = self.get_estimator_by_name(self.best_estimator_name_f1)\n",
    "        \n",
    "        print(\"CV Results for Mean F1 Score:\\n\")\n",
    "        for name, f1 in self.mean_result_f1.items():\n",
    "            print(f\"{name} = {f1:.6f}\")\n",
    "        print(f\"\\nBest Estimator (F1): {self.best_estimator_name_f1}\")\n",
    "        \n",
    "    def calculate_cv_auc(self, n_folds, scoring_average='roc_auc_ovo_weighted'): #or 'roc_auc_ovr_weighted'\n",
    "        for name, estimator in self.estimators:\n",
    "            auc_pipe = Pipeline(\n",
    "                [\n",
    "                    ('ColumnTransformers', self.col_transformer), \n",
    "                    ('ClassificationModel', estimator)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "            \n",
    "            auc_cv_score = cross_val_score(\n",
    "                auc_pipe, \n",
    "                self.x_train, \n",
    "                self.encoded_y_train, \n",
    "                cv=stratif_cv, \n",
    "                scoring=scoring_average, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            self.auc_cv_results[name] = auc_cv_score\n",
    "            self.mean_result_auc[name] = auc_cv_score.mean()\n",
    "        \n",
    "        self.best_estimator_name_auc = max(self.mean_result_auc, key=self.mean_result_auc.get)\n",
    "        self.best_estimator_obj_auc = self.get_estimator_by_name(self.best_estimator_name_auc)\n",
    "        \n",
    "        print(\"CV Results for Mean AUC Score:\\n\")\n",
    "        for name, auc in self.mean_result_auc.items():\n",
    "            print(f\"{name} = {auc:.6f}\")\n",
    "        print(f\"\\nBest Estimator (AUC): {self.best_estimator_name_auc}\")\n",
    "    \n",
    "    def calculate_cv_accuracy(self, n_folds, scoring_average='accuracy'):\n",
    "        for name, estimator in self.estimators:\n",
    "            accuracy_pipe = Pipeline(\n",
    "                [\n",
    "                    ('ColumnTransformers', self.col_transformer), \n",
    "                    ('ClassificationModel', estimator)\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "            \n",
    "            accuracy_cv_score = cross_val_score(\n",
    "                accuracy_pipe, \n",
    "                self.x_train, \n",
    "                self.encoded_y_train, \n",
    "                cv=stratif_cv, \n",
    "                scoring=scoring_average, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            self.accuracy_cv_results[name] = accuracy_cv_score\n",
    "            self.mean_result_accuracy[name] = accuracy_cv_score.mean()\n",
    "        \n",
    "        self.best_estimator_name_accuracy = max(self.mean_result_accuracy, key=self.mean_result_accuracy.get)\n",
    "        self.best_estimator_obj_accuracy = self.get_estimator_by_name(self.best_estimator_name_accuracy)\n",
    "        \n",
    "        print(\"CV Results for Mean Accuracy Score:\\n\")\n",
    "        for name, acc in self.mean_result_accuracy.items():\n",
    "            print(f\"{name} = {acc:.6f}\")\n",
    "        print(f\"\\nBest Estimator (Accuracy): {self.best_estimator_name_accuracy}\")\n",
    "    \n",
    "    def custom_error_cost_score(self, y_true, y_pred):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        error_value = np.sum(cm * self.cost_matrix) / len(y_true)\n",
    "        \n",
    "        return -error_value\n",
    "    \n",
    "    def calculate_cost_matrix_error_cv(self, n_folds=5):\n",
    "        cost_matrix_socrer = make_scorer(self.custom_error_cost_score, greater_is_better=True)\n",
    "        \n",
    "        for name, estimator in self.estimators:\n",
    "            cost_matrix_pipe = Pipeline(\n",
    "                [\n",
    "                    ('ColumnTransformers', self.col_transformer), \n",
    "                    ('ClassificationModel', estimator)\n",
    "                ]\n",
    "            )\n",
    "            stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "            \n",
    "            cost_matrix_cv_score = cross_val_score(\n",
    "                cost_matrix_pipe, \n",
    "                self.x_train, \n",
    "                self.encoded_y_train, \n",
    "                cv=stratif_cv, \n",
    "                scoring=cost_matrix_socrer, \n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            self.error_cost_matrix_results[name] = -cost_matrix_cv_score\n",
    "            self.mean_result_cost_matrix[name] = -cost_matrix_cv_score.mean()    \n",
    "\n",
    "\n",
    "        self.best_estimator_name_cost_matrix = min(self.mean_result_cost_matrix, key=self.mean_result_cost_matrix.get)\n",
    "        self.best_estimator_obj_cost_matrix = self.get_estimator_by_name(self.best_estimator_name_cost_matrix)\n",
    "        \n",
    "        print(\"CV Results for Cost Matrix Error Score:\\n\")\n",
    "        for name, cost_err in self.mean_result_cost_matrix.items():\n",
    "            print(f\"{name} = {cost_err:.6f}\")\n",
    "        print(f\"\\nBest Estimator (Cost Metric Error Score): {self.best_estimator_name_cost_matrix}\")\n",
    "        \n",
    "        \n",
    "    def print_classification_report_cv(self, estimator, n_folds=10):\n",
    "        pipe = Pipeline(\n",
    "            [\n",
    "                ('ColumnTransformers', self.col_transformer), \n",
    "                ('ClassificationModel', estimator)\n",
    "            ]\n",
    "        )\n",
    "        stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "        y_pred = cross_val_predict(pipe, self.x_train, self.encoded_y_train, cv=stratif_cv)\n",
    "        print(f'CV Classification Report Result for {estimator}')\n",
    "        print(self.target_label_mapping)\n",
    "        print(classification_report(self.encoded_y_train, y_pred))\n",
    "\n",
    "    def plot_confusion_matrix_cv(self, estimator, figsize=(5, 5), n_folds=10):\n",
    "        pipe = Pipeline([\n",
    "            ('ColumnTransformers', self.col_transformer), \n",
    "            ('ClassificationModel', estimator)\n",
    "        ])\n",
    "        \n",
    "        stratif_cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "        y_pred = cross_val_predict(pipe, self.x_train, self.encoded_y_train, cv=stratif_cv)\n",
    "        cm = confusion_matrix(self.encoded_y_train, y_pred)\n",
    "        \n",
    "        print(f'CV Confusion Matrix for {estimator}')\n",
    "        print(self.target_label_mapping)\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.heatmap(cm, annot=True, cmap='Reds', fmt='g')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()\n",
    "        \n",
    "    def apply_grid_cv(\n",
    "        self, \n",
    "        estimator=None, \n",
    "        params=None, \n",
    "        cv=5, \n",
    "        scoring='f1_weighted'\n",
    "    ):\n",
    "        pipe = Pipeline([\n",
    "            ('ColumnTransformers', self.col_transformer), \n",
    "            ('ClassificationModel', estimator)\n",
    "        ])\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=pipe, \n",
    "            param_grid=params, \n",
    "            cv=cv, \n",
    "            n_jobs=-1, \n",
    "            verbose=2, \n",
    "            scoring=scoring\n",
    "        )\n",
    "        grid_search.fit(self.x_train, self.encoded_y_train)\n",
    "        \n",
    "        self.grid_object = grid_search\n",
    "        self.grid_best_parameters = self.grid_object.best_params_\n",
    "        self.grid_best_model = self.grid_object.best_estimator_\n",
    "        self.grid_cv_results = self.grid_object.cv_results_\n",
    "        self.grid_best_score = self.grid_object.best_score_\n",
    "        \n",
    "        print(f\"Best parameters found by GridSearchCV ({scoring}):\\n{self.grid_best_parameters}\")\n",
    "        print(f\"\\nBest score found by GridSearchCV ({scoring}):\\n{self.grid_best_score}\")\n",
    "        \n",
    "    def predict_on_test(self, estimator):\n",
    "        self.test_preds = self.grid_best_model.predict(self.x_test)\n",
    "        print(self.target_label_mapping)\n",
    "        print(classification_report(self.target_label_encoder.transform(self.y_test.values.ravel()), self.test_preds))\n",
    "        \n",
    "        self.test_f1_score = f1_score(\n",
    "            self.target_label_encoder.transform(self.y_test.values.ravel()), \n",
    "            self.test_preds, \n",
    "            average='weighted'\n",
    "        )\n",
    "        \n",
    "        self.test_auc_score = roc_auc_score(\n",
    "            self.target_label_encoder.transform(self.y_test.values.ravel()), \n",
    "            self.grid_best_model.predict_proba(self.x_test), \n",
    "            average='weighted',\n",
    "            multi_class='ovo'\n",
    "        )\n",
    "        \n",
    "        self.test_accuracy_score = accuracy_score(\n",
    "            self.target_label_encoder.transform(self.y_test.values.ravel()),\n",
    "            self.test_preds\n",
    "        )\n",
    "        \n",
    "        cm = confusion_matrix(self.target_label_encoder.transform(self.y_test.values.ravel()), self.test_preds)\n",
    "        print(self.target_label_mapping)\n",
    "        plt.figure(figsize=(4,3))\n",
    "        sns.heatmap(cm, annot=True, cmap='Reds', fmt='g')\n",
    "        plt.xlabel('Predicted')\n",
    "        plt.ylabel('Actual')\n",
    "        plt.title('Confusion Matrix')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852d5c1-cd68-4fb6-a691-4436538012a3",
   "metadata": {},
   "source": [
    "# Classification Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "346abda1-a979-431a-b6a2-0219de1a8b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('SVM', SVC(random_state=0, probability=True)),\n",
    "    ('NaiveBayes', GaussianNB()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\")),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0)),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork', MLPClassifier(random_state=0)) #hidden_layer_sizes=(20,20) for 2 hidden layers with 20 neurons each\n",
    "]\n",
    "\n",
    "vote_model = VotingClassifier(\n",
    "    estimators=voting_estimators, \n",
    "    voting='soft'\n",
    ")\n",
    "\n",
    "stacking_estimators = [\n",
    "    ('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    ('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    ('SVM', SVC(random_state=0, probability=True)),\n",
    "    ('NaiveBayes', GaussianNB()),\n",
    "    ('KNN', KNeighborsClassifier()),\n",
    "    ('AdaBoost', AdaBoostClassifier(random_state=0)),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    ('NeuralNetwork', MLPClassifier(random_state=0))\n",
    "]\n",
    "meta_stack_classifier = LogisticRegression(random_state=0, solver=\"saga\")\n",
    "\n",
    "stacking_model = StackingClassifier(\n",
    "    estimators=stacking_estimators, \n",
    "    final_estimator=meta_stack_classifier, \n",
    "    cv=5\n",
    ")\n",
    "\n",
    "estimators = [\n",
    "    #('RandomForest', RandomForestClassifier(random_state=0)),\n",
    "    #('DecisionTree', DecisionTreeClassifier(random_state=0)),\n",
    "    #('SVM', SVC(random_state=0, probability=True)),\n",
    "    #('NaiveBayes', GaussianNB()),\n",
    "    #('KNN', KNeighborsClassifier()),\n",
    "    ('LogisticRegression', LogisticRegression(random_state=0, solver=\"saga\")),\n",
    "    ('GradientBoost', GradientBoostingClassifier(random_state=0)),\n",
    "    ('XGBoost', XGBClassifier(seed=0)),\n",
    "    #('AdaBoost', AdaBoostClassifier(random_state=0)),\n",
    "    #('Voting', vote_model),\n",
    "    #('Stacking', stacking_model),\n",
    "    #('NeuralNetwork', MLPClassifier(random_state=0)) # 2 hidden layers with 20 neurons each\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927301e-086d-4742-9879-86ef9e4d807e",
   "metadata": {},
   "source": [
    "## Median Imputation for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a4e1607-c901-4550-a28f-b7dcfd3b9f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, df.columns != 'Group'] = df.loc[:, df.columns!='Group'].apply(lambda col: col.fillna(col.median()), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b35a4a-292b-455f-ab3c-eca077dcdecd",
   "metadata": {},
   "source": [
    "## Smote oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "403d7a4b-9b66-4e02-800e-20f974a841d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_smote_oh_encode(data, categorical_column, one_h_encoder, cat: bool = True, random_state=0):\n",
    "    \"\"\"\n",
    "    Apply SMOTE (Synthetic Minority Over-sampling Technique) to balance the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (DataFrame): Input DataFrame containing features and target variable.\n",
    "        categorical_column (str): Name of the categorical column to be one-hot encoded.\n",
    "        one_h_encoder (OneHotEncoder): One-hot encoder object fitted on categorical data.\n",
    "        cat (bool): Whether to perform one-hot encoding. Defaults to True.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "        \n",
    "    Returns:\n",
    "        smote_df (DataFrame): Resampled DataFrame with balanced classes.\n",
    "    \"\"\"\n",
    "    if cat:\n",
    "        X = data.drop(columns=['Class'])\n",
    "        y = data['Class']\n",
    "    \n",
    "        X_encoded = pd.DataFrame(\n",
    "            one_h_encoder.transform(X[[categorical_column]]).toarray(),\n",
    "            columns=one_h_encoder.get_feature_names_out([categorical_column])\n",
    "        )\n",
    "    \n",
    "        X = X.drop(columns=[categorical_column])\n",
    "        X = pd.concat([X, X_encoded], axis=1)\n",
    "    else:\n",
    "        X = data.drop(columns=['Class'])\n",
    "        y = data['Class']\n",
    "    \n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=random_state)\n",
    "    X_smote, y_smote = smote.fit_resample(X, y)\n",
    "    \n",
    "    smote_df = pd.concat([pd.DataFrame(X_smote, columns=X.columns), pd.Series(y_smote, name='Class')], axis=1)\n",
    "    \n",
    "    return smote_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "55544505-99c2-4a05-bdbd-9c1f7b79fcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_encoder = OneHotEncoder().fit(df[['Group']])\n",
    "smote_df = apply_smote_oh_encode(data=df, categorical_column='Group', one_h_encoder=ohe_encoder, cat=True, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "149af93c-53f9-4d0f-85f5-150d3df87249",
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote_df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "878f27ac-edd4-4a70-a335-67fb43be22b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_scaler_columns = df.loc[:, ~df.columns.isin(['Group', 'Class'])].columns\n",
    "scaler = StandardScaler().fit(df.loc[:, std_scaler_columns])\n",
    "\n",
    "def apply_std_scaler(data, columns, scaler):\n",
    "    \"\"\"\n",
    "    Apply standard scaling to specified columns of the data using a pre-fitted scaler.\n",
    "\n",
    "    Parameters:\n",
    "        data (DataFrame): Input DataFrame containing features.\n",
    "        columns (list): List of column names to apply standard scaling to.\n",
    "        scaler (StandardScaler): Pre-fitted StandardScaler object.\n",
    "\n",
    "    Returns:\n",
    "        scaled_data (DataFrame): DataFrame with specified columns scaled.\n",
    "    \"\"\"\n",
    "    scaled_data = data.copy()\n",
    "    scaled_data[columns] = scaler.transform(data[columns])\n",
    "    return scaled_data\n",
    "\n",
    "scaled_data = apply_std_scaler(df, std_scaler_columns, scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcadb42e-f685-4969-879e-9388dbe4716f",
   "metadata": {},
   "source": [
    "## Create X_train and Y_train\n",
    "## one hot encoded (group). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "00dc0a6b-5f8b-44f6-b737-5fd05d20c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = smote_df.loc[:, ~smote_df.columns.isin(['Class'])]\n",
    "Y_train = smote_df.loc[:, smote_df.columns.isin(['Class'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa1fdbb-5b7a-43b3-83af-7b48bee5c3b4",
   "metadata": {},
   "source": [
    "# Apply cross validation with f1, auc and accuracy\n",
    "\n",
    "#### *Weighted F1 Score: F1 score calculated by taking the average of F1 scores for each class. Average is weighted by support which is the number of true instances for each label. \n",
    "#### *AUC One vs One Weighted: By considering all pairwise combinations of classes, average AUC is calculated. Average is weighted by the support. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5fcb2a67-1bbe-4f97-8660-0c76963e3245",
   "metadata": {},
   "outputs": [],
   "source": [
    "error_cost_matrix = np.array([[0, 1, 2],\n",
    "                              [1, 0, 1],\n",
    "                              [2, 1, 0]])\n",
    "\n",
    "model_selector = ModelSelection(\n",
    "    x_train=X_train, \n",
    "    y_train=Y_train,\n",
    "    y_test = \n",
    "    numerical_cols=X_train.select_dtypes(include='number').columns.to_list(),\n",
    "    estimators=estimators\n",
    ")\n",
    "model_selector.encode_y_train()\n",
    "model_selector.create_col_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ec19b29-b2f3-4d57-9c58-fbfe15b92c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Results for Mean F1 Score:\n",
      "\n",
      "LogisticRegression = 0.941558\n",
      "GradientBoost = 1.000000\n",
      "XGBoost = 0.996816\n",
      "\n",
      "Best Estimator (F1): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean AUC Score:\n",
      "\n",
      "LogisticRegression = 0.997962\n",
      "GradientBoost = 1.000000\n",
      "XGBoost = 0.999984\n",
      "\n",
      "Best Estimator (AUC): GradientBoost\n",
      "**********************************************\n",
      "CV Results for Mean Accuracy Score:\n",
      "\n",
      "LogisticRegression = 0.940554\n",
      "GradientBoost = 1.000000\n",
      "XGBoost = 0.996816\n",
      "\n",
      "Best Estimator (Accuracy): GradientBoost\n",
      "**********************************************\n"
     ]
    }
   ],
   "source": [
    "model_selector.calculate_cv_f1(n_folds=10, scoring_average='f1_weighted')\n",
    "print('**********************************************')\n",
    "model_selector.calculate_cv_auc(n_folds=10, scoring_average='roc_auc_ovo_weighted')\n",
    "print('**********************************************')   \n",
    "model_selector.calculate_cv_accuracy(n_folds=10, scoring_average='accuracy')\n",
    "print('**********************************************')   \n",
    "#model_selector.calculate_cost_matrix_error_cv(n_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4c26315c-4921-4b8d-8fc5-5f1833c79550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LogisticRegression': array([0.92749779, 0.9372237 , 0.94429708, 0.93103448, 0.94159292,\n",
       "        0.94513274, 0.95309735, 0.94070796, 0.94159292, 0.94336283]),\n",
       " 'GradientBoost': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]),\n",
       " 'XGBoost': array([0.99292661, 0.99646331, 0.99734748, 0.99734748, 0.99646018,\n",
       "        1.        , 0.99646018, 0.99823009, 0.99734513, 0.99557522])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_selector.accuracy_cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cd7525-d0d1-4063-82ee-c4efa2dca9b3",
   "metadata": {},
   "source": [
    "## Predictions on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8827f948-70fc-43b9-87e8-61632050e804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55159d1-3824-4cad-88b9-4bed02893b41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e957c7-af64-4f26-9247-3252efa67882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce174c1-a28b-4df4-a76c-e1caf5d28925",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
